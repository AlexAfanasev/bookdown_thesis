---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Empirical application {#application}

For the empirical application the presented particle marginal Metropolis Hastings procedure will be applied to the price-dividend ratio. This application will be an extension of the presented results and procedures in [@h2_paper]. Besides applying the Bayesian approach as an alternative, further possible specifications of the latent state process will be inspected. First the steps for deriving the state-space model will be explained.

## Model implementation

Starting from the basic definition of the total log-return at time $t+1$, $r_{t+1} = \ln(P_{t+1} + D_{t+1}) - \ln(P_{t})$ with price $P_{t}$ and dividend $D_t$, the log-return can be reformulated as a nonlinear function of the log price-dividend ration $\eta_t$. First subtracting $r_{t+1}$ from itself yields
\begin{equation*}
\begin{split}
0 &= \ln\Bigg(\frac{P_t}{P_{t+1} + D_{t+1}}\frac{P_{t+1} + D_{t+1}}{P_{t}}\Bigg)
\end{split}
\end{equation*}
then adding $\eta_t$ to both sides of the equation
\begin{equation*}
\begin{split}
\eta_{t} &= \ln\Bigg(\frac{P_t}{D_t}\frac{P_{t+1} + D_{t+1}}{P_{t}}\Bigg) - r_{t+1} = \ln\Bigg(\Bigg(1 + \frac{P_{t+1}}{D_{t+1}}\Bigg)\frac{D_{t+1}}{D_{t}}\Bigg) - r_{t+1}
\end{split}
\end{equation*}
and simplifying the above yields
\begin{equation*}
\begin{split}
\eta_{t} &= \ln(1 + \text{exp}(\eta_{t+1})) - r_{t+1} + \Delta d_{t+1} \\
r_{t+1} &= -\eta_{t} + \ln(1 + \text{exp}(\eta_{t+1})) + \Delta d_{t+1}
\end{split}
\end{equation*}
Here, $\Delta d_{t+1} = \ln(D_{t+1}) - \ln(D_t)$ represents the log dividend growth. Applying a first order Taylor expansion around the fixed steady state $\bar{\eta}$ yields a linear approximation
\begin{equation}
\begin{split}
r_{t+1} &\approx -\eta_{t} + \ln(1 + \text{exp}(\bar{\eta})) + \Delta d_{t+1} + \frac{1}{1 + \text{exp}(-\bar{\eta})}(\eta_{t+1}-\bar{\eta}) \\
&\approx k -\eta_{t} + \rho\eta_{t+1} + \Delta d_{t+1}
\end{split}
\end{equation}
with $\rho$ being specified as
\begin{align}
\rho &= \frac{1}{1+\text{exp}(-\bar{\eta})} & \Longleftrightarrow && \bar{\eta} &= -\ln\Bigg(\frac{1}{\rho} - 1\Bigg)
(\#eq:rho-spec)
\end{align}
and $k$ directly following from the above
\begin{equation}
\begin{split}
k &= \ln(1 + \text{exp}(\bar{\eta})) - \bar{\eta}\frac{1}{1 + \text{exp}(-\bar{\eta})} \\
&= \ln\Bigg(1 + \text{exp}\Bigg(-\ln\Bigg(\frac{1}{\rho} - 1\Bigg)\Bigg)\Bigg) + \ln\Bigg(\frac{1}{\rho} - 1\Bigg)\frac{1}{1 + \text{exp}\Big(\ln\Big(\frac{1}{\rho} - 1\Big)\Big)} \\
&= -\ln(1 - \rho) + \rho\ln\Bigg(\frac{1}{\rho} - 1\Bigg) \\
&= -\ln(\rho) - (1 - \rho)\ln\Bigg(\frac{1}{\rho} - 1\Bigg) 
(\#eq:k-spec)
\end{split}
\end{equation}
Previous empirical applications typically assumed a constant parameter $\bar{\eta}$, as in [@campbell_paper]. However, the results presented in [@h2_paper] deliver evidence for a gradually time-varying mean of the log price-dividend ratio. By allowing this time variation $k$ and $\rho$ become also time-varying. Hence, the corresponding parameters $k_t$ and $\rho_t$ are obtained from \@ref(eq:rho-spec) and \@ref(eq:k-spec)
\begin{equation}
\begin{split}
\rho_t &= \frac{1}{1+\text{exp}(-\tilde{\eta}_t)} \\
k_t &= -\ln(\rho_t) - (1 - \rho_t)\ln\Bigg(\frac{1}{\rho_t} - 1\Bigg)
\end{split}
\end{equation}
with $\tilde{\eta}_t$ denoting the time varying local mean for the Taylor approximation. Therefore, the log price-dividend ratio has the following specification
\begin{equation}
\begin{split}
\eta_{t} &\approx k_t - r_{t+1} + \rho_t\eta_{t+1} + \Delta d_{t+1}
(\#eq:lpd-spec)
\end{split}
\end{equation}
Similar approximations as in [@van_nieuwerburgh_paper] are adopted: $\mathbb{E}_t[\rho_{t+i}]\approx\rho_t$, $\mathbb{E}_t[k_{t+i}]\approx\;k_t$ and $\mathbb{E}_t[\rho_{t+i}\eta_{t+1+i}]\approx\mathbb{E}_t[\rho_{t+i}]\mathbb{E}_t[\eta_{t+1+i}]$. The present value formulation of the log price-dividend ratio can then be concluded by taking the conditional expectation and iterating equation \@ref(eq:lpd-spec) forward
\begin{equation}
\begin{split}
\eta_{t} &\approx k_t - \mathbb{E}_t[r_{t+1}] + \rho_t\mathbb{E}_t[\eta_{t+1}] + \mathbb{E}_t[\Delta d_{t+1}] \\
 &\approx k_t - \mathbb{E}_t[r_{t+1}] + \rho_t\mathbb{E}_t[k_{t+1} - r_{t+2} + \rho_{t+1}\eta_{t+2} + \Delta d_{t+2}] + \mathbb{E}_t[\Delta d_{t+1}] \\
 &\approx \cdots \\
 &\approx \frac{k_t}{1-\rho_t} + \sum_{i=1}^{\infty}\rho_t^{i-1}\mathbb{E}_t[\Delta d_{t+i}^e - r_{t+i}^e] + \lim\limits_{i \rightarrow \infty}\rho_t^i\mathbb{E}_t[\eta_{t+i}]
(\#eq:present-value)
\end{split}
\end{equation}
Here, the excess dividend growth $\Delta d_{t}^e = \Delta d_t - r^f_t$ and return $r_t^e = r_t - r_t^f$ are used with $r^f_t$ being the risk-free interest rate.

The state-space model will be used to estimate the latent mean $\tilde{\eta}_t$. As in [@h2_paper], the present value formulation \@ref(eq:present-value) will be used as the observation process by adding an error term $\epsilon_t \sim\mathcal{N}(0, \sigma_{\epsilon}^2)$ that captures rational bubbles, approximation errors and other influences in $\lim\limits_{i \rightarrow \infty}\rho_t^i\mathbb{E}_t[\eta_{t+i}]$.
\begin{equation}
\eta_{t} = \frac{k_t}{1-\rho_t} + \sum_{i=1}^{\infty}\rho_t^{i-1}\mathbb{\tilde{E}}_t[\Delta d_{t+i}^e - r_{t+i}^e] + \epsilon_t
 (\#eq:observation-process)
\end{equation}
A low dimensional vector autoregressive (VAR) model of order $1$ will be used for the objective expectations $\mathbb{\tilde{E}}_t$ conditional on information available at the end of $t$. This approach was first proposed in [@campbell_shiller_paper]. The VAR model will be comprised of the log price-dividend ratio $\eta_t$, excess dividend growth $\Delta d_t^e$, excess return $r_t^e$ and inflation $\pi_t$.

Assuming the following VAR model
\begin{equation}
y_t = \begin{pmatrix} \eta_t \\ \Delta d_t^e \\ r_t^e \\ \pi_t \end{pmatrix} = \alpha + Ay_{t-1} + v_t
 (\#eq:ex-ante-expectations-var-model)
\end{equation}
and the following reparametrization by stacking the vector of constants $\alpha$ on to the parameter matrix $A$
\begin{equation}
y_t = \begin{pmatrix} \eta_t \\ \Delta d_t^e \\ r_t^e \\ \pi_t \\ 1 \end{pmatrix} = \left(
\begin{array}{c|c}
        A & \alpha \\
        \hline
        0 & 1\\
\end{array}
\right)y_{t-1} + \begin{pmatrix}v_t \\ 0\end{pmatrix} = B y_{t-1} + \begin{pmatrix}v_t \\ 0\end{pmatrix}
\end{equation}
the discounted expectations can be evaluated using the vector $h = \begin{pmatrix} 0 & 1 & -1 & 0 & 0\end{pmatrix}^T$
\begin{equation}
\begin{split}
\sum_{i=1}^{\infty}\rho_t^{i-1}\mathbb{\tilde{E}}_t[\Delta d_{t+i}^e - r_{t+i}^e] &= \sum_{i=1}^{\infty}\rho_t^{i-1}h^TB^iy_{t} \\
& = h^TB\sum_{i=0}^{\infty}\rho_t^{i}B^iy_{t} = h^TB(I_5 -\rho_t B)^{-1}y_t
 (\#eq:ex-ante-expectations)
\end{split}
\end{equation}
Similar to [@h2_paper] a adaptive approach will be used for the choice of VAR sample size. Starting at the forecasting origin, observation $t = 30$, VAR models with samples $\Omega_{t,\omega} = \{\eta_\tau, \Delta d^e_\tau, r_\tau^e, \pi_\tau\;|\;\tau = t-\omega+1, ..., t\}$ and varying lengths $\omega = (10, ..., 30)$ will be fitted. The selection criterion will be the root mean squared error of the last $5$ in sample observations of the excess dividend rate and excess return, $\{\Delta d_m^e - r_m^e\}^t_{t-9}$. Additionally, the stability of each VAR model will be checked, at each length $\omega$, by inspecting that all eigenvalues of $A$ have modulus less than 1. If this requirement is violated the specific VAR model will be discarded. The VAR model with $\omega = 10$ will be used if at a specific time point $t$ all VAR models violate the stability requirement.

The empirical application will be conducted on quarterly data for the S&P 500 starting at the 31st of December 1970 up until the 30th of September 2020.The 10-year treasury constant maturity rate has been used for the risk free rate $r^f_t$ and the inflation rate $\pi_t$ has been derived from the consumer price index for all urban consumers. These time series and the price and dividend data for the S&P 500 have been provided by Robert J. Shiller [@shiller_website]

## Random walk state process

Firstly, the state-space model with a random walk state process (**RW Model**) will be analyzed. The latent process follows
\begin{equation}
\begin{split}
\tilde{\eta}_t &= \tilde{\eta}_{t-1} + u_t \\
u_t &\sim \mathcal{N}(0, \sigma_u^2)
(\#eq:basic-model-state-process)
\end{split}
\end{equation}
The parameters of interest are $\theta = \begin{pmatrix} \tilde{\eta}_0 & \sigma_{\epsilon} & \sigma_{u}\end{pmatrix}^T$ with $\tilde{\eta}_0$ being the initial value . Similar to the simulation study uninformative setup, flat priors will be used for the parameters and initial value and equal parameter transformations will be applied
\begin{align*}
\sigma_\epsilon(\varsigma_\epsilon) &= \text{exp}(\varsigma_\epsilon) & \Longleftrightarrow && \varsigma_\epsilon(\sigma_\epsilon) &= \ln(\sigma_\epsilon) \\
\sigma_u(\varsigma_u) &= \text{exp}(\varsigma_u) & \Longleftrightarrow && \varsigma_\epsilon(\sigma_u) &= \ln(\sigma_u)
\end{align*}
Hence, after having applied the change of variable technique the prior specification for the hyperparameters $\{\varsigma_\epsilon, \varsigma_u\}$ becomes
\begin{equation}
\begin{split}
  p_{\varsigma_\epsilon}(\varsigma_\epsilon) &= \text{exp}(\varsigma_\epsilon) \\
  p_{\varsigma_u}(\varsigma_u) &= \text{exp}(\varsigma_u)
\end{split}
\end{equation}
For the PMMH procedure $3$ independent Markov chains with $M = 20000$ iterations and $J = 1000$ particles will be run and the starting parameter $\theta_0$ will be set to 
$$
\theta_0 = \begin{pmatrix}\tilde{\eta}_0^{(0)} \\ \varsigma_\epsilon^{(0)} \\ \varsigma_u^{(0)} \end{pmatrix} = \begin{pmatrix}3.5 \\ \ln(0.05) \\ \ln(0.05) \end{pmatrix}
$$
Furthermore, the parameter proposals will be generated using
$$
\theta_i^{*}\sim \mathcal{N}(\theta_i^{(m-1)}, 0.02^2)
$$
Figure \@ref(fig:basic-model-trace-plot) shows the trace plots for each parameter and each Markov chain. The first 5000 iterations have been dropped. Additionally, Figure \@ref(fig:basic-model-posterior-marginal-density) displays the Gaussian kernel density estimates for the random walk latent state model. The corresponding Table \@ref(tab:basic-model-setup) summarizes the inference results.

```{r, basic-model-setup, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
source(here::here("R", "prepare_raw_data.R"))
source(here::here("R", "model_comparison", "model_comparison_basic_model.R"))

library(tidyverse)

traces <- rbind(
        result[[1]]@traces[5000:20000, 3:5], 
        result[[2]]@traces[5000:20000, 3:5], 
        result[[3]]@traces[5000:20000, 3:5]
)
traces[, 2:3] <- exp(traces[, 2:3])

result_table <- data.frame(
  mean = colMeans(traces),
  median = apply(traces, 2, median),
  lower = apply(traces, 2, quantile, probs = 0.025),
  upper = apply(traces, 2, quantile, probs = 0.975)
)
rownames(result_table) <- c(
  "$\\tilde{\\eta}_0$", "$\\sigma_u$", "$\\sigma_\\epsilon$"
)
result_table <- round(result_table, 4)
opts <- options(knitr.kable.NA = "")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("Mean", "Median", "0.025\\%", "0.975\\%"),
  caption = "Random walk state model: PMMH parameter inference results", 
  booktabs = TRUE, escape = FALSE,
  linesep = "\\addlinespace", 
  align = rep("c", 4)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```

```{r, basic-model-trace-plot, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Random walk state model: PMMH trace plot", fig.width=6, fig.height=8, warning=FALSE}
library(ggplot2)
library(gridExtra)

traces <- list(
  result[[1]]@traces[5000:20000, ], 
  result[[2]]@traces[5000:20000, ], 
  result[[3]]@traces[5000:20000, ]
)

# Plot theme:
p_theme <- theme(
  legend.position = "bottom", 
  legend.title = element_blank(), 
  legend.background = element_rect(fill = "transparent"),
  plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"),
  plot.title = element_text(hjust = 0.5, size = 8),
  axis.title.x = element_text(size = 8),
  axis.title.y = element_text(size = 8, angle = 0, vjust = 0.5)
)

# e_lpd_0
df <- data.frame(
  time = 1:nrow(traces[[1]]),
  value1 = traces[[1]][, 3], 
  value2 = traces[[2]][, 3], 
  value3 = traces[[3]][, 3]
)
df <- reshape2::melt(df, id.vars = "time")
p1 <- ggplot(data = df, aes(x = time, y = value)) +
  geom_line(aes(colour = variable)) + theme_light() +
  scale_colour_manual(
    values = c("red", "green", "blue"),
    labels = c("Chain 1", "Chain 2", "Chain 3")
  ) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\tilde{\\eta}_0$}"),
       x = "m", y = latex2exp::TeX("$\\tilde{\\eta}_0$")) +
  p_theme

# sigma_u
df <- data.frame(
  time = 1:nrow(traces[[1]]),
  value1 = traces[[1]][, 4], 
  value2 = traces[[2]][, 4], 
  value3 = traces[[3]][, 4]
)
df <- reshape2::melt(df, id.vars = "time")
df$value <- exp(df$value)
p2 <- ggplot(data = df, aes(x = time, y = value)) +
  geom_line(aes(colour = variable)) + theme_light() +
  scale_colour_manual(
    values = c("red", "green", "blue"),
    labels = c("Chain 1", "Chain 2", "Chain 3")
  ) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\sigma_u$}"),
       x = "m", y = latex2exp::TeX("$\\sigma_u$")) +
  p_theme

# sigma_epsilon
df <- data.frame(
  time = 1:nrow(traces[[1]]),
  value1 = traces[[1]][, 5], 
  value2 = traces[[2]][, 5], 
  value3 = traces[[3]][, 5]
)
df <- reshape2::melt(df, id.vars = "time")
df$value <- exp(df$value)
p3 <- ggplot(data = df, aes(x = time, y = value)) +
  geom_line(aes(colour = variable)) + theme_light() +
  scale_colour_manual(
    values = c("red", "green", "blue"),
    labels = c("Chain 1", "Chain 2", "Chain 3")
  ) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\sigma_\\epsilon$}"),
       x = "m", y = latex2exp::TeX("$\\sigma_\\epsilon$")) +
  p_theme

legend <- gtable::gtable_filter(ggplot_gtable(ggplot_build(p2)), "guide-box")

grid.arrange(
  p1 + theme(legend.position = "none"), 
  p2 + theme(legend.position = "none"), 
  p3 + theme(legend.position = "none"),
  ncol = 1, legend = legend, nrow = 4, 
  heights = c(1.1, 1.1, 1.1, 0.2)
)
```

```{r, basic-model-posterior-marginal-density, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Random walk state model: Gaussian kernel density estimates of the marginal posterior distributions", fig.width=6, fig.height=4, warning=FALSE}
traces <- rbind(
        result[[1]]@traces[5000:20000, 3:5], 
        result[[2]]@traces[5000:20000, 3:5], 
        result[[3]]@traces[5000:20000, 3:5]
)

# Plot theme:
p_theme <- theme(
  legend.justification = c(0, 1), legend.position = c(0, 1), 
  legend.title = element_blank(), 
  legend.background = element_rect(fill = "transparent"),
  plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"),
  plot.title = element_text(hjust = 0.5, size = 8),
  axis.title.x = element_text(size = 8),
  axis.title.y = element_text(size = 8, vjust = 0.5)
)

# e_lpd_0
df <- data.frame(time = 1:nrow(traces), e_lpd_0 = traces[, 1])
p1 <- ggplot(data = df, aes(x = e_lpd_0)) +
  geom_density(aes(x = e_lpd_0, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = mean(traces[, 1]), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = function(...){1}, aes(colour = "Prior", linetype = "Prior")
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", 
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, "Prior" = 1), name = ""
  ) + 
  labs(title = latex2exp::TeX(
    "Marginal Posterior: \\textbf{$\\tilde{\\eta}_0$}"
  ), x = latex2exp::TeX("$\\tilde{\\eta}_0$"))

# sigma_u
df <- data.frame(time = 1:nrow(traces), sigma_u = exp(traces[, 2]))
p2 <- ggplot(data = df, aes(x = sigma_u)) +
  geom_density(aes(x = sigma_u, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = mean(exp(traces[, 2])), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = function(...){1}, aes(colour = "Prior", linetype = "Prior")
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", 
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, "Prior" = 1), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal Posterior: \\textbf{$\\sigma_u$}"),
       x = latex2exp::TeX("$\\sigma_u$"))

# sigma_epsilon
df <- data.frame(time = 1:nrow(traces), sigma_e = exp(traces[, 3]))
p3 <- ggplot(data = df, aes(x = sigma_e)) +
  geom_density(aes(x = sigma_e, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = mean(exp(traces[, 3])), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = function(...){1}, aes(colour = "Prior", linetype = "Prior")
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", 
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, "Prior" = 1), name = ""
  ) + 
  labs(title = latex2exp::TeX(
    "Marginal Posterior: \\textbf{$\\sigma_\\epsilon$}"
  ), x = latex2exp::TeX("$\\sigma_\\epsilon$"))

get_legend <- function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}
legend <- get_legend(p2)

grid.arrange(
  p1 + theme(legend.position = "none"), 
  p2 + theme(legend.position = "none"), 
  p3 + theme(legend.position = "none"), 
  ncol = 2, legend = legend
)
```

Moreover, Figure \@ref(fig:basic-model-state-estimates) visualizes the marginal posterior mean for the latent state and corresponding $95\%$ pointwise credible intervals. One can directly see that the estimated latent mean captures structural changes in the log price-dividend ratio. 

```{r, basic-model-state-estimates, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Random walk state model: Latent state estimate", fig.width=6, fig.height=3.5, warning=FALSE}
traj <- rbind(
  result[[1]]@filter.traj[1, 5000:20000, ], 
  result[[2]]@filter.traj[1, 5000:20000, ],
  result[[3]]@filter.traj[1, 5000:20000, ]
)

df <- data.frame(time = dates, y = y$lpd, x = colMeans(traj)[-1])
df <- reshape2::melt(df, id.vars = "time")
q <- data.frame(
  time = dates,
  variable = "x",
  lower = apply(traj, 2, quantile, probs = 0.025)[-1],
  upper = apply(traj, 2, quantile, probs = 0.975)[-1]
)
ggplot(data = df, aes(x = time, y = value, group = variable)) +
  geom_line(aes(color = variable)) + 
  geom_ribbon(
    data = q, aes(x = time, ymin = lower, ymax = upper, fill = "red",
                  colour = variable),
    inherit.aes = FALSE, alpha = 0.25, show.legend = FALSE, linetype = 2,
    colour = "red", size = 0.25
  ) +
  theme_light() +
  scale_color_manual(
    values = c("black", "red"), 
    labels = unname(c(
      latex2exp::TeX("PD: $\\eta_t$"),
      latex2exp::TeX("Time-varying mean: $\\tilde{\\eta}_t$")
   ))
  ) +
  theme(
    legend.position = "bottom", 
    legend.title = element_blank(), 
    legend.background = element_rect(fill = "transparent"),
    plot.margin = unit(c(0.1,0.1,0.1,0.1), "cm"),
    plot.title = element_text(hjust = 0.5, size = 8),
    axis.title = element_blank()
  )
```
\newpage

## Cointegration analysis

The connection between economic fundamentals and financial markets will be studied by inspecting possible economcic factors that drive the latent price-dividend ratio. In [@h2_paper] three long determinants of the latent mean of the price-dividend ratio have been found. These three factors form a cointegration relationship with the time-varying mean $\tilde{\eta}_t$: consumption risk, the demographic structure of the population and the dividend payout policy of firms.

As an initial step a similar cointegration analysis will be conducted by analyzing the effect of additional covariates. The previously established connection with consumption risk and the demographic of the population will be added. Overall, the following economic indicators will be analyzed, see \@ref(tab:cointegration-analysis-name-variables).

```{r, cointegration-analysis-name-variables, eval=TRUE, echo=FALSE}
result_table <- data.frame(
  description = c(
    "Consumption Risk",
    "Middle-Aged to Young Ratio",
    "Effective Federal Funds Rate",
    "Log Real M1 Money Supply (Trillions)",
    "Log Real GDP (10 Trillions)"
  )
)
rownames(result_table) <- c("$cr_t$", "$my_t$", "$fr_t$", "$ms_t$", "$gdp_t$")
opts <- options(knitr.kable.NA = "")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("Description"),
  caption = "Cointegration analysis: Covariates", 
  booktabs = TRUE, escape = FALSE,
  linesep = "\\addlinespace", 
  align = rep("l", 1)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```

Annual population data has been collected from Datastream ("US-
POP24Y" for the 20–24-year old agents, "USPOP29Y" for the 25–29, "USPOP44Y"
for the 40–44, and "USPOP49Y" for the 45–49, "USPOPTO" for the total US population). The middle-aged to young ratio has been computed as the ratio of 40-49 to the 20-29 year old agents. Quarterly data between the end of consecutive years has been obtained by applying linear interpolation. 

As has been proposed in [@consumption_risk] consumption risk has been computed as $cr_t = \ln({\sum_{i = 0}^{12}\left | co_{t-i} \right |})$ where $co_{t}$ is the residual from an AR(1) regression for real per capita consumption growth. The real per capita consumption has been computed by taking real personal consumption data from [@real_consumption_data] and the retrieved total US population data from Datastream.

The impact of monetary policy has been of large interest in the asset pricing literature, see [@monetary_policy_asset_prices]. Here, the effective federal funds rate, obtained from [@federal_funds_rate_data], and the log real M1 money supply, retrieved from [@m1_money_supply_data], have been added for establishing a possible link between monetary policy and the latent price-dividend ratio. Furthermore, the log real GDP [@gdp_data] as an indicator of economic growth has been added. 

Figure \@ref(fig:cointegration-analysis-plot-covariates) depicts the time-varying mean $\tilde{\eta}_t$ and all accompanying covariates. The economic effects of the COVID-19 pandemic are directly visible by inspecting the consumption risk $cr_t$, real effective funds rate $fr_t$, log M1 money stock $ms_t$ and log GDP $gdp_t$ time series. As a reaction to the shock that resulted in an sharp increase in consumption risk and large drop in GDP, the Federal Reserve intervened by lowering the federal funds rate, see [@fed_lower_funds_rate], and increasing the money stock by extending quantitative easing, see [@fed_qe].

```{r, cointegration-analysis-plot-covariates, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Cointegration analysis: Visualization of covariates", fig.width=6, fig.height=4, warning=FALSE}
traj <- rbind(
  result[[1]]@filter.traj[1, 5000:20000, ], 
  result[[2]]@filter.traj[1, 5000:20000, ],
  result[[3]]@filter.traj[1, 5000:20000, ]
)

# Plot theme:
p_theme <- theme(
  legend.justification = c(0, 1), legend.position = c(0, 1), 
  legend.title = element_blank(), 
  legend.background = element_rect(fill = "transparent"),
  plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"),
  plot.title = element_text(hjust = 0.5, size = 8),
  axis.title = element_blank(),
  legend.key = element_rect(fill = NA)
)

# e_lpd
df <- data.frame(time = dates, y = colMeans(traj)[-1])
p1 <- ggplot(data = df, aes(x = time, y = y)) +
  geom_line() + theme_light() + p_theme +
  labs(
    title = latex2exp::TeX(
      "Time-varying mean of the PD: \\textbf{$\\tilde{\\eta}_t$}"
    )
  )

# cr
df <- data.frame(time = dates, cr = y$cr)
p2 <- ggplot(data = df, aes(x = time, y = cr)) +
  geom_line() + theme_light() + p_theme +
  labs(title = latex2exp::TeX("Consumption Risk: \\textbf{$cr_t$}"))

# mys
df <- data.frame(time = dates, mys = y$mys)
p3 <- ggplot(data = df, aes(x = time, y = mys)) +
  geom_line() + theme_light() + p_theme +
  labs(title = latex2exp::TeX("Middle-Aged to Young Ratio: \\textbf{$my_t$}"))

# fr
df <- data.frame(time = dates, fr = y$fr)
p4 <- ggplot(data = df, aes(x = time, y = fr)) +
  geom_line() + theme_light() + p_theme +
  labs(title = latex2exp::TeX("Effective Federal Funds Rate: \\textbf{$fr_t$}"))

# ms
df <- data.frame(time = dates, ms = y$ms)
p5 <- ggplot(data = df, aes(x = time, y = ms)) +
  geom_line() + theme_light() + p_theme +
  labs(title = latex2exp::TeX("Real M1 Money Supply: \\textbf{$ms_t$}"))

# Phi
# ms
df <- data.frame(time = dates, gdp = y$gdp)
p6 <- ggplot(data = df, aes(x = time, y = gdp)) +
  geom_line() + theme_light() + p_theme +
  labs(title = latex2exp::TeX("Real GDP: \\textbf{$gdp_t$}"))

grid.arrange(
  p1 + theme(legend.position = "none"), 
  p2 + theme(legend.position = "none"), 
  p3 + theme(legend.position = "none"), 
  p4 + theme(legend.position = "none"), 
  p5 + theme(legend.position = "none"), 
  p6 + theme(legend.position = "none"), 
  ncol = 2
)
```

Before investigating the cointegration relationship the individual variables will be characterized by means of the augmented Dickey-Fuller (ADF) test. Table \@ref(tab:cointegration-analysis-unit-root-tests) shows the test statistics for all variables. For variables in level the test is applied with a constant and drift and for variables in difference only a constant term has been added. All tests have been applied using only one lag.

```{r, cointegration-analysis-unit-root-tests, eval=TRUE, echo=FALSE}
traj <- rbind(
  result[[1]]@filter.traj[1, 5000:20000, ], 
  result[[2]]@filter.traj[1, 5000:20000, ],
  result[[3]]@filter.traj[1, 5000:20000, ]
)
e_lpd <- colMeans(traj)[-1]

# with corona
e_lpd1 <- urca::summary(urca::ur.df(e_lpd, type = "trend", lags = 1))
cr1 <- urca::summary(urca::ur.df(y$cr, type = "trend", lags = 1))
my1 <- urca::summary(urca::ur.df(y$mys, type = "trend", lags = 1))
fr1 <- urca::summary(urca::ur.df(y$fr, type = "trend", lags = 1))
ms1 <- urca::summary(urca::ur.df(y$ms, type = "trend", lags = 1))
gdp1 <- urca::summary(urca::ur.df(y$gdp, type = "trend", lags = 1))

d_e_lpd1 <- urca::summary(urca::ur.df(diff(e_lpd), type = "drift", lags = 1))
d_cr1 <- urca::summary(urca::ur.df(diff(y$cr), type = "drift", lags = 1))
d_my1 <- urca::summary(urca::ur.df(diff(y$mys), type = "drift", lags = 1))
d_d_my1 <- urca::summary(urca::ur.df(diff(diff(y$mys)), type = "drift", lags = 1))
d_fr1 <- urca::summary(urca::ur.df(diff(y$fr), type = "drift", lags = 1))
d_ms1 <- urca::summary(urca::ur.df(diff(y$ms), type = "drift", lags = 1))
d_gdp1 <- urca::summary(urca::ur.df(diff(y$gdp), type = "drift", lags = 1))
with_corona <- list(
  e_lpd1, cr1, my1, fr1, ms1, gdp1, d_e_lpd1, d_cr1, d_my1, d_d_my1, d_fr1, d_ms1, d_gdp1
)

result_table <- data.frame(
  with_corona = sapply(with_corona, function(x){x@teststat[1]}),
  c_1 = sapply(with_corona, function(x){x@cval[1, 1]}),
  c_2 = sapply(with_corona, function(x){x@cval[1, 2]}),
  c_3 = sapply(with_corona, function(x){x@cval[1, 3]})
)
stars <- rev(c("*", "**", "***"))
result_table <- round(result_table, 4)
result_table[, 1] <- sapply(1:13, function(i){paste(
    result_table[i, 1], 
    ifelse(
        is.na(stars[result_table[i, 1] < with_corona[[i]]@cval[1, ]][1]),
        "", stars[result_table[i, 1] < with_corona[[i]]@cval[1, ]][1]
    )
)})

rownames(result_table) <- c(
  "$\\tilde{\\eta}_t$", "$cr_t$", "$my_t$", "$fr_t$", "$ms_t$", "$gdp_t$",
  "$\\Delta\\tilde{\\eta}_t$", "$\\Delta cr_t$", "$\\Delta my_t$", "$\\Delta^2 my_t$",
  "$\\Delta fr_t$", "$\\Delta ms_t$",  "$\\Delta gdp_t$"
)
opts <- options(knitr.kable.NA = "")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("t-statistic", "$\\alpha = 0.01\\%$", "$\\alpha = 0.05\\%$",
                "$\\alpha = 0.1\\%$"),
  caption = "Cointegration analysis: Augmented Dickey-Fuller unit root test", 
  booktabs = TRUE, escape = FALSE,
  linesep = "", 
  align = rep("l", 4)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position") %>%
  kableExtra::add_header_above(c("", "", "Critical Values" = 3)) %>%
  kableExtra::footnote(
    symbol = c("Significance at 1% level", 
               "Significance at 5% level",
               "Significance at 10% level"),
    symbol_manual = rev(c("*", "**", "***")),
    footnote_as_chunk = F
  )
```

The significance results of most factors indicate first order integration at conventional significance levels. Only the effective federal funds rate has a significance level of $10\%$. For this thesis the federal funds rate will be handled as a first order integrated time series. Additionally, the test results for the middle aged to young ratio indicate second order integration. However, for longer time series there exist significant evidence for first order integration, [@long_term_mys], and hence the middle aged to young ratio will be treated as a first order integrated process aswell.

Moreover, the cointegration analysis will be applied using the following single equation error correction model (SECM)
\begin{equation}
\begin{split}
\Delta \tilde{\eta}_t = &\delta_0 + \alpha(\tilde{\eta}_{t-1} - \beta_1 cr_{t-1} - \beta_2 my_{t-1} - \beta_3 fr_{t-1} - \beta_4 ms_{t-1} - \beta_5 gdp_{t-1}) + \\
& \delta_1 \Delta cr_{t} + \delta_2 \Delta my_{t} + \delta_3 \Delta fr_{t} + \beta_4 \Delta ms_{t} + \beta_5 \Delta gdp_{t} + \phi_1\Delta \tilde{\eta}_{t-1} + \phi_2\Delta \tilde{\eta}_{t-2} + e_t
(\#eq:secm)
\end{split}
\end{equation}
Table \@ref(tab:cointegration-analysis-study) displays the results of the inspected cointegration relationship with parameter estimates evaluated based on a nonlinear regression approach. 
```{r, cointegration-analysis-study, eval=TRUE, echo=FALSE}
traj <- rbind(
  result[[1]]@filter.traj[1, 5000:20000, ], 
  result[[2]]@filter.traj[1, 5000:20000, ],
  result[[3]]@filter.traj[1, 5000:20000, ]
)

data <- zoo::as.zoo(data.frame(
  e_lpd = colMeans(traj)[-1], cr = y$cr, mys = y$mys, fr = y$fr, ms = y$ms, 
  gdp = y$gdp
), order.by = zoo::index(1:length(y$cr)))

# complete analysis
ardl_model_1 <- ARDL::ardl(
  e_lpd ~ cr + mys + fr + ms + gdp, order = c(3, 1, 1, 1, 1, 1), data = data
)
cointegration_model_1 <- ARDL::uecm(ardl_model_1)
m_1 <- ARDL::multipliers(cointegration_model_1)
cointegration_model_1 <- summary(cointegration_model_1)

# drop cr, fr, gdp
ardl_model_2 <- ARDL::ardl(
  e_lpd ~ mys + ms, order = c(3, 1, 1), data = data
)
cointegration_model_2 <- ARDL::uecm(ardl_model_2)
m_2 <- ARDL::multipliers(cointegration_model_2)
cointegration_model_2 <- summary(cointegration_model_2)

result_table <- data.frame(
  complete = c(
    cointegration_model_1$coefficients[2, 1], m_1[2:6, 2],
    cointegration_model_1$r.squared, cointegration_model_1$adj.r.squared
  ),
  droped = c(
    cointegration_model_2$coefficients[2, 1], NA, m_2[2, 2], NA, m_2[3, 2], NA, 
    cointegration_model_2$r.squared, cointegration_model_2$adj.r.squared
  )
)
result_table <- round(result_table, 4)
stars <- rev(c("*", "**", "***"))

# Cointegration test
result_table[1, 1] <- paste(
  result_table[1, 1], 
  ifelse(
    is.na(
      stars[cointegration_model_1$coefficients[2, 3] < c(-4.95756, -4.41519, -4.41315)][1]
    ),
    "", 
    stars[cointegration_model_1$coefficients[2, 3] < c(-4.95756, -4.41519, -4.41315)][1]
  )
)
result_table[1, 2] <- paste(
  result_table[1, 2], 
  ifelse(
    is.na(
      stars[cointegration_model_2$coefficients[2, 3] < c(-3.89644, -3.33613, -3.04445)][1]
    ),
    "", 
    stars[cointegration_model_2$coefficients[2, 3] < c(-3.89644, -3.33613, -3.04445)][1]
  )
)

# Long run relationship test
result_table[2:6, 1] <- sapply(2:6, function(i){paste(
    result_table[i, 1],
    ifelse(
        is.na(stars[m_1[i, 5] < c(0.01, 0.05, 0.1)][1]),
        "", stars[m_1[i, 5] < c(0.01, 0.05, 0.1)][1]
    )
)})
result_table[c(3, 5), 2] <- sapply(c(3, 5), function(i){paste(
    result_table[i, 2],
    ifelse(
        is.na(stars[m_2[ifelse(i == 3, 2, 3), 5] < c(0.01, 0.05, 0.1)][1]),
        "", stars[m_2[ifelse(i == 3, 2, 3), 5] < c(0.01, 0.05, 0.1)][1]
    )
)})
rownames(result_table) <- c(
  "$\\alpha$", "$\\beta_1$", "$\\beta_2$", "$\\beta_3$",
  "$\\beta_4$", "$\\beta_5$", "$R^2$", "adj $R^2$"
)
opts <- options(knitr.kable.NA = "-")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("All covariates", "Excluding $cr_t$, $fr_t$, $gdp_t$"),
  caption = "Cointegration analysis: Results", 
  booktabs = TRUE, escape = FALSE,
  linesep = "", 
  align = rep("l", 2)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position") %>%
  kableExtra::footnote(
    symbol = c("Significance at 1% level", 
               "Significance at 5% level",
               "Significance at 10% level"),
    symbol_manual = rev(c("*", "**", "***")),
    footnote_as_chunk = F
  )
```

Test statistics for the adjustment coefficient $\alpha$ are used for testing a possible cointegration relationship. The model including all covariates has no significant cointegration relationship based on a $10\%$ singificance level. The statistic of $-3.07$ is larger than the critical value $-4.41$ at the $10\%$ level, obtained from the surface regression approach by [@cointegration_critical_values]. 

Furthermore, the test has been repeated by removing the insignificant covariates $cr_t$, $fr_t$ and $gdp_t$. Again the cointegration relationship is insignificant at a $10\%$ level. However, the test statistic $-2.998$ is larger but relatively close to the critical value $-3.04$. 

Overall the cointegration analysis has been applied under the assumption that the latent mean of the price-dividend ratio follows a random walk state process. Contrary to the results presented in [@h2_paper] a cointegration relationship could no be established. Besides assuming that this connection is not present for the collected data multiple reasons are available for the mixing results. Firstly, it is possible that the state process is misspecified and another specification, possbily already including covariates, is more suitable. Additionally, the impact of the missing dividend payout policy factor used in [@h2_paper] is not accounted here. Moreover, it is possible that the cointegration relationship is only present for data with larger frequency than quarterly. Furthermore, different data sources and a shorter time frame have been used. 

## State process extension with covariates

As mentioned, it is possible that the filtered latent mean of the price-dividend ratio could be misspecified by using a random walk state process. Hence, this chapter will continue the analysis of possible exogenous covariates effecting the latent process.

Three additional models with different state process specifications will be tested and the results will be compared with the random walk model. Each model will contain the previously described covariates. Firstly, the model extensions and the estimation results will be presented. Then the latent state estimates will be compared and model comparison performed.

Common similarities between all approaches will be that the same parameter transformations as in the [simulation study][Simulation study] chapter will be applied. Hence, the hyperparameters $\psi$, $\varsigma_\epsilon$ and $\varsigma_u$ will be used. Moreover, all PMMH approaches will be run in parallel using 3 independent Markov chains, $J = 1000$ particles and $M = 20000$ iterations. Similar to the random walk state process a flat prior specification will be used and the first $5000$ iterations of the PMMH approach will be dropped.

The first model (**Model 1**) describes a long run equilibrium relationship that could not be detected within the [cointegration analysis][Cointegration analysis]
\begin{equation}
\begin{split}
\tilde{\eta}_t &= \beta_0 + \beta_1 cr_{t} + \beta_2 my_{t} + \beta_3 fr_{t} + \beta_4 ms_{t} + \beta_5 gdp_{t} + u_t \\
u_t &\sim \mathcal{N}(0, \sigma_u^2)
(\#eq:static-model-state-process)
\end{split}
\end{equation}
Combined with the observation process \@ref(eq:observation-process) the necessary parameters are $\theta = \begin{pmatrix} \beta_0 & \beta_1 & \beta_2 & \beta_3 & \beta_4 & \beta_5 & \sigma_{\epsilon} & \sigma_{u}\end{pmatrix}^T$. Note that due to the missing autoregressive part of the model the initial value $\tilde{\eta}_0$ does not need to be included.

Furthermore, as initial parameters $\theta_0$ the parameter estimates for the complete single equation error correction model from Table \@ref(tab:cointegration-analysis-study) will be used
$$
\theta_0 = \begin{pmatrix}\beta_0^{(0)} \\ \beta_1^{(0)} \\ \beta_2^{(0)} \\ \beta_3^{(0)} \\ \beta_4^{(0)} \\ \beta_5^{(0)} \\ \varsigma_\epsilon^{(0)} \\ \varsigma_u^{(0)} \end{pmatrix} = \begin{pmatrix} 2.75 \\ 0.18 \\ 1.86 \\ 0.75 \\0.69 \\ -0.15 \\ \ln(0.05) \\ \ln(0.09) \end{pmatrix}
$$
The second setup (**Model 2**) is an extension of the previous model \@ref(eq:static-model-state-process) and handles possibly correlated errors by adding to the setup an AR(1) structure
\begin{equation}
\begin{split}
\tilde{\eta}_t &= \beta_0 + \beta_1 cr_{t} + \beta_2 my_{t} + \beta_3 fr_{t} + \beta_4 ms_{t} + \beta_5 gdp_{t} + v_t \\
v_t &= \phi v_{t-1} + u_t \\
u_t &\sim \mathcal{N}(0, \sigma_u^2)
(\#eq:ar-error-model-state-process)
\end{split}
\end{equation}
The parameter $\phi$ will be restricted for stationarity in the error process, $\left| \phi \right| < 1$. Similar starting values as in the previous setup are applied
$$
\theta_0 = \begin{pmatrix}\beta_0^{(0)} \\ \beta_1^{(0)} \\ \beta_2^{(0)} \\ \beta_3^{(0)} \\ \beta_4^{(0)} \\ \beta_5^{(0)} \\ \psi \\ \varsigma_\epsilon^{(0)} \\ \varsigma_u^{(0)} \end{pmatrix} = \begin{pmatrix} 2.75 \\ 0.18 \\ 1.86 \\ 0.75 \\0.69 \\ -0.15 \\ \text{atanh}(0.95) \\ \ln(0.05) \\ \ln(0.05) \end{pmatrix}
$$
Moreover, for both models the following proposal distribution is used
$$\theta_i^{*}\sim \mathcal{N}(\theta_i^{(m-1)}, 0.02^2)$$
As has been shown in the [simulation study][Simulation study], autoregressive distributed lag models are closely related to error correction models. Hence, building upon the results presented in the [cointegration analysis][Cointegration analysis] the next state process specification (**Model 3**) will be the following autoregressive distributed lag model
\begin{equation}
\begin{split}
\tilde{\eta}_t &= \beta_0 + \phi \tilde{\eta}_{t-1} + \beta_1 cr_{t} + \beta_2 my_{t} + \beta_3 fr_{t} + \beta_4 ms_{t} + \beta_5 gdp_{t} + u_t \\
u_t &\sim \mathcal{N}(0, \sigma_u^2)
(\#eq:ardl-model-state-process)
\end{split}
\end{equation}
Compared to model \@ref(eq:ar-error-model-state-process) the state process of the ARDL model is linear in parameters. The starting values for this approach are derived by obtaining the corresponding parameter estimates of an ARDL model specification of the complete single equation error correction model from Table \@ref(tab:cointegration-analysis-study)
$$
\theta_0 = \begin{pmatrix}\beta_0^{(0)} \\ \beta_1^{(0)} \\ \beta_2^{(0)} \\ \beta_3^{(0)} \\ \beta_4^{(0)} \\ \beta_5^{(0)} \\ \psi \\ \varsigma_\epsilon^{(0)} \\ \varsigma_u^{(0)} \end{pmatrix} = \begin{pmatrix} 2.75 \\ 0.18 \\ 1.86 \\ 0.75 \\0.69 \\ -0.15 \\ \text{atanh}(0.95) \\ \ln(0.05) \\ \ln(0.05) \end{pmatrix}
$$
Here the variance for the proposal distribution will be different for the parameters $\psi, \varsigma_u, \varsigma_\epsilon$ with $\lambda = 0.015^2$ and for the other parameters $\lambda = 0.001^2$.

Figures \@ref(fig:extension-static-model-trace), \@ref(fig:extension-ar-error-model-trace) and \@ref(fig:extension-ardl-model-trace) show the trace plots for each model and for each parameter and each Markov chain. Furthermore, figures \@ref(fig:extension-static-model-marginal-density), \@ref(fig:extension-ar-error-model-marginal-density) and \@ref(fig:extension-ardl-model-marginal-density) display the Gaussian kernel density estimates of the marginal posterior distributions. As can be seen ...

Table \@ref(tab:pmmh-estimation-result) summarizes the estimation results for all models.
```{r, pmmh-estimation-result, eval=TRUE, echo=FALSE}
library(tidyverse)

r <- readRDS(
  here::here("data", "results", "model_study_result_basic_model.rds")
)
N <- ncol(r[[1]]@data)
model_1 <- lapply(r, function(x){x@traces[5000:20000, ]})
model_1 <- do.call(rbind, model_1)

result_table <- data.frame(
  model_1_mean = c(
    NA, NA, NA, NA, NA, NA, NA, mean(model_1[, 3]), 
    mean(exp(model_1[, 4])), mean(exp(model_1[, 5]))
  ),
  model_1_lower = c(
    NA, NA, NA, NA, NA, NA, NA, quantile(model_1[, 3], probs = 0.025), 
    quantile(exp(model_1[, 4]), probs = 0.025), 
    quantile(exp(model_1[, 5]), probs = 0.025)
  ),
  model_1_upper = c(
    NA, NA, NA, NA, NA, NA, NA, quantile(model_1[, 3], probs = 0.975), 
    quantile(exp(model_1[, 4]), probs = 0.975), 
    quantile(exp(model_1[, 5]), probs = 0.975)
  ),
  model_2_mean = c(
    NA, NA, NA, NA, NA, NA, NA, NA, NA, NA
  ),
  model_2_lower = c(
    NA, NA, NA, NA, NA, NA, NA, NA, NA, NA
  ),
  model_2_upper = c(
    NA, NA, NA, NA, NA, NA, NA, NA, NA, NA
  ),
  model_3_mean = c(
    NA, NA, NA, NA, NA, NA, NA, NA, NA, NA
  ),
  model_3_lower = c(
    NA, NA, NA, NA, NA, NA, NA, NA, NA, NA
  ),
  model_3_upper = c(
    NA, NA, NA, NA, NA, NA, NA, NA, NA, NA
  )
)
result_table <- data.frame(sapply(result_table, as.numeric))

result_table <- round(result_table, 4)
rownames(result_table) <- c(
  "$\\phi$", "$\\beta_0$", "$\\beta_1$", "$\\beta_2$", "$\\beta_3$", 
  "$\\beta_4$", "$\\beta_5$", "$\\tilde{\\eta}_0$", "$\\sigma_u$", 
  "$\\sigma_\\epsilon$"
)
opts <- options(knitr.kable.NA = "")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = rep(c("mean", "$0.025\\%$", "$0.975\\%$"), 3),
  caption = "Model extension: PMMH results", 
  booktabs = TRUE, escape = FALSE,
  linesep = "", 
  align = rep("l", 4)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position") %>%
  kableExtra::add_header_above(
    c("", "Model 1" = 3, "Model 2" = 3, "Model 3" = 3)
  )
```
Table \@ref(tab:pmmh-model-comparison) summarizes the model comparison results for all models.
```{r, pmmh-model-comparison, eval=TRUE, echo=FALSE}
r <- readRDS(
  here::here("data", "results", "model_study_result_basic_model.rds")
)
N <- ncol(r[[1]]@data)
rw_model <- lapply(r, function(x){x@traces[5000:20000, ]})
rw_model <- do.call(rbind, rw_model)

result_table <- data.frame(
  num_params = c(3, NA, NA, NA),
  lik = c(mean(rw_model[, 1]), NA, NA, NA),
  bic = c(-2*mean(rw_model[, 1]) +  log(N)*3, NA, NA, NA)
)

rownames(result_table) <- c(
  "RW Model", "Model 1", "Model 2", "Model 3"
)
result_table <- round(result_table, 4)
opts <- options(knitr.kable.NA = "")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("Number of parameters", "log-lik", "BIC"),
  caption = "Model extension: Model comparison", 
  booktabs = TRUE, escape = FALSE,
  linesep = "", 
  align = rep("l", 4)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```
