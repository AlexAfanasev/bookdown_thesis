---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Empirical application {#application}

For the empirical application the presented PMMH procedure will be applied for modeling the time-varying mean of the PD. This application will be an extension of the presented results and procedures by @h2_paper. Besides applying the Bayesian approach as an alternative, a possible state process extension will be tested. First, the necessary steps for deriving the state-space model will be explained.

## Model implementation

Starting from the basic definition of the total log return at time $t+1$, $r_{t+1}$,  with price $P_{t}$ and dividend $D_t$, the log return can be reformulated as a nonlinear function of the PD, $\eta_t$. First subtracting $r_{t+1}$ from itself yields
\begin{equation*}
\begin{split}
r_{t+1} - r_{t+1} &= \ln\Bigg(\frac{P_{t+1} + D_{t+1}}{P_{t}}\Bigg) - \ln\Bigg(\frac{P_{t+1} + D_{t+1}}{P_{t}}\Bigg) \\
0 &= \ln\Bigg(\frac{P_{t+1} + D_{t+1}}{P_{t}}\frac{P_t}{P_{t+1} + D_{t+1}}\Bigg)
\end{split}
\end{equation*}
Then adding the PD, $\eta_t = \ln(P_t) - \ln(D_t)$, to both sides of the equation
\begin{equation*}
\begin{split}
\eta_{t} &= \ln\Bigg(\frac{P_t}{D_t}\frac{P_{t+1} + D_{t+1}}{P_{t}}\Bigg) - r_{t+1} = \ln\Bigg(\Bigg(1 + \frac{P_{t+1}}{D_{t+1}}\Bigg)\frac{D_{t+1}}{D_{t}}\Bigg) - r_{t+1}
\end{split}
\end{equation*}
and simplifying the above yields
\begin{equation*}
\begin{split}
\eta_{t} &= \ln(1 + \text{exp}(\eta_{t+1})) - r_{t+1} + \Delta d_{t+1} \\
r_{t+1} &= -\eta_{t} + \ln(1 + \text{exp}(\eta_{t+1})) + \Delta d_{t+1}
\end{split}
\end{equation*}
Here, $\Delta d_{t+1} = \ln(D_{t+1}) - \ln(D_t)$ represents the log dividend growth. Applying a first order Taylor expansion around the fixed steady state $\bar{\eta}$ for $\tilde{\eta}_{t+1}$ yields a linear approximation
\begin{equation}
\begin{split}
r_{t+1} &\approx -\eta_{t} + \ln(1 + \text{exp}(\bar{\eta})) + \Delta d_{t+1} + \frac{1}{1 + \text{exp}(-\bar{\eta})}(\eta_{t+1}-\bar{\eta}) \\
&\approx k -\eta_{t} + \rho\eta_{t+1} + \Delta d_{t+1}
\end{split}
\end{equation}
With $\rho$ being specified as
\begin{align}
\rho &= \frac{1}{1+\text{exp}(-\bar{\eta})} & \Longleftrightarrow && \bar{\eta} &= -\ln\Bigg(\frac{1}{\rho} - 1\Bigg)
(\#eq:rho-spec)
\end{align}
and $k$ directly following from the above
\begin{equation}
\begin{split}
k &= \ln(1 + \text{exp}(\bar{\eta})) - \bar{\eta}\frac{1}{1 + \text{exp}(-\bar{\eta})} \\
&= \ln\Bigg(1 + \text{exp}\Bigg(-\ln\Bigg(\frac{1}{\rho} - 1\Bigg)\Bigg)\Bigg) + \ln\Bigg(\frac{1}{\rho} - 1\Bigg)\frac{1}{1 + \text{exp}\Big(\ln\Big(\frac{1}{\rho} - 1\Big)\Big)} \\
&= -\ln(1 - \rho) + \rho\ln\Bigg(\frac{1}{\rho} - 1\Bigg) \\
&= -\ln(\rho) - (1 - \rho)\ln\Bigg(\frac{1}{\rho} - 1\Bigg) 
(\#eq:k-spec)
\end{split}
\end{equation}
Previous empirical applications typically assumed a constant parameter $\bar{\eta}$, as done by @campbell_paper. However, the results presented by @h2_paper deliver evidence for a gradually time-varying mean of the PD. By allowing this time variation, $k$ and $\rho$ become also time-varying. Hence, the corresponding parameters $k_t$ and $\rho_t$ are obtained from Equation \@ref(eq:rho-spec) and Equation \@ref(eq:k-spec)
\begin{equation}
\begin{split}
\rho_t &= \frac{1}{1+\text{exp}(-\tilde{\eta}_t)} \\
k_t &= -\ln(\rho_t) - (1 - \rho_t)\ln\Bigg(\frac{1}{\rho_t} - 1\Bigg)
\end{split}
\end{equation}
with $\tilde{\eta}_t$ denoting the time-varying local mean for the Taylor approximation. Therefore, the PD has the following specification
\begin{equation}
\begin{split}
\eta_{t} &\approx k_t - r_{t+1} + \rho_t\eta_{t+1} + \Delta d_{t+1}
(\#eq:lpd-spec)
\end{split}
\end{equation}
Similar approximations as by @van_nieuwerburgh_paper are adopted: $\mathbb{E}_t[\rho_{t+i}]\approx\rho_t$, $\mathbb{E}_t[k_{t+i}]\approx\;k_t$ and $\mathbb{E}_t[\rho_{t+i}\eta_{t+1+i}]\approx\mathbb{E}_t[\rho_{t+i}]\mathbb{E}_t[\eta_{t+1+i}]$. The present-value formulation of the PD can then be concluded by taking the conditional expectation and iterating Equation \@ref(eq:lpd-spec) forward
\begin{equation}
\begin{split}
\eta_{t} &\approx k_t - \mathbb{E}_t[r_{t+1}] + \rho_t\mathbb{E}_t[\eta_{t+1}] + \mathbb{E}_t[\Delta d_{t+1}] \\
 &\approx k_t - \mathbb{E}_t[r_{t+1}] + \rho_t\mathbb{E}_t[k_{t+1} - r_{t+2} + \rho_{t+1}\eta_{t+2} + \Delta d_{t+2}] + \mathbb{E}_t[\Delta d_{t+1}] \\
 &\approx \cdots \\
 &\approx \frac{k_t}{1-\rho_t} + \sum_{i=1}^{\infty}\rho_t^{i-1}\mathbb{E}_t[\Delta d_{t+i}^e - r_{t+i}^e] + \lim\limits_{i \rightarrow \infty}\rho_t^i\mathbb{E}_t[\eta_{t+i}]
(\#eq:present-value)
\end{split}
\end{equation}
Here, the excess dividend growth $\Delta d_{t}^e = \Delta d_t - r^f_t$ and return $r_t^e = r_t - r_t^f$ are used with $r^f_t$ being the risk-free interest rate.

A nonlinear state-space model will be used to filter the latent mean process. As presented by @h2_paper, the present-value formulation \@ref(eq:present-value) will be used as the observation process by adding an error term $\epsilon_t \sim\mathcal{N}(0, \sigma_{\epsilon}^2)$ that captures rational bubbles, approximation errors and other influences in $\lim\limits_{i \rightarrow \infty}\rho_t^i\mathbb{E}_t[\eta_{t+i}]$
\begin{equation}
\eta_{t} = \frac{k_t}{1-\rho_t} + \sum_{i=1}^{\infty}\rho_t^{i-1}\mathbb{\tilde{E}}_t[\Delta d_{t+i}^e - r_{t+i}^e] + \epsilon_t
 (\#eq:observation-process)
\end{equation}
A low dimensional vector autoregressive (VAR) model of order $1$ will be used for the objective expectations $\mathbb{\tilde{E}}_t$ conditional on information available at $t$. This approach was first proposed by @campbell_shiller_paper. The VAR model will be comprised of the PD $\eta_t$, excess dividend growth $\Delta d_t^e$, excess return $r_t^e$ and inflation $\pi_t$.

Assuming the following VAR model
\begin{equation}
\begin{split}
y_t &= \begin{pmatrix} \eta_t \\ \Delta d_t^e \\ r_t^e \\ \pi_t \end{pmatrix} = \alpha + Ay_{t-1} + v_t \\
v_t &\sim \mathcal{N}_4(0, \Sigma)
 (\#eq:ex-ante-expectations-var-model)
\end{split}
\end{equation}
and the following reparametrization by stacking the vector of constants $\alpha$ on to the parameter matrix $A$
\begin{equation}
y_t = \begin{pmatrix} \eta_t \\ \Delta d_t^e \\ r_t^e \\ \pi_t \\ 1 \end{pmatrix} = \left(
\begin{array}{c|c}
        A & \alpha \\
        \hline
        0 & 1\\
\end{array}
\right)y_{t-1} + \begin{pmatrix}v_t \\ 0\end{pmatrix} = B y_{t-1} + \begin{pmatrix}v_t \\ 0\end{pmatrix}
\end{equation}
The discounted objective expectations can be evaluated using the vector $h = \begin{pmatrix} 0 & 1 & -1 & 0 & 0\end{pmatrix}^T$
\begin{equation}
\begin{split}
\sum_{i=1}^{\infty}\rho_t^{i-1}\mathbb{\tilde{E}}_t[\Delta d_{t+i}^e - r_{t+i}^e] &= \sum_{i=1}^{\infty}\rho_t^{i-1}h^TB^iy_{t} \\
& = h^TB\sum_{i=0}^{\infty}\rho_t^{i}B^iy_{t} = h^TB(I_5 -\rho_t B)^{-1}y_t
 (\#eq:ex-ante-expectations)
\end{split}
\end{equation}
Similar to the procedure proposed by @h2_paper, a adaptive approach will be used for the choice of VAR model sample size. Starting at the forecasting origin, observation $t = 30$, VAR models with samples $\Omega_{t,\omega} = \{\eta_\tau, \Delta d^e_\tau, r_\tau^e, \pi_\tau\;|\;\tau = t-\omega+1, ..., t\}$ and varying lengths $\omega = (10, ..., 30)$ will be fitted. The selection criterion will be the root mean squared error of the last five in sample observations of the difference of the excess dividend rate and return, $\{\Delta d_m^e - r_m^e\}^{m=t}_{m=t-4}$. Additionally, the stability of each VAR model will be checked, at each length $\omega$, by inspecting that all eigenvalues of matrix $A$, from Equation \@ref(eq:ex-ante-expectations-var-model), have modulus less than 1. If this requirement is violated, the specific VAR model will be discarded. If at a specific time point $t$ all VAR models violate the stability requirement, the VAR model with $\omega = 10$ will be used.

The empirical application will be conducted on quarterly data for the S&P 500 starting at the 30th of September 1963 up until the 30th of September 2020, with the forecast origin being the 31st of December 1970. The 10-year treasury constant maturity rate has been used for the risk free rate $r^f_t$ and the inflation rate $\pi_t$ has been derived from the consumer price index for all urban consumers. These time series, and the price and dividend data for the S&P 500, can be retrieved from Robert J. Shiller's website [@shiller_website].

## Random walk state process

Firstly, the state-space model with a random walk state process (**RW Model**) will be analyzed. The latent process follows
\begin{equation}
\begin{split}
\tilde{\eta}_t &= \tilde{\eta}_{t-1} + u_t \\
u_t &\sim \mathcal{N}(0, \sigma_u^2)
(\#eq:basic-model-state-process)
\end{split}
\end{equation}
The parameters of interest are 
$$\theta = \begin{pmatrix} \tilde{\eta}_0 & \sigma_{\epsilon} & \sigma_{u}\end{pmatrix}^T$$ 
with $\tilde{\eta}_0$ being the initial value. Similar to the simulation study uninformative setup, flat priors will be used for the parameters and initial value. Equal parameter transformations will be applied
\begin{align*}
\sigma_\epsilon(\varsigma_\epsilon) &= \text{exp}(\varsigma_\epsilon) & \Longleftrightarrow && \varsigma_\epsilon(\sigma_\epsilon) &= \ln(\sigma_\epsilon) \\
\sigma_u(\varsigma_u) &= \text{exp}(\varsigma_u) & \Longleftrightarrow && \varsigma_\epsilon(\sigma_u) &= \ln(\sigma_u)
\end{align*}
Hence, after having applied the change-of-variable technique the prior specification for the hyperparameters $\{\varsigma_\epsilon, \varsigma_u\}$ becomes
\begin{equation}
\begin{split}
  p_{\varsigma_\epsilon}(\varsigma_\epsilon) &= \text{exp}(\varsigma_\epsilon) \\
  p_{\varsigma_u}(\varsigma_u) &= \text{exp}(\varsigma_u)
\end{split}
\end{equation}
For the PMMH procedure three independent Markov chains with $M = 50000$ iterations and $J = 1000$ particles will be run and the starting parameter $\theta^{(0)}$ will be set to 
$$
\theta^{(0)} = \begin{pmatrix}\tilde{\eta}_0^{(0)} \\ \varsigma_\epsilon^{(0)} \\ \varsigma_u^{(0)} \end{pmatrix} = \begin{pmatrix}3.5 \\ \ln(0.05) \\ \ln(0.05) \end{pmatrix}
$$
Furthermore, the parameter proposals will be generated using
$$
\theta_i^{*}\sim \mathcal{N}(\theta_i^{(m-1)}, 0.02^2)
$$
Figure \@ref(fig:basic-model-trace-plot) shows the trace plots for each parameter and each Markov chain. The first 10000 iterations have been dropped. Additionally, Figure \@ref(fig:basic-model-posterior-marginal-density) displays the Gaussian kernel density estimates of the marginal posterior distributions of the random walk latent state model parameters. The corresponding Table \@ref(tab:basic-model-setup) summarizes the inference results.

```{r, basic-model-setup, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
source(here::here("R", "prepare_raw_data.R"))
source(here::here("R", "model_comparison", "model_comparison_basic_model.R"))

source(here::here("R", "pd_pomp.R"))
y <- read.csv(here::here("data", "final_dataset.csv"))

rw_latent_lpd_pomp <- pomp::pomp(
    data = y[, c(1, 2)],
    times = "time",
    t0 = 0,
    rinit = function(e_lpd_0, ...) {
        return(c(e_lpd = e_lpd_0))
    },
    rprocess = pomp::discrete_time(
        pomp::Csnippet("e_lpd = e_lpd + rnorm(0, exp(sigma_u));"),
        delta.t = 1
    ),
    dmeasure = rw_latent_lpd_dmeasure,
    statenames = c("e_lpd"),
    paramnames = c("e_lpd_0", "sigma_u", "sigma_e"),
    covar = pomp::covariate_table(rbind(0, y[, -c(2, 6, 7, 8, 9)]),
                                  times = "time"),
    covarnames = colnames(rbind(0, y[, -c(1, 2, 6, 7, 8, 9)]))
)

pf <- pomp::pfilter(rw_latent_lpd_pomp, 
                    params = c(e_lpd_0 = 3.361, sigma_u = log(0.0573), 
                               sigma_e = log(0.0189)),
                    Np = 3000, filter.mean = TRUE)

library(tidyverse)
library(gridExtra)

traces <- rbind(
        result[[1]]@traces[5000:20000, 3:5], 
        result[[2]]@traces[5000:20000, 3:5], 
        result[[3]]@traces[5000:20000, 3:5]
)
traces[, 2:3] <- exp(traces[, 2:3])

result_table <- data.frame(
  mean = colMeans(traces),
  median = apply(traces, 2, median),
  lower = apply(traces, 2, quantile, probs = 0.025),
  upper = apply(traces, 2, quantile, probs = 0.975)
)
rownames(result_table) <- c(
  "$\\tilde{\\eta}_0$", "$\\sigma_u$", "$\\sigma_\\epsilon$"
)
result_table <- round(result_table, 4)
opts <- options(knitr.kable.NA = "")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("Mean", "Median", "0.025\\%", "0.975\\%"),
  caption = "Random walk model: PMMH parameter inference results", 
  booktabs = TRUE, escape = FALSE,
  linesep = "\\addlinespace", 
  align = rep("c", 4)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```

Moreover, Figure \@ref(fig:basic-model-state-estimates) visualizes the marginal posterior mean for the latent state and corresponding $95\%$ pointwise credible intervals. One can directly see that the estimated latent mean captures structural changes in the PD. Additionally, the latent state estimate from applying the particle filter is depicted. This estimate has been derived using $J = 3000$ particles and as parameters the marginal posterior means from Table \@ref(tab:basic-model-setup). As visible the state estimates between the PMMH approach and the particle filter vary, with the particle filter based estimate capturing structural changes later than the PMMH based estimate.

```{r, basic-model-state-estimates, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Random walk model: Filtered latent state", fig.width=6, fig.height=4, warning=FALSE}
traj <- rbind(
  result[[1]]@filter.traj[1, 10000:50000, ], 
  result[[2]]@filter.traj[1, 10000:50000, ],
  result[[3]]@filter.traj[1, 10000:50000, ]
)

df <- data.frame(time = dates, y = y$lpd, x_pmmh = colMeans(traj)[-1], 
                 x_pf = pf@filter.mean[1, ])
df <- reshape2::melt(df, id.vars = "time")
q <- data.frame(
  time = dates,
  variable = "x_pmmh",
  lower = apply(traj, 2, quantile, probs = 0.025)[-1],
  upper = apply(traj, 2, quantile, probs = 0.975)[-1]
)
ggplot(data = df, aes(x = time, y = value, group = variable)) +
  geom_line(aes(color = variable)) + 
  geom_ribbon(
    data = q, aes(x = time, ymin = lower, ymax = upper, fill = "red",
                  colour = variable),
    inherit.aes = FALSE, alpha = 0.25, show.legend = FALSE, linetype = 2,
    colour = "red", size = 0.25
  ) +
  theme_light() +
  scale_color_manual(
    values = c("black", "red", "blue"), 
    labels = unname(c(
      latex2exp::TeX("PD: $\\eta_t$"),
      latex2exp::TeX("PMMH: Time-varying mean $\\tilde{\\eta}_t$"),
      latex2exp::TeX("PF: Time-varying mean $\\tilde{\\eta}_t$")
   ))
  ) +
  theme(
    legend.position = "bottom", 
    legend.title = element_blank(), 
    legend.background = element_rect(fill = "transparent"),
    plot.margin = unit(c(0.1,0.1,0.1,0.1), "cm"),
    plot.title = element_text(hjust = 0.5, size = 8),
    axis.title = element_blank(),
    axis.text.x = element_text(size = 7),
    axis.text.y = element_text(size = 7)
  )
```

## Cointegration analysis

The connection between economic fundamentals and financial markets will be studied by inspecting possible economic factors that drive the latent mean of the PD. @h2_paper have found three long-term determinants of the latent mean of the PD. These three factors form a cointegration relationship with the time-varying mean $\tilde{\eta}_t$: consumption risk, the demographic structure of the population and the dividend payout policy of firms.

A similar cointegration analysis will be conducted by analyzing the effect of additional covariates. The previously established connection with consumption risk and the demographic structure of the population will be added. Overall, the following economic indicators will be analyzed, see Table \@ref(tab:cointegration-analysis-name-variables).

```{r, cointegration-analysis-name-variables, eval=TRUE, echo=FALSE}
result_table <- data.frame(
  description = c(
    "Consumption Risk",
    "Middle-Aged to Young Ratio",
    "Effective Federal Funds Rate",
    "Log Real M1 Money Supply (Trillions)",
    "Log Real GDP (10 Trillions)"
  )
)
rownames(result_table) <- c("$cr_t$", "$my_t$", "$fr_t$", "$ms_t$", "$gdp_t$")
opts <- options(knitr.kable.NA = "")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("Description"),
  caption = "Cointegration analysis: Exogenous covariates", 
  booktabs = TRUE, escape = FALSE,
  align = rep("l", 1)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```

Annual population data has been collected from Datastream ("US-
POP24Y" for the 20–24-year old agents, "USPOP29Y" for the 25–29, "USPOP44Y"
for the 40–44, and "USPOP49Y" for the 45–49, "USPOPTO" for the total US population). The middle-aged to young ratio has been computed as the ratio of 40-49 to the 20-29 year old agents. Quarterly data between the end of consecutive years has been obtained by applying linear interpolation. 

As has been proposed in [@consumption_risk], consumption risk has been computed as $cr_t = \ln({\sum_{i = 0}^{12}\left | co_{t-i} \right |})$ where $co_{t}$ is the residual from an autoregressive (AR) order one regression for real per capita consumption growth. The real per capita consumption has been computed by taking real personal consumption data from [@real_consumption_data] and the retrieved total US population data from Datastream.

The impact of monetary policy has been of large interest in the asset pricing literature, see [@monetary_policy_asset_prices]. Here, the effective federal funds rate, obtained from [@federal_funds_rate_data], and the log real M1 money supply, retrieved from [@m1_money_supply_data], have been added for establishing a possible link between monetary policy and the latent mean of the PD. Furthermore, the log real GDP [@gdp_data] as an indicator of economic growth has been added. 

Figure \@ref(fig:cointegration-analysis-plot-covariates) depicts the time-varying mean $\tilde{\eta}_t$ and all accompanying exogenous covariates. The economic effects of the COVID-19 pandemic are directly visible by inspecting the consumption risk $cr_t$, real effective funds rate $fr_t$, log M1 money supply $ms_t$ and log real GDP $gdp_t$ time series. As a reaction to the shock that resulted in an sharp increase in consumption risk and large drop in GDP, the Federal Reserve intervened by lowering the federal funds rate, see [@fed_lower_funds_rate], and increasing the money supply by extending quantitative easing, see [@fed_qe].

```{r, cointegration-analysis-plot-covariates, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Cointegration analysis: Visualization of exogenous covariates", fig.width=6, fig.height=6.5, warning=FALSE}
traj <- rbind(
  result[[1]]@filter.traj[1, 10000:50000, ], 
  result[[2]]@filter.traj[1, 10000:50000, ],
  result[[3]]@filter.traj[1, 10000:50000, ]
)

# Plot theme:
p_theme <- theme(
  legend.justification = c(0, 1), legend.position = c(0, 1), 
  legend.title = element_blank(), 
  legend.background = element_rect(fill = "transparent"),
  plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"),
  plot.title = element_text(hjust = 0.5, size = 8),
  axis.title = element_blank(),
  legend.key = element_rect(fill = NA)
)

# e_lpd
df <- data.frame(time = dates, y = colMeans(traj)[-1])
p1 <- ggplot(data = df, aes(x = time, y = y)) +
  geom_line() + theme_light() + p_theme +
  labs(
    title = latex2exp::TeX(
      "Time-varying mean of the PD: \\textbf{$\\tilde{\\eta}_t$}"
    )
  )

# cr
df <- data.frame(time = dates, cr = y$cr)
p2 <- ggplot(data = df, aes(x = time, y = cr)) +
  geom_line() + theme_light() + p_theme +
  labs(title = latex2exp::TeX("Consumption Risk: \\textbf{$cr_t$}"))

# mys
df <- data.frame(time = dates, mys = y$mys)
p3 <- ggplot(data = df, aes(x = time, y = mys)) +
  geom_line() + theme_light() + p_theme +
  labs(title = latex2exp::TeX("Middle-Aged to Young Ratio: \\textbf{$my_t$}"))

# fr
df <- data.frame(time = dates, fr = y$fr)
p4 <- ggplot(data = df, aes(x = time, y = fr)) +
  geom_line() + theme_light() + p_theme +
  labs(title = latex2exp::TeX("Effective Federal Funds Rate: \\textbf{$fr_t$}"))

# ms
df <- data.frame(time = dates, ms = y$ms)
p5 <- ggplot(data = df, aes(x = time, y = ms)) +
  geom_line() + theme_light() + p_theme +
  labs(title = latex2exp::TeX("Log Real M1 Money Supply: \\textbf{$ms_t$}"))

# Phi
# ms
df <- data.frame(time = dates, gdp = y$gdp)
p6 <- ggplot(data = df, aes(x = time, y = gdp)) +
  geom_line() + theme_light() + p_theme +
  labs(title = latex2exp::TeX("Log Real GDP: \\textbf{$gdp_t$}"))

grid.arrange(
  p1 + theme(legend.position = "none"), 
  p2 + theme(legend.position = "none"), 
  p3 + theme(legend.position = "none"), 
  p4 + theme(legend.position = "none"), 
  p5 + theme(legend.position = "none"), 
  p6 + theme(legend.position = "none"), 
  ncol = 2
)
```

Before investigating the cointegration relationship, the individual variables will be characterized by means of the augmented Dickey-Fuller (ADF) test. Table \@ref(tab:cointegration-analysis-unit-root-tests) shows the test statistics for all variables. For variables in level the ADF test has been applied with a constant and drift and for variables in difference only a constant term has been added. All tests have been applied using only one lag.

```{r, cointegration-analysis-unit-root-tests, eval=TRUE, echo=FALSE}
traj <- rbind(
  result[[1]]@filter.traj[1, 10000:50000, ], 
  result[[2]]@filter.traj[1, 10000:50000, ],
  result[[3]]@filter.traj[1, 10000:50000, ]
)
e_lpd <- colMeans(traj)[-1]

# with corona
e_lpd1 <- urca::summary(urca::ur.df(e_lpd, type = "trend", lags = 1))
cr1 <- urca::summary(urca::ur.df(y$cr, type = "trend", lags = 1))
my1 <- urca::summary(urca::ur.df(y$mys, type = "trend", lags = 1))
fr1 <- urca::summary(urca::ur.df(y$fr, type = "trend", lags = 1))
ms1 <- urca::summary(urca::ur.df(y$ms, type = "trend", lags = 1))
gdp1 <- urca::summary(urca::ur.df(y$gdp, type = "trend", lags = 1))

d_e_lpd1 <- urca::summary(urca::ur.df(diff(e_lpd), type = "drift", lags = 1))
d_cr1 <- urca::summary(urca::ur.df(diff(y$cr), type = "drift", lags = 1))
d_my1 <- urca::summary(urca::ur.df(diff(y$mys), type = "drift", lags = 1))
d_d_my1 <- urca::summary(urca::ur.df(diff(diff(y$mys)), type = "drift", lags = 1))
d_fr1 <- urca::summary(urca::ur.df(diff(y$fr), type = "drift", lags = 1))
d_ms1 <- urca::summary(urca::ur.df(diff(y$ms), type = "drift", lags = 1))
d_gdp1 <- urca::summary(urca::ur.df(diff(y$gdp), type = "drift", lags = 1))
with_corona <- list(
  e_lpd1, cr1, my1, fr1, ms1, gdp1, d_e_lpd1, d_cr1, d_my1, d_d_my1, d_fr1, d_ms1, d_gdp1
)

result_table <- data.frame(
  with_corona = sapply(with_corona, function(x){x@teststat[1]}),
  c_1 = sapply(with_corona, function(x){x@cval[1, 1]}),
  c_2 = sapply(with_corona, function(x){x@cval[1, 2]}),
  c_3 = sapply(with_corona, function(x){x@cval[1, 3]})
)
stars <- rev(c("*", "**", "***"))
result_table <- round(result_table, 4)
result_table[, 1] <- sapply(1:13, function(i){paste(
    result_table[i, 1], 
    ifelse(
        is.na(stars[result_table[i, 1] < with_corona[[i]]@cval[1, ]][1]),
        "", stars[result_table[i, 1] < with_corona[[i]]@cval[1, ]][1]
    )
)})

rownames(result_table) <- c(
  "$\\tilde{\\eta}_t$", "$cr_t$", "$my_t$", "$fr_t$", "$ms_t$", "$gdp_t$",
  "$\\Delta\\tilde{\\eta}_t$", "$\\Delta cr_t$", "$\\Delta my_t$", "$\\Delta^2 my_t$",
  "$\\Delta fr_t$", "$\\Delta ms_t$",  "$\\Delta gdp_t$"
)
opts <- options(knitr.kable.NA = "")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("t-statistic", "$\\alpha = 1\\%$", "$\\alpha = 5\\%$",
                "$\\alpha = 10\\%$"),
  caption = "Cointegration analysis: Results ADF unit root test", 
  booktabs = TRUE, escape = FALSE,
  linesep = "", 
  align = rep("l", 4)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position") %>%
  kableExtra::add_header_above(c("", "", "Critical Values" = 3)) %>%
  kableExtra::footnote(
    symbol = c("Significance at 1% level", 
               "Significance at 5% level",
               "Significance at 10% level"),
    symbol_manual = rev(c("*", "**", "***")),
    footnote_as_chunk = F
  )
```

The test results of most factors indicate first order integration at conventional significance levels. Only the effective federal funds rate has a significance level of $10\%$. For this master's thesis the federal funds rate will be handled as a first order integrated time series. Additionally, the test results for the middle-aged to young ratio indicate second order integration. However, for longer time series there exists significant evidence for first order integration, [@long_term_mys]. Hence, the middle-aged to young ratio will be treated as a first order integrated process as well.

Moreover, the cointegration analysis will be applied using the following single equation error correction model (SECM)
\begin{equation*}
\begin{split}
\Delta \tilde{\eta}_t = &\alpha(\tilde{\eta}_{t-1} - \beta_0 - \beta_1 cr_{t-1} - \beta_2 my_{t-1} - \beta_3 fr_{t-1} - \beta_4 ms_{t-1} - \beta_5 gdp_{t-1}) + \\
&\delta_1 \Delta cr_{t} + \delta_2 \Delta my_{t} + \delta_3 \Delta fr_{t} + \beta_4 \Delta ms_{t} + \beta_5 \Delta gdp_{t} + \phi_1\Delta \tilde{\eta}_{t-1} + \phi_2\Delta \tilde{\eta}_{t-2} + e_t
\end{split}
\end{equation*}
```{r, cointegration-analysis-study-1, eval=TRUE, echo=FALSE}
traj <- rbind(
  result[[1]]@filter.traj[1, 10000:50000, ], 
  result[[2]]@filter.traj[1, 10000:50000, ],
  result[[3]]@filter.traj[1, 10000:50000, ]
)

data <- zoo::as.zoo(data.frame(
  e_lpd = colMeans(traj)[-1], cr = y$cr, mys = y$mys, fr = y$fr, ms = y$ms, 
  gdp = y$gdp
), order.by = zoo::index(1:length(y$cr)))

# complete analysis
ardl_model_1 <- ARDL::ardl(
  e_lpd ~ cr + mys + fr + ms + gdp, order = c(3, 1, 1, 1, 1, 1), data = data
)
cointegration_model_1 <- ARDL::uecm(ardl_model_1)
m_1 <- ARDL::multipliers(cointegration_model_1)
cointegration_model_1 <- summary(cointegration_model_1)

# drop cr, fr, gdp
ardl_model_2 <- ARDL::ardl(
  e_lpd ~ mys + ms, order = c(3, 1, 1), data = data
)
cointegration_model_2 <- ARDL::uecm(ardl_model_2)
m_2 <- ARDL::multipliers(cointegration_model_2)
cointegration_model_2 <- summary(cointegration_model_2)

result_table <- data.frame(
  complete = c(
    cointegration_model_1$coefficients[2, 1], m_1[1:6, 2],
    cointegration_model_1$r.squared, cointegration_model_1$adj.r.squared
  ),
  droped = c(
    cointegration_model_2$coefficients[2, 1], m_2[1, 2], NA, m_2[2, 2], 
    NA, m_2[3, 2], NA, 
    cointegration_model_2$r.squared, cointegration_model_2$adj.r.squared
  )
)
result_table <- round(result_table, 4)
stars <- rev(c("*", "**", "***"))

# Cointegration test
result_table[1, 1] <- paste(
  result_table[1, 1], 
  ifelse(
    is.na(
      stars[cointegration_model_1$coefficients[2, 3] < c(-4.7970, -4.1922, -3.8670)][1]
    ),
    "", 
    stars[cointegration_model_1$coefficients[2, 3] < c(-4.7970, -4.1922, -3.8670)][1]
  )
)
result_table[1, 2] <- paste(
  result_table[1, 2], 
  ifelse(
    is.na(
      stars[cointegration_model_2$coefficients[2, 3] < c(-4.0947, -3.5057, -3.1924)][1]
    ),
    "", 
    stars[cointegration_model_2$coefficients[2, 3] < c(-4.0947, -3.5057, -3.1924)][1]
  )
)

# Long run relationship test
result_table[2:7, 1] <- sapply(2:7, function(i){paste(
    result_table[i, 1],
    ifelse(
        is.na(stars[m_1[i - 1, 5] < c(0.01, 0.05, 0.1)][1]),
        "", stars[m_1[i - 1, 5] < c(0.01, 0.05, 0.1)][1]
    )
)})
result_table[c(2, 4, 6), 2] <- sapply(1:3, function(i){paste(
    
    result_table[c(2, 4, 6)[i], 2],
    ifelse(
        is.na(stars[m_2[i, 5] < c(0.01, 0.05, 0.1)][1]),
        "", stars[m_2[i, 5] < c(0.01, 0.05, 0.1)][1]
    )
)})
rownames(result_table) <- c(
  "$\\alpha$", "$\\beta_0$", "$\\beta_1$", "$\\beta_2$", "$\\beta_3$",
  "$\\beta_4$", "$\\beta_5$", "$R^2$", "adj $R^2$"
)
```
Table \@ref(tab:cointegration-analysis-study-2) displays the results of the inspected cointegration relationship based on a nonlinear regression approach. Test statistics for the adjustment coefficient $\alpha$ are used for testing a possible cointegration relationship. The model including all covariates has no significant cointegration relationship based on a $10\%$ significance level. The statistic of `r round(cointegration_model_1$coefficients[2, 3], 3)` is larger than the critical value $-3.8670$ at the $10\%$ level, obtained from the surface regression approach by [@cointegration_critical_values]. 
```{r, cointegration-analysis-study-2, eval=TRUE, echo=FALSE}
opts <- options(knitr.kable.NA = "-")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("All covariates", "Excluding $cr_t$, $fr_t$, $gdp_t$"),
  caption = "Cointegration analysis: Results", 
  booktabs = TRUE, escape = FALSE,
  linesep = "", 
  align = rep("l", 2)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position") %>%
  kableExtra::footnote(
    symbol = c("Significance at 1% level", 
               "Significance at 5% level",
               "Significance at 10% level"),
    symbol_manual = rev(c("*", "**", "***")),
    footnote_as_chunk = F
  )
```

Furthermore, the test has been repeated by removing the insignificant covariates $cr_t$, $fr_t$ and $gdp_t$. Again the cointegration relationship is insignificant at a $10\%$ level. However, the test statistic `r round(cointegration_model_2$coefficients[2, 3], 3)` is larger but relatively close to the critical value $-3.1924$. 

Overall, the cointegration analysis has been applied under the assumption that the latent mean of the PD follows a random walk state process. Contrary to the results presented in [@h2_paper], a cointegration relationship could no be established. Besides assuming that this connection is not present for the collected data, multiple reasons are available for the mixing results. 

Firstly, it is possible that the state process is misspecified and another specification, possibly already including covariates, is more suitable. Additionally, the impact of the missing dividend payout policy factor used in [@h2_paper] is not accounted here. Moreover, it is possible that the cointegration relationship is only present for data with lower frequency than quarterly. Furthermore, different data sources and a shorter time frame have been used. 

## State process extension with covariates

As mentioned, it is possible that the latent mean of the PD could be misspecified by using a random walk state process. Hence, this chapter will continue the analysis of exogenous covariates effecting the latent process. An additional model containing covariates will be tested and the results will be compared with the random walk model. Firstly, the model extension and the estimation results will be presented. Then the latent state estimates will be compared and model comparison performed.

Common similarities between the approaches will be that the same parameter transformations as in the [simulation study][Simulation study] chapter will be applied. Hence, the hyperparameters $\psi$, $\varsigma_\epsilon$ and $\varsigma_u$ will be used. Moreover, the PMMH approach will be run in parallel using 3 independent Markov chains, $J = 1000$ particles and $M = 50000$ iterations. Similar to the random walk state process, a flat prior specification will be used and the first $10000$ iterations of the PMMH approach will be dropped.

The model extension (**Covariate Model**) describes a possible long run equilibrium relationship that could not be detected within the [cointegration analysis][Cointegration analysis]
\begin{equation}
\begin{split}
\tilde{\eta}_t &= \beta_0 + \beta_1 my_{t} + \beta_2 ms_{t} + u_t \\
u_t &\sim \mathcal{N}(0, \sigma_u^2)
(\#eq:static-model-state-process)
\end{split}
\end{equation}
Here, only the middle-aged to young ratio and the log real M1 money suppy have been added. Combined with the observation process \@ref(eq:observation-process) the necessary parameters are $\theta = \begin{pmatrix} \beta_0 & \beta_1 & \beta_2 & \sigma_{\epsilon} & \sigma_{u}\end{pmatrix}^T$. Note that due to the missing autoregressive part of the model, the initial value $\tilde{\eta}_0$ does not need to be included.

Furthermore, as initial parameters $\theta^{(0)}$ the long-run equilibrium parameter estimates from the single equation error correction model from Table \@ref(tab:cointegration-analysis-study-2) will be used
$$
\theta^{(0)} = \begin{pmatrix}\beta_0^{(0)} \\ \beta_1^{(0)} \\ \beta_2^{(0)} \\ \varsigma_\epsilon^{(0)} \\ \varsigma_u^{(0)} \end{pmatrix} = \begin{pmatrix} 2.4 \\ 1.6 \\ 0.4 \\ \ln(0.05) \\ \ln(0.05) \end{pmatrix}
$$
Moreover, for this model the following proposal distribution is used
$$\theta_i^{*}\sim \mathcal{N}(\theta_i^{(m-1)}, 0.015^2)$$
Figure \@ref(fig:extension-static-model-trace) shows the trace plot for each parameter and each Markov chain. Furthermore, Figure \@ref(fig:extension-static-model-marginal-density) displays the Gaussian kernel density estimates of the marginal posterior distributions. Table \@ref(tab:pmmh-estimation-result) summarizes the estimation results
```{r, pmmh-estimation-result, eval=TRUE, echo=FALSE}
library(tidyverse)

model_1 <- lapply(
  readRDS(
    here::here("data", "results", "model_study_result_static_model.rds")
  ), function(x){x@traces[10000:50000, ]}
)
model_1 <- do.call(rbind, model_1)

result_table <- data.frame(
  model_1_mean = c(
    colMeans(model_1[, 3:5]), colMeans(exp(model_1[, 6:7]))
  ),
  model_1_median = c(
    apply(model_1[, 3:5], 2, median),
    apply(exp(model_1[, 6:7]), 2, median)
  ),
  model_1_lower = c(
    quantile(model_1[, 3], probs = 0.025), 
    quantile(model_1[, 4], probs = 0.025), 
    quantile(model_1[, 5], probs = 0.025), 
    quantile(exp(model_1[, 6]), probs = 0.025), 
    quantile(exp(model_1[, 7]), probs = 0.025)
  ),
  model_1_upper = c(
    quantile(model_1[, 3], probs = 0.975), 
    quantile(model_1[, 4], probs = 0.975), 
    quantile(model_1[, 5], probs = 0.975), 
    quantile(exp(model_1[, 6]), probs = 0.975), 
    quantile(exp(model_1[, 7]), probs = 0.975)
  )
)
result_table <- data.frame(sapply(result_table, as.numeric))

result_table <- round(result_table, 4)
rownames(result_table) <- c(
  "$\\beta_0$", "$\\beta_1$", "$\\beta_2$", "$\\sigma_u$", 
  "$\\sigma_\\epsilon$"
)
opts <- options(knitr.kable.NA = "-")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("Mean", "Median", "$0.025\\%$", "$0.975\\%$"),
  caption = "Covariate extension: PMMH results", 
  booktabs = TRUE, escape = FALSE,
  linesep = "", 
  align = rep("c", 3)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```
As can be seen, the estimated parameters $\beta_0$, $\beta_1$ and $\beta_2$ diverge from the estimated long-run relationship in the cointegration analysis. However, the estimates using the state process containing covariates are close to the results already presented in Table \@ref(tab:cointegration-analysis-study-2). Moreover, the standard deviation $\sigma_u$ for the error term in the latent state process is significantly larger for the covariate extension compared to the random walk model.

Figure \@ref(fig:pmmh-estimation-figure) displays the filtered latent state for the model extension and the model with random walk state process. Both filtered latent state estimates are moving closely together with the random walk model being smoother and the covariate extension model having more spikes. 

```{r, pmmh-estimation-figure, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Covariate extension: Filtered latent state", fig.width=6, fig.height=4, warning=FALSE}
library(ggplot2)

rw_model <- lapply(
  readRDS(
    here::here("data", "results", "model_study_result_basic_model.rds")
  ), function(x){x@filter.traj[1, 10000:50000, ]}
)
rw_model <- do.call(rbind, rw_model)[, -1]
static_model <- lapply(
  readRDS(
    here::here("data", "results", "model_study_result_static_model.rds")
  ), function(x){x@filter.traj[1, 10000:50000, ]}
)
static_model <- do.call(rbind, static_model)[, -1]

df <- data.frame(time = dates, lpd = y$lpd, rw_model = colMeans(rw_model), 
                 static_model = colMeans(static_model))
df <- reshape2::melt(df, id.vars = "time")
ggplot(data = df, aes(x = time, y = value, group = variable)) +
  geom_line(aes(color = variable)) + 
  scale_color_manual(
    values = c("black", "red", "blue"), 
    labels = unname(c(
      latex2exp::TeX("PD: $\\eta_t$"),
      latex2exp::TeX("RW model: $\\tilde{\\eta}_t$"),
      latex2exp::TeX("Covariate model: $\\tilde{\\eta}_t$")
   ))
  ) +
  theme_light() +
  theme(
    legend.position = "bottom", 
    legend.title = element_blank(), 
    legend.background = element_rect(fill = "transparent"),
    plot.margin = unit(c(0.1,0.1,0.1,0.1), "cm"),
    plot.title = element_text(hjust = 0.5, size = 8),
    axis.title = element_blank(),
    axis.text.x = element_text(size = 7),
    axis.text.y = element_text(size = 7)
  )
```
For the model comparison the Deviance information criterion (DIC) will be used. Similar to the Akaike information criterion (AIC) and the Bayesian information criterion (BIC), the model with the lower DIC should be preferred. The deviance $D(\theta)$ is defined as
$$D(\theta) = -2\ell(\theta) + C$$
with $\ell(\theta$ being the log likelihood for the likelihood function \@ref(eq:pomp-likelihood). $C$ is a constant that cancels out in the model comparison and therefore it can be discarded. 

The DIC follows as
\begin{equation}
DIC = D(\hat\theta) + 2p_d \\
\end{equation}
where $\hat\theta$ is the mean of the posterior distribution and $p_d$ is the effective number of parameters that can be approximated by
$$p_d = \frac{1}{2}\hat{\text{Var}}(D(\theta))$$
Hence, $p_d$ is half the estimated variance of the deviance from the posterior samples $\theta$. Table \@ref(tab:pmmh-model-comparison) depicts the model comparison results. The log likelihood of $\hat\theta$ has been computed using $10$ runs of the particle filter with $1000$ particles and averaging the resulting log likelihoods. As visible, measured by the DIC the random walk model is superior.

```{r, pmmh-model-loglik, eval=TRUE, echo=FALSE}
l_rw_model <- readRDS(here::here("data", "results", "rw_lik.rds"))
l_cov_model <- readRDS(here::here("data", "results", "cov_lik.rds"))
```

```{r, pmmh-model-comparison, eval=TRUE, echo=FALSE}
r <- readRDS(
    here::here("data", "results", "model_study_result_basic_model.rds")
)
N <- ncol(r[[1]]@data)
rw_model <- lapply(r, function(x){x@traces[10000:50000, ]})
rw_model <- do.call(rbind, rw_model)

model_1 <- lapply(
    readRDS(
        here::here("data", "results", "model_study_result_static_model.rds")
    ), function(x){x@traces[10000:50000, ]}
)
model_1 <- do.call(rbind, model_1)

result_table <- data.frame(
  num_params = c(3, 5),
  lik = c(l_rw_model, l_cov_model),
  dic = c(
    -2*l_rw_model + var(-2*rw_model[, 1]), 
    -2*l_cov_model + var(-2*model_1[, 1])
  )
)

rownames(result_table) <- c("RW Model", "Covariate Model")
result_table <- round(result_table, 4)
opts <- options(knitr.kable.NA = "")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("Number of parameters", "$\\ell(\\hat{\\theta})$", "DIC"),
  caption = "Covariate extension: Model comparison", 
  booktabs = TRUE, escape = FALSE,
  linesep = "", 
  align = rep("c", 4)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position")
rm(list = ls())
```

