---
output: pdf_document
editor_options: 
  chunk_output_type: console
---

# Simulation study {#simulation}

A simulation study will be conducted in order to compare the likelihood based inference procedure, via the Nelder-Mead algorithm, with the Bayesian approach, based on the particle marginal Metropolis Hastings algorithm. The simulation setup will be closely related to the empirical application. Hence, a variation of the log-linear present-value formulation of the log price dividend ratio will be used. 

Within the simulation study the latent state $x_n$ will take the form of an autoregressive distributed lag model and will be simulated according to the following process
\begin{equation}
\begin{split}
& x_0 = 3.5 \\
& x_{n} = \beta_0 + \phi x_{n-1} + \beta_{1}z_{1, n} + \beta_{2}z_{2, n} + u_{n} \\ 
& u_{n} \sim \mathcal{N}(0, \sigma_{u}^2)
(\#eq:simulation-study-state)
\end{split}
\end{equation}
with restriction $\left|\phi\right|<1$ and where $z_{1,n}$ and $z_{2,n}$ are exogenous covariates simulated according to
\begin{equation}
\begin{split}
& \begin{pmatrix} z_{1, 0} \\ z_{2, 0} \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \\
& \begin{pmatrix} z_{1, n} \\ z_{2, n} \end{pmatrix} = \begin{pmatrix} z_{1, n-1} \\ z_{2, n-1} \end{pmatrix} + \begin{pmatrix} w_{1, n} \\ w_{2, n} \end{pmatrix} \\
& \begin{pmatrix} w_{1, n} \\ w_{2, n} \end{pmatrix} \sim \mathcal{N}\Bigg(\begin{pmatrix} 0 \\ 0 \end{pmatrix}, 0.0025\;I_{2}\Bigg)
\end{split}
\end{equation}
This setup is an indirect specification of an error correction mechanism. The relationship to the error correction model can be seen by subtracting $x_{n-1}$ and adding and subtracting $\beta_{1}z_{1, n-1}$, $\beta_{2}z_{2, n-1}$ from both sides of the state process \@ref(eq:simulation-study-state)
\begin{equation*}
\begin{split}
 \Delta x_{n} & = \beta_{1}\Delta z_{1, n} + \beta_{2}\Delta z_{2, n} - (1 - \phi) x_{n-1} + \beta_0 + \beta_{1}z_{1, n-1} + \beta_{2}z_{2, n-1} + u_{n} \\ 
 & = \beta_{1} \Delta z_{1, n} + \beta_{2} \Delta z_{2, n} - (1 - \phi)\Bigg(x_{n-1} - \frac{\beta_0}{1 - \phi} - \frac{\beta_{1}}{1 - \phi}z_{1, n} - \frac{\beta_{2}}{1 - \phi}z_{2, n}\Bigg) + u_{n}
\end{split}
\end{equation*}
Hence, given that $\left|\phi\right|<1$, the long run relationship between the steady-state latent process, $x_{n} = x_{n-1} = \bar{x}$,  and the steady-state covariates, $z_{1, n} = z_{1, n-1} = \bar{z}_1$, $z_{2, n} = z_{2, n-1} = \bar{z}_2$, is specified by
$$\bar{x}=\frac{\beta_0}{1 - \phi} + \frac{\beta_{1}}{1 - \phi}\bar{z}_{1} + \frac{\beta_{2}}{1 - \phi}\bar{z}_{2}$$
The state process will be bound to be between the minimum, 2.775, and the maximum, 4.487, of the observed log price dividend ratio.

Furthermore, the observation process has the following specification
\begin{equation}
\begin{split}
& y_{n} = \frac{k_{n}}{1 - \rho_{n}} + d_{n} +  \epsilon_{n} \\
& \epsilon_{n}\sim \mathcal{N}(0, \sigma_{\epsilon}^2)
\end{split}
\end{equation}
where $k_{n}$ and $\rho_n$ directly depend on the latent state $x_n$ via
\begin{equation}
\begin{split}
& \rho_{n} = \frac{1}{1+\text{exp}(-x_{n})}\\
& k_{n} = -\ln(\rho_{n})-(1-\rho_{n})\ln\bigg(\frac{1}{\rho_{n}} - 1\bigg)
\end{split}
\end{equation}
and $d_{n}$ will be handled as an exogenous covariate arising from the following specification
\begin{equation}
\begin{split}
& d_{n} = -1 + 0.85(d_{n - 1} + 1) + 0.15v_{n-1} + v_{n} \\ 
& v_{n} \sim \mathcal{N}(0, 0.003)
\end{split}
\end{equation}
All in all, the processes $y_{n}$, $d_{n}$, $z_{1, n}$ and $z_{2, n}$ are observable whereas the state process $x_n$ is not. Note that the initial value problem of obtaining an estimate for $x_0$ can be solved within the Bayesian setup as well. In order to do this a prior for $x_0$ has to be assigned. Hence, the results of the likelihood based and Bayesian approach for estimating the latent state and the parameters $\theta = \begin{pmatrix} x_0 & \phi & \beta_0 & \beta_{1} & \beta_{2} & \sigma_{\epsilon} & \sigma_{u} \end{pmatrix}^T$ will be compared. For this simulation, the parameters will be set to
$$
\theta = \begin{pmatrix} x_0 \\ \phi \\ \beta_0 \\ \beta_{1} \\ \beta_{2} \\ \sigma_{\epsilon} \\ \sigma_{u} \end{pmatrix} = \begin{pmatrix} 3.5 \\0.85 \\ 0.525 \\ 0.4 \\ -0.2 \\ 0.02 \\ 0.05 \end{pmatrix}
$$
Since particle filter based inference methods are very computationally intensive, the simulation procedure will be applied $1000$ times for $N = 50$ simulated observations. For the likelihood based inference $10$ rounds of Nelder-Mead optimization will be run, at each of the $1000$ iterations, and the result with the highest estimated likelihood will be taken as the maximum likelihood estimate. Furthermore, for the Bayesian approach three different prior setups will be applied and compared. Finally, the likelihood based approach will be applied using a cloud of $J=1000$ particles and the particle marginal Metropolis Hastings procedures will use $J=500$ particles.

The Nelderâ€“Mead algorithm was developed for unbounded optimization problems. However, this simulation setup has the following parameters $\{\phi, \sigma_{\epsilon}, \sigma_{u}\}$ that are bounded by $\left|\phi\right|<1$, $\sigma_u>0$ and $\sigma_{\epsilon}>0$. Hence, these transformations will be applied
\begin{align}
\phi &= \text{tanh}(\psi) & \sigma_u &= \text{exp}(\varsigma_u) & \sigma_{\epsilon} &= \text{exp}(\varsigma_{\epsilon})
\end{align}
where $\psi, \varsigma_u, \varsigma_{\epsilon} \in \mathbb{R}$ are unconstrained hyperparameters on the real line. The resulting estimates for these hyperparameters can than easily be converted in order to obtain the initial parameters. 

Furthermore, parameter transformations are also beneficial to random walk Metropolis Hastings approaches. Without them, proposal values can be generated that violate the parameter constraints and this increases the autocorrelation of the Markov chain. Hence, the presented parameter transformation will be applied for the particle marginal Metropolis Hastings procedure as well. Therefore, both applications will use the following parameter vector $\theta = \begin{pmatrix} x_0 & \psi & \beta_0 & \beta_{1} & \beta_{2} & \varsigma_{\epsilon} & \varsigma_{u}\end{pmatrix}^T$ instead of the initially presented for inference purposes.

The impact of the chosen prior specification will be analyzed by comparing three setups. Firstly, **setup 1** will use a flat improper prior specification that incorporates almost no prior information about the initially presented parameters. **Setup 2** and **setup 3** will use informative priors, where the former will incorporate true information about the initial parameters and the latter will be misspecified in the sense that it assigns a small probability to the initial parameters.

The change of variable technique has to be applied in order to transfer the prior beliefs about the parameters $\phi$, $\sigma_u$ and $\sigma_\epsilon$ to the hyperparameters $\psi$, $\varsigma_u$, $\varsigma_{\epsilon}$.

Since the transformation for $\sigma_u$ and $\sigma_{\epsilon}$ is equivalent, the following derivations will be presented for an exemplary parameter $\sigma$. Given the prior $p_{\phi}(\phi)$, $p_{\sigma}(\sigma)$ the accompanying prior $p_{\psi}(\psi)$, $p_{\varsigma}(\varsigma)$ can be derived using
\begin{align*}
\phi(\psi) &= \text{tanh}(\psi) & \Longleftrightarrow && \psi(\phi) &= \text{atanh}(\phi) \\
\sigma(\varsigma) &= \text{exp}(\varsigma) & \Longleftrightarrow && \varsigma(\sigma) &= \ln(\sigma)
\end{align*}
Differentiating the initial parameters $\phi$ and $\sigma$ with respect to the hyperparameter yields
\begin{equation*}
\begin{split}
  \frac{\partial \phi(\psi)}{\partial \psi} &= 1 - \text{tanh}(\psi)^2 \\
  \frac{\partial \sigma(\varsigma)}{\partial \varsigma} &= \text{exp}(\varsigma)
\end{split}
\end{equation*}
Hence, the change of variable technique yields the following prior specifications for the hyperparameters
\begin{equation}
\begin{split}
  p_{\psi}(\psi) &= p_{\phi}(\text{tanh}(\psi))(1 - \text{tanh}(\psi)^2) \\
  p_{\varsigma}(\varsigma) &= p_{\sigma}(\text{exp}(\varsigma))\text{exp}(\varsigma)
(\#eq:hyperparamter-prior)
\end{split}
\end{equation}
For **setup 1**, the initially presented parameters will have the following prior specification
\begin{align*}
  p_{x_0} &= 1 &
  p_\phi(\phi) &= 
  \begin{cases} 
    1, & \text{if}\;\; -1 < \phi < 1  \\ 
    0, & \text{otherwise}
  \end{cases} \\
 p_{\sigma_u}(\sigma_u) &= 
 \begin{cases} 
    1, & \text{if}\;\; \sigma_u > 0  \\ 
    0, & \text{otherwise}
  \end{cases}
 & p_{\sigma_\epsilon}(\sigma_\epsilon) &= 
 \begin{cases} 
    1, & \text{if}\;\; \sigma_{\epsilon} > 0  \\ 
    0, & \text{otherwise}
  \end{cases} \\
 p_{\beta_0}(\beta_0) &= 1 & p_{\beta_1}(\beta_1) &= 1 & p_{\beta_2}(\beta_2) &= 1
\end{align*}
Note that within this uninformative flat prior setup all possibly valid parameter values will have a prior density of one. Using result \@ref(eq:hyperparamter-prior) the belonging prior specification for the hyperparameters for **setup 1** simplifies to
\begin{equation*}
\begin{split}
p_{\psi}(\psi) &= p_{\phi}(\text{tanh}(\psi))(1 - \text{tanh}(\psi)^2) = (1 - \text{tanh}(\psi)^2) \\
  p_{\varsigma_u}(\varsigma_u) &= p_{\sigma_u}(\text{exp}(\varsigma_u))\text{exp}(\varsigma_u) = \text{exp}(\varsigma_u) \\
  p_{\varsigma_\epsilon}(\varsigma_\epsilon) &= p_{\varsigma_\epsilon}(\text{exp}(\varsigma_\epsilon))\text{exp}(\varsigma_\epsilon) = \text{exp}(\varsigma_\epsilon)
\end{split}
\end{equation*}
**Setup 2** represents the informative prior setup. The truncated normal distribution will be used for parameter $\phi$ and the Gamma distribution for the parameters $\sigma_u$ and $\sigma_\epsilon$. Hence, this specification is applied
\begin{align*}
  p_{x_0} &= f^{\mathcal{N}}(x_0 | 3.5, 0.1^2) \\
  p_\phi(\phi) &= f^{\mathcal{T}\mathcal{N}}(\phi | 0.8, 0.1^2, -1, 1) &  p_{\beta_0}(\beta_0) &= f^{\mathcal{N}}(\beta_0 | 0.4, 0.1^2) \\
  p_{\sigma_u}(\sigma_u) &= f^{\mathcal{G}}(\sigma_u | 12, 0.005) &
  p_{\beta_1}(\beta_1) &= f^{\mathcal{N}}(\beta_1 | 0.5, 0.2^2) \\
  p_{\sigma_\epsilon}(\sigma_\epsilon)  &= f^{\mathcal{G}}(\sigma_{\epsilon} | 6, 0.005) & p_{\beta_2}(\beta_2) &= f^{\mathcal{N}}(\beta_2 | -0.3, 0.2^2)
\end{align*}
where $f^{\mathcal{N}}(x | \mu, \sigma^2)$ is the density function of the normal distribution with mean $\mu$ and variance $\sigma^2$ and $f^{\mathcal{G}}(x | \alpha, \beta)$ is the density function of the gamma distribution with shape parameter $\alpha$ and scale parameter $\beta$. Additionally, the truncated normal density function is denoted by $f^{\mathcal{T}\mathcal{N}}(x|\mu, \sigma^2, a, b)$ and has the following definition
$$
f^{\mathcal{T}\mathcal{N}}(x|\mu, \sigma^2, a, b) = 
\begin{cases} 
  \dfrac{1}{\sigma}\dfrac{\phi\Big(\dfrac{x-\mu}{\sigma}\Big)}{\Phi\Big(\dfrac{b-\mu}{\sigma}\Big) - \Phi\Big(\dfrac{a-\mu}{\sigma}\Big)}, & \text{if}\;\; a < x < b  \\ 
  0, & \text{otherwise}
\end{cases}
$$
with $\phi(\cdot)$ being the density function of the standard normal distribution and $\Phi(\cdot)$ its cumulative distribution function. Again, the accompanying prior specification for the hyperparameters can be obtained using \@ref(eq:hyperparamter-prior).

**Setup 3** represents the misspecified prior setup. The same prior distributions as in the previous setup will be used however with varying parameters.
\begin{align*}
  p_{x_0} &= f^{\mathcal{N}}(x_0 | 3.0, 0.1^2) \\
  p_\phi(\phi) &= f^{\mathcal{T}\mathcal{N}}(\phi | 0.3, 0.1^2, -1, 1) & p_{\beta_0}(\beta_0) &= f^{\mathcal{N}}(\beta_0 | 0.8, 0.1^2) \\
  p_{\sigma_u}(\sigma_u) &= f^{\mathcal{G}}(\sigma_u | 10, 0.01) & p_{\beta_1}(\beta_1) &= f^{\mathcal{N}}(\beta_1 | 0.0, 0.1^2)  \\
  p_{\sigma_\epsilon}(\sigma_\epsilon)  &= f^{\mathcal{G}}(\sigma_{\epsilon} | 10, 0.005) & p_{\beta_2}(\beta_2) &= f^{\mathcal{N}}(\beta_2 | 0.0, 0.1^2)
\end{align*}

The MCMC approaches will use $7500$ iterations and each parameter proposal $\theta^*_i$ will be generated based on 
$$\theta_i^{*}\sim \mathcal{N}(\theta_i^{(m-1)}, 0.025^2)$$
Furthermore, the starting parameter $\theta^{(0)} = \begin{pmatrix} x_0^{(0)} & \psi^{(0)} & \beta_0^{(0)} & \beta_{1}^{(0)} & \beta_{2}^{(0)} & \varsigma_{\epsilon}^{(0)} & \varsigma_{u}^{(0)}\end{pmatrix} ^T$, for the inference methods, will be set randomly at each iteration of the simulation study with
$$\theta^{(0)} \sim \mathcal{N}\Big(\begin{pmatrix} 3.3 & \text{atanh}(0.7) & 1.05 & 0 & 0 & \ln(0.05) & \ln(0.1)\end{pmatrix}^T, 0.1^2\; I_6\Big)$$
One out of the $1000$ simulations will be inspected in detail and then the aggregate result of the simulation study will be presented. The detailed analysis will be conducted on simulation $4$.

```{r, nonlinear-simulation, eval=TRUE, echo=FALSE}
source(here::here("R", "simulation_study", "simulation.R"))
```

Looking at the simulated data, Figure \@ref(fig:sim-4-plot-1),  the relationship between the state and observation process can be inspected. It is visible that the observation process is varying around the state process and that the latter lags behind the former.

```{r, sim-4-plot-1, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Exemplary simulation: Data", fig.width=5, fig.height=2, warning=FALSE}
# Plot data
sim_4 <- simulation_result[[4]]

library(ggplot2)

df <- data.frame(time = 0:(N - 1), x = sim_4$data_latent, 
                 y = sim_4$data_observed[, 1])
df <- reshape2::melt(df, id.vars = "time")
ggplot(data = df, aes(x = time, y = value, group = variable)) +
  geom_line(aes(color = variable)) + 
  scale_color_manual(
    values = c("red", "black"), 
    labels = c(expression(x[n]), expression(y[n]))
  ) +
  theme_light() +
  theme(
    legend.justification = c(0, 1), legend.position = c(0, 1), 
    legend.title = element_blank(), 
    legend.background = element_rect(fill = "transparent"),
    plot.margin = unit(c(0.1,0.1,0.1,0.1), "cm"),
    axis.title = element_blank()
  ) + 
  labs(x = "n", y = "")
```

The trace plots from the Bayesian approaches can be seen in Figure \@ref(fig:sim-4-appendix-trace-plot-setup-1), Figure \@ref(fig:sim-4-appendix-trace-plot-setup-2) and Figure \@ref(fig:sim-4-appendix-trace-plot-setup-3). All approaches indicate a necessary burn in phase of roughly $3000$ iterations. Hence, for the next visualizations and figures the first $3000$ iterations of the Bayesian approaches have been discarded. Furthermore, for this particular example **setup 2** seems to have the fastest and **setup 1** the slowest convergence. The Gaussian kernel density estimates of the marginal posterior distributions can be seen in Figure \@ref(fig:sim-4-appendix-marginal-posterior-setup-1), Figure \@ref(fig:sim-4-appendix-marginal-posterior-setup-2) and Figure \@ref(fig:sim-4-appendix-marginal-posterior-setup-3).

Furthermore, the impact of the prior specification is visible. The misspecified, **setup 3**, and the uninformative case, **setup 1**, show the largest deviation from the true parameters.

Figure \@ref(fig:sim-4-plot-2) displays the state estimates $\hat{x}_n$ from all approaches. For the likelihood based approach this corresponds to the particle filter state estimate using the estimated MLE parameters. Furthermore, for the Bayesian approaches the marginal posterior mean for the latent state will be used.  One can see, that all setups are relatively good at estimating the state process. Moreover, for the Bayesian approaches $95\%$ pointwise credible intervals have been added with **setup 2** having the narrowest estimates.

\vfill
```{r, sim-4-plot-2, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Exemplary simulation: Latent state estimates", fig.width=5, fig.height=6.5, warning=FALSE}
# Filtering Results
sim_4 <- simulation_result[[4]]

library(ggplot2)
library(gridExtra)

# Plot theme:
p_theme <- theme(
  legend.position = "bottom", 
  legend.title = element_blank(), 
  legend.background = element_rect(fill = "transparent"),
  plot.margin = unit(c(0.1,0.1,0.1,0.1), "cm"),
  plot.title = element_text(hjust = 0.5, size = 8),
  axis.title = element_blank()
)

# Plot nelder mead result
df <- data.frame(
  time = 0:(N - 1), x = sim_4$data_latent,  
  x1 = sim_4$smc_result$pf
)
df <- reshape2::melt(df, id.vars = "time")
nm <- ggplot(data = df, aes(x = time, y = value, group = variable)) +
  geom_line(aes(color = variable), size = c(rep(0.5, 51), rep(0.3, 51))) + 
  scale_color_manual(
    values = c("black", "red"), 
    labels = c(expression(x[n]), expression(hat(x)[n]))
  ) +
  theme_light() +
  labs(title = latex2exp::TeX("State Estimate: \\textbf{Nelder-Mead Approach}"), 
       x = "", y = "") +
  p_theme

# Plot Noninformative Prior result
df <- data.frame(
  time = 0:(N - 1), x = sim_4$data_latent, 
  x1 = sim_4$pmmh_noninformative$state_mean
)
df <- reshape2::melt(df, id.vars = "time")
q <- data.frame(
  time = 0:(N - 1),
  variable = "x1",
  lower = sim_4$pmmh_noninformative$state_lower,
  upper = sim_4$pmmh_noninformative$state_upper
)
pm_1 <- ggplot(data = df, aes(x = time, y = value, group = variable)) +
  geom_line(aes(color = variable), size = c(rep(0.5, 51), rep(0.3, 51))) + 
  geom_ribbon(
    data = q, aes(x = time, ymin = lower, ymax = upper, fill = "red"),  
    inherit.aes = FALSE, alpha = 0.25, show.legend = FALSE, linetype = 2,
    colour = "red", size = 0.25
  ) +
  scale_color_manual(
    values = c("black", "red"), 
    labels = c(expression(x[n]), expression(hat(x)[n]))
  ) +
  theme_light() +
  labs(title = latex2exp::TeX("State Estimate: \\textbf{Setup 1}"), 
       x = "", y = "") +
  p_theme

# Plot true informative Prior result
df <- data.frame(
  time = 0:(N - 1), x = sim_4$data_latent,  
  x1 = sim_4$pmmh_true_informative$state_mean
)
df <- reshape2::melt(df, id.vars = "time")
q <- data.frame(
  time = 0:(N - 1),
  variale = "x1",
  lower = sim_4$pmmh_true_informative$state_lower,
  upper = sim_4$pmmh_true_informative$state_upper
)
pm_2 <- ggplot(data = df, aes(x = time, y = value, group = variable)) +
  geom_line(aes(color = variable), size = c(rep(0.5, 51), rep(0.3, 51))) + 
  geom_ribbon(
    data = q, aes(x = time, ymin = lower, ymax = upper, fill = "red"),  
    inherit.aes = FALSE, alpha = 0.25, show.legend = FALSE, linetype = 2,
    colour = "red", size = 0.25
  ) +
  scale_color_manual(
    values = c("black", "red"), 
    labels = c(expression(x[n]), expression(hat(x)[n]))
  ) +
  theme_light() +
  labs(title = latex2exp::TeX("State Estimate: \\textbf{Setup 2}"), 
       x = "", y = "") +
  p_theme

# Plot false informative prior result
df <- data.frame(
  time = 0:(N - 1), x = sim_4$data_latent,  
  x1 = sim_4$pmmh_false_informative$state_mean
)
df <- reshape2::melt(df, id.vars = "time")
q <- data.frame(
  time = 0:(N - 1),
  variable = "x1",
  lower = sim_4$pmmh_false_informative$state_lower,
  upper = sim_4$pmmh_false_informative$state_upper
)
pm_3 <- ggplot(data = df, aes(x = time, y = value, group = variable)) +
  geom_line(aes(color = variable), size = c(rep(0.5, 51), rep(0.3, 51))) + 
  geom_ribbon(
    data = q, aes(x = time, ymin = lower, ymax = upper, fill = "red"),  
    inherit.aes = FALSE, alpha = 0.25, show.legend = FALSE, linetype = 2,
    colour = "red", size = 0.25
  ) +
  scale_color_manual(
    values = c("black", "red"), 
    labels = c(expression(x[n]), expression(hat(x)[n]))
  ) +
  theme_light() +
  labs(title = latex2exp::TeX("State Estimate: \\textbf{Setup 3}"), 
       x = "", y = "") +
  p_theme

get_legend <- function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}
legend <- gtable::gtable_filter(ggplot_gtable(ggplot_build(pm_2)), "guide-box")

grid.arrange(
  nm + theme(legend.position = "none"), 
  pm_1 + theme(legend.position = "none"), 
  pm_2 + theme(legend.position = "none"), 
  pm_3 + theme(legend.position = "none"), 
  ncol = 1, legend = legend, nrow = 5, 
  heights = c(1.1, 1.1, 1.1, 1.1, 0.2)
)
```
\vfill
\newpage

Additionally, the results from the $10$ runs of the likelihood based approach using the Nelder-Mead algorithm can be inspected by looking at Table \@ref(tab:nm-result-table). The resulting parameter estimates within the $10$ repetitions vary a lot, which can be seen by the minimum and maximum estimates displayed. Furthermore, some parameter estimates are far of the true values.

```{r, nm-result-table, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
result_table <- data.frame(
  true = c(true_params_simulation, NA),
  mle = sim_4$smc_result$optim_result[
    , which.max(sim_4$smc_result$optim_result[8, ])
  ],
  min = apply(sim_4$smc_result$optim_result, MARGIN = 1, min),
  max = apply(sim_4$smc_result$optim_result, MARGIN = 1, max)
)
rownames(result_table) <- c(
  "$\\beta_0$", "$\\beta_1$", "$\\beta_2$", "$\\sigma_u$", 
  "$\\sigma_\\epsilon$", "$\\phi$", "$x_0$","$\\mathcal{L}(\\theta)$"
)
result_table[4:5, ] <- exp(result_table[4:5, ])
result_table[6, ] <- tanh(result_table[6, ])
result_table <- round(result_table, 4)
opts <- options(knitr.kable.NA = "")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("True Values", "Estimates", "Min", "Max"),
  caption = "Exemplary simulation: Results of the Nelder-Mead approach", booktabs = TRUE, escape = FALSE,
  linesep = "\\addlinespace", 
  align = rep("c", 4)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```

As expected, the first indication from inspecting the exemplary simulation results is that the informative **setup 2** results in the most precise parameter estimates. The aggregate analysis will clarify this observation.

Figure \@ref(fig:aggregate-simulation-result-plot) shows boxplots of the parameter estimates from the $1000$ simulation iterations. As visible, the variations of the parameter estimates from the Nelder-Mead approach and the marginal posterior means of the other 3 Bayesian approaches are depicted. Furthermore, the last visualization, in the bottom right corner, shows the root mean squared error (RMSE) for the latent state estimation for all approaches
$$\text{RMSE} = \sqrt{\frac{1}{N}\sum_{n = 1}^{N}(\hat{x}_n - x_n)^2}$$
The aggregate results show that the likelihood based approach has the highest variation and average deviation for the parameter and latent state estimates.  Moreover, the informative and uninformative prior setups, **setup 2** and **setup 1**, have the smallest average deviations with **setup 2** having also the smallest variance. The misspecified prior setup, **setup 3**, seems to have a smaller variance compared to the uninformative setup but a higher average deviation. All in all, it can be concluded that within the simulation study all Bayesian approaches outperform the likelihood based approach.

```{r, aggregate-simulation-result-plot, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Aggregate simulation: Result", fig.width=7, fig.height=10, warning=FALSE, message=FALSE}
res_smc <- matrix(nrow = N_sim, ncol = length(params_simulation))
mse_smc <- vector("numeric", length = N_sim)
res_pmmh_noninformative <- matrix(
  nrow = N_sim, ncol = length(params_simulation)
)
mse_pmmh_noninformative <- vector("numeric", length = N_sim)
res_pmmh_true_informative <- matrix(
  nrow = N_sim, ncol = length(params_simulation)
)
mse_pmmh_true_informative <- vector("numeric", length = N_sim)
res_pmmh_false_informative <- matrix(
  nrow = N_sim, ncol = length(params_simulation)
)
mse_pmmh_false_informative <- vector("numeric", length = N_sim)

for (i in 1:N_sim) {
    res_smc[i, ] <- simulation_result[[i]]$smc_result$par
    mse_smc[i] <- sqrt(mean((
      simulation_result[[i]]$smc_result$pf - 
        simulation_result[[i]]$data_latent
    )[2:N]^2))
    for (j in 1:length(params_simulation)) {
        res_pmmh_noninformative[i, j] <- mean(
          simulation_result[[i]]$pmmh_noninformative$traces[3000:7500, j+2]
        )
        mse_pmmh_noninformative[i] <- sqrt(mean((
          simulation_result[[i]]$pmmh_noninformative$state_mean - 
            simulation_result[[i]]$data_latent
        )[2:N]^2))
        res_pmmh_true_informative[i, j] <- mean(
          simulation_result[[i]]$pmmh_true_informative$traces[3000:7500, j+2]
        )
        mse_pmmh_true_informative[i] <- sqrt(mean((
          simulation_result[[i]]$pmmh_true_informative$state_mean - 
            simulation_result[[i]]$data_latent
        )[2:N]^2))
        res_pmmh_false_informative[i, j] <- mean(
          simulation_result[[i]]$pmmh_false_informative$traces[3000:7500, j+2]
        )
        mse_pmmh_false_informative[i] <- sqrt(mean((
          simulation_result[[i]]$pmmh_false_informative$state_mean - 
            simulation_result[[i]]$data_latent
        )[2:N]^2))
    }
}

# Plot theme
p_theme <- theme(
  legend.position = "bottom",
  legend.title = element_blank(), 
  legend.background = element_rect(fill = "transparent"),
  plot.margin = unit(c(0.2, 0.2, 0.2, 0.2), "cm"),
  plot.title = element_text(hjust = 0.5, size = 8),
  #axis.title = element_blank(),
  axis.title.x = element_text(size = 8),
  axis.title.y = element_text(size = 8, vjust = 0.5, angle = 0),
  legend.key = element_rect(fill = NA)
)

# Beta 0
df <- data.frame(
  lik = res_smc[, 1], setup1 = res_pmmh_noninformative[, 1],
  setup2 = res_pmmh_true_informative[, 1], 
  setup3 = res_pmmh_false_informative[, 1] 
)
df <- reshape2::melt(df)
df$type <- sapply(df$variable, function(x){ifelse(
  x == "lik", "Likelihood approach", "Bayesian approach")
})
p1 <- ggplot(df, aes(x = variable, y = value, fill = type), colour = "black") + 
  stat_boxplot(geom = 'errorbar') + geom_boxplot() + 
  geom_hline(
    aes(yintercept = true_params_simulation["beta_0"], colour = "True"), 
    linetype = 2, show.legend = TRUE, size = 1
  ) + 
  scale_colour_manual(values = c("grey")) + 
  theme_light() + p_theme +
  scale_x_discrete(
    labels = c(
      "lik" = "Nelder-Mead", "setup1" = "Setup 1", "setup2" = "Setup 2",
      "setup3" = "Setup 3"
    )
  ) +
  labs(y = latex2exp::TeX("$\\hat{\\beta}_0$"), x = "")

# Beta 1
df <- data.frame(
  lik = res_smc[, 2], setup1 = res_pmmh_noninformative[, 2],
  setup2 = res_pmmh_true_informative[, 2], 
  setup3 = res_pmmh_false_informative[, 2] 
)
df <- reshape2::melt(df)
df$type <- sapply(df$variable, function(x){ifelse(
  x == "lik", "Likelihood approach", "Bayesian approach")
})
p2 <- ggplot(df, aes(x = variable, y = value, fill = type), colour = "black") + 
  stat_boxplot(geom = 'errorbar') + geom_boxplot() + 
  geom_hline(
    aes(yintercept = true_params_simulation["beta_1"], colour = "True"), 
    linetype = 2, show.legend = TRUE, size = 1
  ) + 
  scale_colour_manual(values = c("grey")) + 
  theme_light() + p_theme +
  scale_x_discrete(
    labels = c(
      "lik" = "Nelder-Mead", "setup1" = "Setup 1", "setup2" = "Setup 2",
      "setup3" = "Setup 3"
    )
  ) +
  labs(y = latex2exp::TeX("$\\hat{\\beta}_1$"), x = "")

# Beta 2
df <- data.frame(
  lik = res_smc[, 3], setup1 = res_pmmh_noninformative[, 3],
  setup2 = res_pmmh_true_informative[, 3], 
  setup3 = res_pmmh_false_informative[, 3] 
)
df <- reshape2::melt(df)
df$type <- sapply(df$variable, function(x){ifelse(
  x == "lik", "Likelihood approach", "Bayesian approach")
})
p3 <- ggplot(df, aes(x = variable, y = value, fill = type), colour = "black") + 
  stat_boxplot(geom = 'errorbar') + geom_boxplot() + 
  geom_hline(
    aes(yintercept = true_params_simulation["beta_2"], colour = "True"), 
    linetype = 2, show.legend = TRUE, size = 1
  ) + 
  scale_colour_manual(values = c("grey")) + 
  theme_light() + p_theme +
  scale_x_discrete(
    labels = c(
      "lik" = "Nelder-Mead", "setup1" = "Setup 1", "setup2" = "Setup 2",
      "setup3" = "Setup 3"
    )
  ) +
  labs(y = latex2exp::TeX("$\\hat{\\beta}_2$"), x = "")

# sigma_x
df <- data.frame(
  lik = res_smc[, 4], setup1 = res_pmmh_noninformative[, 4],
  setup2 = res_pmmh_true_informative[, 4], 
  setup3 = res_pmmh_false_informative[, 4] 
)
df <- reshape2::melt(exp(df))
df$type <- sapply(df$variable, function(x){ifelse(
  x == "lik", "Likelihood approach", "Bayesian approach")
})
p4 <- ggplot(df, aes(x = variable, y = value, fill = type), colour = "black") + 
  stat_boxplot(geom = 'errorbar') + geom_boxplot() + 
  geom_hline(
    aes(yintercept = exp(true_params_simulation["sigma_x"]), colour = "True"), 
    linetype = 2, show.legend = TRUE, size = 1
  ) + 
  scale_colour_manual(values = c("grey")) + 
  theme_light() + p_theme +
  scale_x_discrete(
    labels = c(
      "lik" = "Nelder-Mead", "setup1" = "Setup 1", "setup2" = "Setup 2",
      "setup3" = "Setup 3"
    )
  ) +
  labs(y = latex2exp::TeX("$\\hat{\\sigma}_u$"), x = "")

# sigma_y
df <- data.frame(
  lik = res_smc[, 5], setup1 = res_pmmh_noninformative[, 5],
  setup2 = res_pmmh_true_informative[, 5], 
  setup3 = res_pmmh_false_informative[, 5] 
)
df <- reshape2::melt(exp(df))
df$type <- sapply(df$variable, function(x){ifelse(
  x == "lik", "Likelihood approach", "Bayesian approach")
})
p5 <- ggplot(df, aes(x = variable, y = value, fill = type), colour = "black") + 
  stat_boxplot(geom = 'errorbar') + geom_boxplot() + 
  geom_hline(
    aes(yintercept = exp(true_params_simulation["sigma_y"]), colour = "True"), 
    linetype = 2, show.legend = TRUE, size = 1
  ) + 
  scale_colour_manual(values = c("grey")) + 
  theme_light() + p_theme +
  scale_x_discrete(
    labels = c(
      "lik" = "Nelder-Mead", "setup1" = "Setup 1", "setup2" = "Setup 2",
      "setup3" = "Setup 3"
    )
  ) +
  labs(y = latex2exp::TeX("$\\hat{\\sigma}_\\epsilon$"), x = "")

# phi
df <- data.frame(
  lik = res_smc[, 6], setup1 = res_pmmh_noninformative[, 6],
  setup2 = res_pmmh_true_informative[, 6], 
  setup3 = res_pmmh_false_informative[, 6] 
)
df <- reshape2::melt(tanh(df))
df$type <- sapply(df$variable, function(x){ifelse(
  x == "lik", "Likelihood approach", "Bayesian approach")
})
p6 <- ggplot(df, aes(x = variable, y = value, fill = type), colour = "black") + 
  stat_boxplot(geom = 'errorbar') + geom_boxplot() + 
  geom_hline(
    aes(yintercept = tanh(true_params_simulation["phi"]), colour = "True"), 
    linetype = 2, show.legend = TRUE, size = 1
  ) + 
  scale_colour_manual(values = c("grey")) + 
  theme_light() + p_theme +
  scale_x_discrete(
    labels = c(
      "lik" = "Nelder-Mead", "setup1" = "Setup 1", "setup2" = "Setup 2",
      "setup3" = "Setup 3"
    )
  ) +
  labs(y = latex2exp::TeX("$\\hat{\\phi}$"), x = "")

# x_0
df <- data.frame(
  lik = res_smc[, 7], setup1 = res_pmmh_noninformative[, 7],
  setup2 = res_pmmh_true_informative[, 7], 
  setup3 = res_pmmh_false_informative[, 7] 
)
df <- reshape2::melt(df)
df$type <- sapply(df$variable, function(x){ifelse(
  x == "lik", "Likelihood approach", "Bayesian approach")
})
p7 <- ggplot(df, aes(x = variable, y = value, fill = type), colour = "black") + 
  stat_boxplot(geom = 'errorbar') + geom_boxplot() + 
  geom_hline(
    aes(yintercept = true_params_simulation["x_0"], colour = "True"), 
    linetype = 2, show.legend = TRUE, size = 1
  ) + 
  scale_colour_manual(values = c("grey")) + 
  theme_light() + p_theme +
  scale_x_discrete(
    labels = c(
      "lik" = "Nelder-Mead", "setup1" = "Setup 1", "setup2" = "Setup 2",
      "setup3" = "Setup 3"
    )
  ) +
  labs(y = latex2exp::TeX("$\\hat{\\x}_0$"), x = "")

# mse
df <- data.frame(
  lik = mse_smc, setup1 = mse_pmmh_noninformative,
  setup2 = mse_pmmh_true_informative, 
  setup3 = mse_pmmh_false_informative
)
df <- reshape2::melt(df)
df$type <- sapply(df$variable, function(x){ifelse(
  x == "lik", "Likelihood approach", "Bayesian approach")
})
p8 <- ggplot(df, aes(x = variable, y = value, fill = type), colour = "black") + 
  stat_boxplot(geom = 'errorbar') + geom_boxplot() + 
  theme_light() + p_theme +
  scale_x_discrete(
    labels = c(
      "lik" = "Nelder-Mead", "setup1" = "Setup 1", "setup2" = "Setup 2",
      "setup3" = "Setup 3"
    )
  ) +
  labs(y = latex2exp::TeX("RMSE"), x = "")

legend <- gtable::gtable_filter(ggplot_gtable(ggplot_build(p2)), "guide-box")

grid.arrange(
  arrangeGrob(
    p1 + theme(legend.position = "none"), 
    p2 + theme(legend.position = "none"), 
    p3 + theme(legend.position = "none"), 
    p4 + theme(legend.position = "none"), 
    p5 + theme(legend.position = "none"), 
    p6 + theme(legend.position = "none"), 
    p7 + theme(legend.position = "none"), 
    p8 + theme(legend.position = "none", 
               axis.title.y = element_text(angle = 90)),
    ncol = 2
  ),
  arrangeGrob(grid::nullGrob(), legend, grid::nullGrob(), nrow = 1),
  heights = c(4.5, 0.1), ncol = 1
)
```
