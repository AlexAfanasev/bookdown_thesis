--- 
site: bookdown::bookdown_site
documentclass: book
classoption: oneside
bibliography: [book.bib, packages.bib]
biblio-style: apalike
toc: true
toc_depth: 3
toc_appendix: yes
indent: true
link-citations: yes
lot: true
lof: true
fontsize: 11pt
geometry: "left=3.5cm, right=3.5cm, top=2.5cm, bottom=2.5cm"
---

```{r, global_setup, include=FALSE}
# 1. automatically create a bib database for used R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

---
output: pdf_document
editor_options: 
  chunk_output_type: console
---

\mainmatter

# Introduction {#intro}

Discounted cash flow models are widely used in the financial industry for determining the present value of an investment. @campbell_shiller_paper present, a log linear approximation of the present-value formulation. They show that the current log price-to-dividend ratio (PD) is approximately the sum of a constant plus the difference of discounted, expected future log returns and future log dividend growth. Furthermore, within their setup, both the constant and the discount rate depend on the long-term mean of the PD.

A major assumption for this setup is the existence of the long term mean requiring stationarity of the PD. This requirement has been tested and evaluated in [@dog_bark_paper; @h2_paper]. Contrary to the assumption made by Campbell and Shiller, the results indicate that the PD is indeed nonstationary. Therefore, a more realistic assumption is the nonexistence of the long-term mean. Hence, the discount rate and constant should be regarded as time varying. 

Based on these findings there have been multiple suggestions for modeling the PD as a nonstationary process. Partially observed Markov processes, also called state-space models, can be used for handling the time varying long-term mean. To be more precise, it can be handled as a latent process that is directly connected to the observation process of the PD. 

As an example, the latent mean can be modeled with discrete shifts as proposed in [@van_nieuwerburgh_paper]. However, this project will be based mostly on the results depicted in [@h2_paper] where a nonlinear state-space model has been applied.

Nonlinear partially observed Markov processes have been successfully used for a variety of statistical modeling tasks for time series data. Besides the already mentioned application for the PD [@h2_paper], further exemplary applications are stochastic volatility models [@stochastic_vol_models], infectious disease models [@covid_paper] or ecological research for predator-prey analysis [@predator_prey_analysis]. Typically, fitting them is computationally very expensive. Hence, fast and robust statistical software is required.

Because of the nonlinear structure of these models, complex Monte Carlo based methods have to be used for obtaining a stochastic estimate of the likelihood function. Trying to use deterministic optimization routines has drawbacks due to inability of handling the stochasticity of the objective function. Luckily, during the last years further advances in estimation procedures for nonlinear state-space models have been made. As a possible solution Bayesian approaches have been proposed. Of great interest will be the recently developed particle Markov chain Monte Carlo methods, in particular the particle marginal Metropolis Hastings procedure (PMMH), by [@doucet_pmmh].

This master's thesis will extend the work presented in [@h2_paper] by applying Bayesian techniques to the problem of filtering the latent mean of the PD. All in all, this project will have the following goals:

1) Using a nonlinear model setup, compare the estimation results using a deterministic optimization routine with the results from the Bayesian PMMH procedure.

2) Using a model setup similar to the one used in [@h2_paper], apply the Bayesian approach. Estimate and compare different state processes with and without exogenous covariates.

Chapter two will start with a review of the existing literature. Namely already applied state-space models in the context of the log linear approximation of the present-value formulation will be presented. Moreover, conducted research around macroeconomic variables affecting the latent mean of the PD, new inferential procedures surrounding nonlinear state-space models and corresponding software will be inspected. The third chapter will present the basic structure of partially observed Markov processes. There, likelihood based and Bayesian approaches will be explained. In chapter four a simulation study, that is closely related to the empirical application, will be conducted. The likelihood based approach will be compared with the PMMH approach using different prior specifications. Chapter five will present the results of the empirical application. The latent mean of the PD will be filtered and the state and observation process parameters estimated. Besides that, the effect of further macroeconomic variables will be tested, and a state process containing these exogenous covariates evaluated. Finally, the last chapter will be the conclusion and summarize everything.

<!--chapter:end:Rmd//01-Introduction.Rmd-->

# Literature review {#literature-review}

A variety of state space-models have been applied in the context of the log linear approximation of the present-value formulation. In particular, assuming stationarity of the PD and hence the existence of the long-term mean, Binsbergen and Koijen [@binsbergen_paper] used a linear state-space model for modeling state processes for the expected return and expected dividend growth. Furthermore, in [@tvp_kf] the possibility of time varying parameters has been added to this application.

However, as mentioned, this thesis will focus on the results from [@h2_paper] where the PD has been modeled as a nonstationary process. There the latent mean has been characterized as a random walk and as a autoregressive process of order one. The results indicate that the random walk model is superior measured by typical model comparison figures as the Bayesian information criterion. 

Furthermore, parameter inference has been carried out by maximizing the likelihood using the Nelder–Mead optimization method, see [@nelder_mead]. Using this approach, due to the stochasticity of the likelihood function, the Hessian cannot be approximated reliably and therefore confidence intervals for the parameters based on the observed Fisher information cannot be retrieved. 

In [@h2_paper2] a similar nonlinear state-space model has been applied, however for the price-to-rent ratio of the US housing market using the Federal Housing Finance Agency housing price index. Parameter inference has been conducted using multiple runs of a grid search procedure. Again, uncertainty quantification of parameter estimates is not feasible within this setup.

This master's thesis will therefore try to extend the summarized approaches by applying the Bayesian PMMH approach. Hence, after having obtained an approximation of the posterior distribution it is possible to retrieve credible intervals for the parameters of interest and pointwise credible intervals for the filtered state process.

Besides that, using the filtered latent mean possible determinants have been studied within a cointegration analysis in [@h2_paper]. This analysis has shown that a cointegration relationship between the latent mean of the PD, consumption risk, the middle-aged to young ratio and the proportion of firms with traditional dividend payout policy exists. Hence, adding to these results, further macroeconomic variables will be inspected via a cointegration analysis and by estimating a state process extension containing exogenous covariates directly.

In particular, the effect of monetary policy on the latent mean of the PD will be inspected. Empirical evidence [@monetary_policy_1; @monetary_policy_2; @monetary_policy_asset_prices] has shown that monetary policy, especially quantitative easing, affects asset prices. Therefore, the effects of changes in real M1 money supply growth and the effective federal funds rate will be investigated. Moreover, the possible link between economic growth and increasing stock prices will be checked as well, even though research indicates that this relationship is questionable [@economic_growth].

As mentioned, nonlinear state-space models require fast running software due to complex Monte Carlo procedures being employed. For the \texttt{R} programming language three possible and freely available packages can be used for modeling nonlinear state-space models: \texttt{Biips} [@biips_paper], \texttt{NIMBLE} [@nimble_paper], \texttt{pomp} [@pomp_article]. The first two are extensions of the BUGS software for Bayesian analysis using Markov chain Monte Carlo (MCMC), see [@bugs_paper]. 

For this masters thesis the \texttt{pomp} package has been used. Contrary to the other two it contains simulation based algorithms for both Bayesian and likelihood based procedures. Furthermore, its plug and play architecture makes it easy to develop and test new models. Moreover, for speeding up the inferential procedures, it allows to program lower level code in \texttt{C}. Besides that, within this project all applications have been run on a powerful cloud server, parallel computing has been used and some key code parts have been written in \texttt{C} and \texttt{C++} using \texttt{Rcpp} [@rcpp] and \texttt{RcppArmadillo} [@rcpparmadillo].

<!--chapter:end:Rmd//02-Literature-Review.Rmd-->

---
output: pdf_document
bibliography: ../book.bib
editor_options: 
  chunk_output_type: console
---
# Theoretical framework {#theory}

This chapter will present a brief description of the basic structure of partially observed Markov processes. Furthermore, relevant algorithms for performing likelihood based and Bayesian inference will be introduced and their disadvantages and advantages elaborated. However, the content of this master's thesis will be limited to the case of discrete time non-Gaussian and nonlinear state-space models and it will be based upon the literature of [@pomp_article; @doucet_inference; @doucet_johansen_tutorial; @cappe_smc; @doucet_pmmh].

## Partially observed Markov processes

Partially observed Markov processes, also known as state-space models, consist of an unobserved typically continuous-valued Markov state process which is directly connected to the observation process. 

As illustrated in [@pomp_article], let $\theta \in \mathbb{R}^p$ be the $p$-dimensional vector of model parameters and $\{X(t;\;\theta), t\in T\}$ be the latent state process with $T \subset \mathbb{N}$ and $X(t;\;\theta) \in \mathbb{R}^q$. The times at which $X(t;\;\theta)$ is partially observed are denoted $\{t_i \in T, i=1, ..., N\}$ with $t_0 \leq t_1 < t_2 < ... < t_N$ and $t_0 \in T$ being the initial time. Furthermore, the following representation is set $X_i = X(t_i; \theta)$ and $X_{i:j} = (X_i, X_{i+1}, ..., X_j)$. Besides that, one observes the latent state process by way of a $r$-dimensional observation process $Y_{1:N}$ with $Y_n \in \mathbb{R}^r$.

The state process can be described with an initial density $\mu_{\theta}(x_{0})$ and a transition density $f_{\theta}(x_{n}|x_{n-1})$
\begin{equation}
\begin{split}
    X_{0} & \sim \mu_{\theta}(x_{0}), \\
    X_{n} | (X_{0:n-1} = x_{0:n-1}) & \sim f_{\theta}(x_{n} | x_{n-1}),
\end{split}
\end{equation}
The observation process has the following specification
\begin{equation}
    Y_{n} | (X_{0:n} = x_{0:n}, Y_{1:n-1} = y_{1:n-1}) \sim g_{\theta}(y_{n} | x_{n})
\end{equation}
where $g_{\theta}(y_{n}|x_{n})$ represents the conditional marginal density, see [@doucet_inference].

The transition density $f_{\theta}(x_{n}|x_{n-1})$, measurement density $g_{\theta}(y_{n}|x_{n})$ and initial density $\mu_{\theta}(x_{0})$ specify the joint density of the system
\begin{equation}
    p_{\theta}(x_{0:N}, y_{1:N}) = p_{\theta}(y_{1:N}|x_{0:N})p_{\theta}(x_{0:N}) =  \mu_{\theta}(x_{0})\prod^{N}_{n=1}f_{\theta}(x_{n} | x_{n-1})g_{\theta}(y_{n} | x_{n})
    (\#eq:pomp-joint-density)
\end{equation}
Using the joint density \@ref(eq:pomp-joint-density) it is possible to obtain the marginal density
\begin{equation}
    p_{\theta}(y_{1:n}) = \int p_{\theta}(x_{0:n}, y_{1:n})dx_{0:n}
    (\#eq:pomp-marginal-density)
\end{equation}
Within this setup the state process represents a first order Markov process. Higher order Markov state processes can be easily constructed by extending the dimension of the state space. As an example, given a state process with transition density $X_{n} | (X_{0:n-1} = x_{0:n-1}) \sim f_{\theta}(x_{n} | x_{n-1}, x_{n-2})$ one can construct a new process $S_{1:N} = ((X_0, X_1), (X_1, X_2), ..., (X_{N-1}, X_N))$ which has the first order Markov process structure. Additionally, it is possible to add further dependence structures to the basic model by including exogenous covariates. The observed covariate process $Z_{1:N} = (Z_1, ..., Z_N)$ can be added to the observation model $g_{\theta}(y_{n}|x_{n}, z_{n})$ and the state model $f_{\theta}(x_{n}|x_{n-1}, z_{n})$. Figure \@ref(fig:pomp-model-graphic) displays the structure of a state-space model containing exogenous covariates within the state process.

```{r pomp-model-graphic, fig.align='center', fig.cap='Exemplary state-space model with covariates', echo=FALSE}
knitr::include_graphics(here::here("images", "pomp_model_graph.pdf"))
```

Typically, one can separate two setups. Firstly, the case where the latent state and the observation processes are Gaussian with a linear relationship. This setup has an analytical solution to the parameter inference and hidden state estimation problem which is based upon the famous Kalman filter [@kalman_filter]. Secondly, there is the general case where the processes are not necessarily Gaussian and the relationship between the state and observation processes is not necessarily linear. Furthermore, their processes themselves do not have to be linear. Unfortunately, these models are more difficult to fit and require powerful simulation techniques [@doucet_inference].

## Particle filtering

Given a fixed collection of observations $y^*_{1:N} = (y^*_1, ..., y^*_N)$ one is interested in carrying out inference about $\theta$ and $x_{0:N} = (x_0, ..., x_N)$. The two methods at hand, likelihood based inference and Bayesian inference, require the evaluation of the likelihood function.

Typically, one separates the joint likelihood $\mathcal{L}(\theta, x_{0:N})$, of the parameter vector $\theta$ and the state process $x_{0:N}$, and the marginal likelihood $\mathcal{L}(\theta)$. The former has the following representation and can be obtained using the joint density \@ref(eq:pomp-joint-density)
\begin{equation}
\begin{split}
  \mathcal{L}(\theta, x_{0:N}) & = p_{\theta}(x_{0:n}, y^*_{1:n}) \\
  & = \mu_{\theta}(x_{0})\prod^{N}_{n=1}f_{\theta}(x_{n} | x_{n-1})g_{\theta}(y^*_{n} | x_{n})
  (\#eq:pomp-joint-likelihood)
\end{split}
\end{equation}
and the marginal likelihood for parameter $\theta$ is the marginal density \@ref(eq:pomp-marginal-density) evaluated at the collected data
\begin{equation}
\begin{split}
  \mathcal{L}(\theta) = p_{\theta}(y^*_{1:n}) & = \int p_{\theta}(x_{0:n}, y^*_{1:n})dx_{0:n} \\
  & = \int \mu_{\theta}(x_{0})\prod^{N}_{n=1}f_{\theta}(x_{n} | x_{n-1})g_{\theta}(y^*_{n} | x_{n})dx_{0:N}
  (\#eq:pomp-likelihood)
\end{split}
\end{equation}
Given that parameter inference will be the main goal, the focus will be on the marginal likelihood \@ref(eq:pomp-likelihood) and it will be called likelihood in the following.

For finite state hidden Markov models the integrals would correspond to finite sums and the likelihood could be computed exactly. Furthermore, for linear Gaussian models Kalman filter techniques can be used for evaluating the likelihood exactly. However, for nonlinear and non-Gaussian models it is not possible to compute the likelihood in closed-form and numerical methods have to be employed, [@doucet_johansen_tutorial]. 

A naive approach for estimating the likelihood function \@ref(eq:pomp-likelihood) using Monte Carlo integration, as presented in the \texttt{pomp} documentation [@king_pomp], can be based upon the Markov property
\begin{equation}
\begin{split}
  p_{\theta}(x_{0:N}) & = \mu_{\theta}(x_0) \cdot f_{\theta}(x_1|x_0) \cdot f_{\theta}(x_2|x_1) \cdot \; ... \; \cdot f_{\theta}(x_{N}|x_N-1) \\
  & = \mu_{\theta}(x_0) \prod^{N}_{n=1}f_{\theta}(x_{n}|x_{n-1})
  (\#eq:markov-property)
\end{split}
\end{equation}
Hence, by directly sampling a set of trajectories $\{x_{0:N}^j, j = 1,..., J\}$ of size $J$ from \@ref(eq:markov-property), one can compute a numerical approximation of the likelihood
$$\mathcal{L}(\theta) = \mathbb{E}\Bigg[\prod^{N}_{n=1}g_{\theta}(y^*_{n} | x_{n})\Bigg] \approx \frac{1}{J}\sum^J_{j=1}\prod^{N}_{n=1}g_{\theta}(y^*_{n} | x^j_{n})$$
However, this setup would require a very large number of samples $J$ in order to obtain a relatively good approximation, since most of the trajectories diverge a lot from the true state and therefore yield extremely small likelihoods. Additionally, as a trajectory diverges from the true process, it will almost never come back.

A better approach would be to generate samples of the state process $X_{0:N}$ conditional on the observed data $y^*_{1:N}$. Sequential Monte Carlo methods, alias particle filters, pick up this idea by combining importance sampling and resampling steps using a cloud of particles. In the following the basic idea of the bootstrap particle filter, see [@bootstrap_filter], will be explained.

As a first step, the likelihood of the partially observed Markov model can be factorized differently
\begin{equation*}
\begin{split}
  \mathcal{L}(\theta) & = \prod^{N}_{n=1}p_{\theta}(y^*_n|y^*_{1:n-1}) \\
  & = \prod^{N}_{n=1}\int p_{\theta}(x_n, y^*_n|y^*_{1:n-1}) dx_n  \\
  & =\prod^{N}_{n=1} \int g_{\theta}(y^*_n|x_{n})p_{\theta}(x_n|y^*_{1:n-1})dx_n
\end{split}
\end{equation*}
Now, the likelihood can be estimated by approximating the integral at each time point $t_n$, as shown in [@cappe_smc]. One can think of this task as applying Monte Carlo integration by drawing samples from the so called prediction distribution $p_{\theta}(x_n|y^*_{1:n-1})$. Drawing samples from this distribution can only be done by using a so called filtering or updating distribution $p_{\theta}(x_n|y^*_{1:n})$, as can be seen by extending the prediction distribution, see the derivations in the [appendix][Prediction & Filtering distribution]
\begin{equation*}
\begin{split}
  p_{\theta}(x_n|y^*_{1:n-1}) & = \int p_{\theta}(x_n, x_{n-1}|y^*_{1:n-1})dx_{n-1} \\
  & = \int f_{\theta}(x_n|x_{n-1})p_{\theta}(x_{n-1}|y^*_{1:n-1})dx_{n-1}
\end{split}
\end{equation*}
The filtering distribution is obtained by applying Bayes theorem, see the derivations in the [appendix][Prediction & Filtering distribution]
\begin{equation*}
\begin{split}
  p_{\theta}(x_n|y^*_{1:n}) & = p_{\theta}(x_n|y^*_n, y^*_{1:n-1}) = \frac{g_{\theta}(y^*_n|x_{n})p_{\theta}(x_n|y^*_{1:n-1})}{p_{\theta}(y^*_n|y^*_{1:n-1})}
\end{split}
\end{equation*}
Hence, for each time point, $t_n, n= 1, ...., N$, it is possible to obtain the prediction distribution using the filtering distribution from $t_{n-1}$. One can obtain the filtering distribution at $t_n$ from the prediction distribution at $t_n$. Furthermore, the filtering distribution at $t_0$ is just the initial value distribution $\mu_{\theta}(x_0)$.

At $t_0$ one starts by sampling values from the initial value distribution $\mu_{\theta}(x_0)$ in order to get a sample from the filtering distribution, $x^F_{0, j}$, $j = 1, ..., J$. Then, at $t_{1}$ a sample from the prediction distribution, $x^P_{1,j}$, $j = 1, ..., J$ can be obtained by sampling from $f_{\theta}(x^P_{1, j}|x^F_{0, j})$. Particles with low weights are discarded by resampling $\{x^P_{n, j}\;,j \in1:J\}$ with weights $w_{n, j} = g_{\theta}(y^*_n|x^P_{n, j})$ and one obtains a sample from the filtering distribution at $t_1$, $x^F_{1, j}$, $j = 1, ..., J$. This process will be repeated for all time points.

In general, a variety of resampling schemes are available with the basic schema being multinomial resampling. However, the systematic resampling approach will be used, based upon the results depicted in [@resampling_schemes] where multiple approaches have been compared.

Moreover, at each time point the conditional log likelihood can be approximated and a filtering estimate for the state can be computed
\begin{equation*}
\begin{split}
  \hat{\ell}_{n|n-1} & = \ln\Bigg(\frac{1}{J}\sum^J_{j=1}w_{n, j}\Bigg) \\
  \hat{x}_n & = \frac{1}{J}\sum^J_{j=1}x^F_{n, j}
\end{split}
\end{equation*}
Note that the filtered state is an estimated value for the following conditional expectation: 
$$\mathbb{E}[X_n | Y_1 = y_1^*, ..., Y_n = y_n^*]$$
After having iterated through this sampling, resampling procedure, the full log likelihood approximation can be computed:
$$\hat{\ell}(\theta) = \sum_{n=1}^N\hat{\ell}_{n|n-1}$$
An exact description of these algorithms can be found in [@pomp_article] and the algorithms in the [appendix][Particle Filter and Systematic resampling]. 

## Likelihood based inference

The objective of likelihood based inference is to find the vector of parameters $\theta$ for which the observations $y^*_{1:N}$ are most likely, under the chosen model setup. Hence, this process can be translated into maximizing the objective function, the likelihood function \@ref(eq:pomp-likelihood), with respect to $\theta$ evaluated at the collected data $y^*_{1:N}$. The parameter vector that maximizes the likelihood is called the maximum likelihood estimate
\begin{equation*}
  \hat{\theta} = \argmax_{\theta}\mathcal{L}(\theta)
\end{equation*}
Since the natural logarithm is a monotonically increasing function, one obtains equal results by maximizing the log likelihood function instead.

The particle filter returns a stochastic estimate of the likelihood function which has been shown to be unbiased, see [@del_moral]. Furthermore, with increasing number of particles $J$ the variance in the estimation can be reduced significantly but it cannot be removed completely. It is natural to assume that the maximization problem translates into maximizing the particle filters likelihood estimate.

Unfortunately, the stochasticity of the likelihood estimate creates issues for most optimization routines, as the ones defined in the \texttt{R} \texttt{optim} function, which assume that the objective function is evaluated deterministically. Furthermore, the analytical properties of the target function, as smoothness and concavity, assumed by most deterministic numerical optimization algorithms, are often paramount.

The particle filter does not return information about the derivative. Approximating the derivative via finite difference methods is not promising. Since the likelihood function is already a noisy estimate, one can expect the derivative estimates to be even noisier. Equivalently, confidence intervals can not be obtained numerically since approximating the Hessian via finite difference methods, in order to get the observed Fisher information, is not feasible. Hence, a derivative-free optimization algorithm has to be chosen. \texttt{R}'s \texttt{optim} default method, the Nelder-Mead algorithm, can be used for this case.

However, using this optimization algorithm does not come without drawbacks. Firstly, repeatedly applying the Nelder-Mead procedure will yield different maximum likelihood estimates due to the stochasticity of the likelihood estimate. Hence, it makes sense to use a very large number of particles, in order to get small Monte Carlo errors in the likelihood estimates, and to compare end results from multiple rounds of applying the Nelder-Mead optimization. Unfortunately, this adds more computational complexity and amplifies the slowness, one of the major drawbacks of this optimization algorithm.

There have been recent scientific advances that solve the mentioned problems within likelihood based inference for partially observed Markov processes. A very interesting alternative by the name iterated filtering has been introduced in [@iterated_filtering] and further improved in [@iterated_filtering_improved]. The basic idea of the iterated filtering algorithm will be introduced briefly. However, for simplification likelihood based inference, within this master's thesis, will be limited to the application of the Nelder-Mead algorithm.

Iterated filtering is a simulation based algorithm that uses a particle filter for an artificial state-space model where the parameters are following a random walk process. The perturbations of the parameters are successively reduced and it has been shown that the algorithm converges towards the maximum likelihood estimate [@iterated_filtering_improved]. Furthermore, it is possible to compute confidence intervals by constructing profile likelihoods for relevant parameters and applying the so called Monte Carlo adjusted profile algorithm. This approach is complicated and computationally intensive but the advantage of this methodology is that it adjusts the width of the confidence interval by accounting for the Monte Carlo error in the likelihood approximation, see [@mcap_algorithm]. 

## Bayesian inference

Bayesian inference relies on the idea that the observed data updates the formulated prior believe, yielding the posterior distribution. Within the context of partially observed Markov processes it is possible to define two related approaches that require a prior specification $\pi(\theta)$ on the unknown parameter vector $\theta$.

Both cases will be described in the context of the random walk Metropolis Hastings algorithm. This procedure provides a simulation based approach for obtaining samples from the posterior distribution. The basic idea is that for a given starting parameter $\theta^{(0)}$ and a requested number of runs $M$, at each step $m = 1, ..., M$ a random walk proposal for each parameter $\theta^*_i$ is generated based on 
$$\theta_i^{*}\sim \mathcal{N}(\theta_i^{(m-1)}, \lambda_i^2)$$
where $\lambda_i$ is the jump size of the random walk proposal for parameter $\theta^*_i$.

The proposal density will be denoted with $q(\theta^*|\theta^{(m-1)})$ and the proposal will then be accepted with probability $\alpha(\theta^{*}|\theta^{(m-1)})$. If it is accepted one sets $\theta^{(m)} = \theta^{*}$ and otherwise $\theta^{(m)} = \theta^{(m-1)}$. Typically, these algorithms require the evaluation of the likelihood function. However, in the case of nonlinear, non-Gaussian state-space models it is only possible to obtain estimates of the likelihood function. Hence, both approaches use the particle filter within the Metropolis Hastings algorithm in order get an unbiased estimate of the likelihood \@ref(eq:pomp-likelihood). Note that this makes these algorithms very computationally intensive since at each iteration $m$ a likelihood estimate has to be obtained via the particle filter.

The first case, the pseudo marginal Metropolis Hastings algorithm [@pseudo_marginal], targets the posterior distribution
$$p(\theta|y^*_{1:N}) = \frac{\mathcal{L}(\theta)\pi(\theta)}{p(y^*_{1:N})}$$
where the denominator $p(y^*_{1:N})$ represents the probability of the data and is obtained from the marginal density \@ref(eq:pomp-marginal-density)
$$p(y^*_{1:N}) = \int p_\theta(y^*_{1:N}) d\theta$$
Luckily, the sampling procedure does not require the evaluation of the denominator since the acceptance probability simplifies to
\begin{equation}
\begin{split}
  \alpha(\theta^{*}|\theta^{(m-1)}) = & \min\Bigg\{1,\; \frac{p(\theta^*|y^*_{1:N} )\;q(\theta^{(m-1)}|\theta^*)}{p(\theta^{(m-1)}|y^*_{1:N})\;q(\theta^*|\theta^{(m-1)})}\Bigg\} \\
   & = \min\Bigg\{1, \frac{\mathcal{L}(\theta^*)\;\pi(\theta^*)\;q(\theta^{(m-1)}|\theta^*)}{\mathcal{L}(\theta^{(m-1)})\;\pi(\theta^{(m-1)})\;q(\theta^*|\theta^{(m-1)})}\Bigg\}
(\#eq:acceptance-marginal-approach)
\end{split}
\end{equation}
The second case targets the joint posterior density $p(x_{0:N}, \theta|y^*_{1:N})$, see [@doucet_inference], which can be obtained from using the joint likelihood \@ref(eq:pomp-joint-likelihood)
$$
p(x_{0:N}, \theta|y^*_{1:N}) = \frac{\mathcal{L}(\theta, x_{0:N})\pi(\theta)}{p(y^*_{1:N})}
$$
Besides requiring a proposal for the parameter vector $\theta$, this approach needs an additional proposal for the latent state process. A simple approach would be to generate the state proposal independent of the observed data $y^*_{1:N}$. As with the naive Monte Carlo integration example, explained within the [particle filtering][Particle filtering] section, this approach would yield trajectories that diverge a lot from the true state.

Hence, the particle marginal Metropolis Hastings approach (PMMH), uses the proposal $\theta^*$ and the observed data in order to generate a proposal $x^*_{0:N}$. The pair $(\theta^*, x^*_{0:N})$ can be generated from the following proposal density, see [@doucet_pmmh]
$$q(\theta^*, x^*_{0:N}|\theta^{(m-1)}, x^{(m-1)}_{0:N}) = q(\theta^*|\theta^{(m-1)})p_{\theta^*}(x^*_{0:N}|y^*_{1:N})$$
The resulting acceptance probability reduces to the same probability as for the marginal approach \@ref(eq:acceptance-marginal-approach), see the derivations in the [appendix][PMMH - acceptance probability] and [@doucet_pmmh]
\begin{equation}
\begin{split}
\alpha(\theta^{*}|\theta^{(m-1)}) &= \min\Bigg\{1, \frac{p(\theta^*, x^*_{0:N}|y^*_{1:N})\;q(\theta^{(m-1)}, x^{(m-1)}_{0:N}|\theta^*, x^*_{0:N})}{p(\theta^{(m-1)}, x^{(m-1)}_{0:N}|y^*_{1:N})\;q(\theta^*, x^*_{0:N}|\theta^{(m-1)}, x^{(m-1)}_{0:N})}\Bigg\} \\
 & = \min\Bigg\{1, \frac{\mathcal{L}(\theta^*)\;\pi(\theta^*)\;q(\theta^{(m-1)}|\theta^*)}{\mathcal{L}(\theta^{(m-1)})\;\pi(\theta^{(m-1)})\;q(\theta^*|\theta^{(m-1)})}\Bigg\}
\end{split}
\end{equation}
Hence, drawing samples from the joint posterior density can be achieved by using the previous acceptance probability from the pseudo marginal approach \@ref(eq:acceptance-marginal-approach). Therefore, posterior samples for the latent state process can be obtained by storing a random trajectory from the particle cloud at each step $m$, obtained by the particle filter.

By sampling from the posterior distribution it is possible to obtain point estimates for the parameters and states (e.g., mean or mode) as well as Bayesian credible intervals as measures of uncertainty. Contrary to the estimated state $\hat{x}_n$ from the particle filter where the expectation conditional on the data $y_{1:n}^*$ has been approximated, the PMMH approach yields a state estimate by approximating the expectation conditional on the complete data set $y_{1:N}^*$:
$$\mathbb{E}[X_n|Y_1 = y_1^*, ..., Y_N= y_N^*]$$
The major difficulty of the random walk Metropolis Hastings procedure is setting appropriate jump size parameters. The goal is that the sampler explores the parameter space efficiently by rejecting and accepting not too many proposals. Within this master's thesis the jump size will be set based on inspecting initial runs of the PMMH procedure using a small sample size of $M = 5000$. In particular the jump size will be set such that the acceptance rate is between $15\%$ to $25\%$ for the initial tests.

Moreover, there are further possible improvements that deal with this problem. The parameter tuning can be automated by employing an adaptive Metropolis procedure that uses the previous history of the Chain to estimate a covariance matrix for the sampler, see [@adaptive_mcmc]. Additionally, gradient estimates have been successfully incorporated for generating proposals that are more likely to be accepted, see [@mala_pmmh1; @mala_pmmh2].

In the following, this master's thesis will be limited to the application of the PMMH algorithm. An overview of the procedure can be found in [@pomp_article] and in the algorithms [appendix][PMMH].

<!--chapter:end:Rmd//03-Theoretical-Framework.Rmd-->

---
output: pdf_document
editor_options: 
  chunk_output_type: console
---

# Simulation study {#simulation}

A simulation study will be conducted in order to compare the likelihood based inference procedure, via the Nelder-Mead algorithm, with the Bayesian approach, based on the PMMH algorithm. The simulation setup will be closely related to the empirical application. Hence, a variation of the log linear present-value formulation of the PD will be used.

Within the simulation study the latent state $x_n$ will take the form of an autoregressive distributed lag (ARDL) model and will be simulated according to the following process
\begin{equation}
\begin{split}
& x_0 = 3.5 \\
& x_{n} = \beta_0 + \phi x_{n-1} + \beta_{1}z_{1, n} + \beta_{2}z_{2, n} + u_{n} \\ 
& u_{n} \sim \mathcal{N}(0, \sigma_{u}^2)
(\#eq:simulation-study-state)
\end{split}
\end{equation}
with restriction $\left|\phi\right|<1$ and where $z_{1,n}$ and $z_{2,n}$ are exogenous covariates simulated according to
\begin{equation}
\begin{split}
& \begin{pmatrix} z_{1, 0} \\ z_{2, 0} \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \\
& \begin{pmatrix} z_{1, n} \\ z_{2, n} \end{pmatrix} = \begin{pmatrix} z_{1, n-1} \\ z_{2, n-1} \end{pmatrix} + \begin{pmatrix} w_{1, n} \\ w_{2, n} \end{pmatrix} \\
& \begin{pmatrix} w_{1, n} \\ w_{2, n} \end{pmatrix} \sim \mathcal{N}\Bigg(\begin{pmatrix} 0 \\ 0 \end{pmatrix}, 0.0025\;I_{2}\Bigg)
\end{split}
\end{equation}
This setup is an indirect specification of an error correction mechanism. The relationship to the error correction model can be seen by subtracting $x_{n-1}$ and adding and subtracting $\beta_{1}z_{1, n-1}$, $\beta_{2}z_{2, n-1}$ from both sides of the state process \@ref(eq:simulation-study-state)
\begin{equation*}
\begin{split}
 \Delta x_{n} & = \beta_{1}\Delta z_{1, n} + \beta_{2}\Delta z_{2, n} - (1 - \phi) x_{n-1} + \beta_0 + \beta_{1}z_{1, n-1} + \beta_{2}z_{2, n-1} + u_{n} \\ 
 & = \beta_{1} \Delta z_{1, n} + \beta_{2} \Delta z_{2, n} - (1 - \phi)\Bigg(x_{n-1} - \frac{\beta_0}{1 - \phi} - \frac{\beta_{1}}{1 - \phi}z_{1, n} - \frac{\beta_{2}}{1 - \phi}z_{2, n}\Bigg) + u_{n}
\end{split}
\end{equation*}
Hence, given that $\left|\phi\right|<1$, the long run relationship between the steady-state latent process, $x_{n} = x_{n-1} = \bar{x}$,  and the steady-state covariates, $z_{1, n} = z_{1, n-1} = \bar{z}_1$, $z_{2, n} = z_{2, n-1} = \bar{z}_2$, is specified by
$$\bar{x}=\frac{\beta_0}{1 - \phi} + \frac{\beta_{1}}{1 - \phi}\bar{z}_{1} + \frac{\beta_{2}}{1 - \phi}\bar{z}_{2}$$
The state process will be bound to be between the minimum, 2.775, and the maximum, 4.487, of the observed PD in the [empirical application][Empirical application].

Furthermore, the observation process has the following specification
\begin{equation}
\begin{split}
& y_{n} = \frac{k_{n}}{1 - \rho_{n}} + d_{n} +  \epsilon_{n} \\
& \epsilon_{n}\sim \mathcal{N}(0, \sigma_{\epsilon}^2)
\end{split}
\end{equation}
where $k_{n}$ and $\rho_n$ directly depend on the latent state $x_n$ via
\begin{equation}
\begin{split}
& \rho_{n} = \frac{1}{1+\text{exp}(-x_{n})}\\
& k_{n} = -\ln(\rho_{n})-(1-\rho_{n})\ln\bigg(\frac{1}{\rho_{n}} - 1\bigg)
\end{split}
\end{equation}
and for simplicity $d_{n}$ will be handled as an exogenous covariate arising from the following specification
\begin{equation}
\begin{split}
& d_{n} = -1 + 0.85(d_{n - 1} + 1) + 0.15v_{n-1} + v_{n} \\ 
& v_{n} \sim \mathcal{N}(0, 0.003)
\end{split}
\end{equation}
All in all, the processes $y_{n}$, $d_{n}$, $z_{1, n}$ and $z_{2, n}$ are observable whereas the state process $x_n$ is not. Note that the initial value problem of obtaining an estimate for $x_0$ can be solved within the Bayesian setup as well. In order to do this a prior for $x_0$ has to be assigned. Hence, the results of the likelihood based and Bayesian approach for estimating the latent state and the parameters $\theta = \begin{pmatrix} x_0 & \phi & \beta_0 & \beta_{1} & \beta_{2} & \sigma_{\epsilon} & \sigma_{u} \end{pmatrix}^T$ will be compared. For this simulation, the parameters will be set to
$$
\theta = \begin{pmatrix} x_0 \\ \phi \\ \beta_0 \\ \beta_{1} \\ \beta_{2} \\ \sigma_{\epsilon} \\ \sigma_{u} \end{pmatrix} = \begin{pmatrix} 3.5 \\0.85 \\ 0.525 \\ 0.4 \\ -0.2 \\ 0.02 \\ 0.05 \end{pmatrix}
$$
Since particle filter based inference methods are very computationally intensive, the simulation procedure will be applied $1000$ times for $N = 50$ simulated observations. For the likelihood based inference $10$ rounds of Nelder-Mead optimization will be run. At each of the $1000$ iterations the result with the highest estimated likelihood will be taken as the maximum likelihood estimate. Furthermore, for the Bayesian approach three different prior setups will be applied and compared. Finally, the likelihood based approach will be applied using a cloud of $J=2000$ particles and the PMMH procedures will use $J=500$ particles.

The Nelder–Mead algorithm was developed for unbounded optimization problems. However, this simulation setup has the following parameters $\{\phi, \sigma_{\epsilon}, \sigma_{u}\}$ that are bounded by $\left|\phi\right|<1$, $\sigma_u>0$ and $\sigma_{\epsilon}>0$. Hence, these transformations will be applied
\begin{align}
\phi &= \text{tanh}(\psi) & \sigma_u &= \text{exp}(\varsigma_u) & \sigma_{\epsilon} &= \text{exp}(\varsigma_{\epsilon})
(\#eq:parameter-transformation)
\end{align}
where $\psi, \varsigma_u, \varsigma_{\epsilon} \in \mathbb{R}$ are unconstrained hyperparameters on the real line. The resulting estimates for these hyperparameters can than easily be converted in order to obtain the initial parameters. 

Furthermore, parameter transformations are also beneficial to random walk Metropolis Hastings approaches. Without them, proposal values can be generated that violate the parameter constraints and this increases the autocorrelation of the Markov chain. Hence, the presented parameter transformation will be applied for the PMMH procedure as well. Therefore, both applications will use the following parameter vector $\theta = \begin{pmatrix} x_0 & \psi & \beta_0 & \beta_{1} & \beta_{2} & \varsigma_{\epsilon} & \varsigma_{u}\end{pmatrix}^T$ instead of the initially presented for inference purposes.

The impact of the chosen prior specification will be analyzed by comparing three setups. Firstly, **setup 1** will use a flat improper prior specification that incorporates no prior information about the initially presented parameters. **Setup 2** and **setup 3** will use informative priors, where the former will incorporate true information about the initial parameters and the latter will be misspecified in the sense that it assigns a smaller probability to the true initial parameters.

The change of variable technique has to be applied in order to transfer the prior beliefs about the parameters $\phi$, $\sigma_u$ and $\sigma_\epsilon$ to the hyperparameters $\psi$, $\varsigma_u$, $\varsigma_{\epsilon}$.

Since the transformation for $\sigma_u$ and $\sigma_{\epsilon}$ is equivalent, the following derivations will be presented for an exemplary parameter $\sigma$. Given the prior $p_{\phi}(\phi)$, $p_{\sigma}(\sigma)$ the accompanying prior $p_{\psi}(\psi)$, $p_{\varsigma}(\varsigma)$ can be derived using
\begin{align*}
\phi(\psi) &= \text{tanh}(\psi) & \Longleftrightarrow && \psi(\phi) &= \text{atanh}(\phi) \\
\sigma(\varsigma) &= \text{exp}(\varsigma) & \Longleftrightarrow && \varsigma(\sigma) &= \ln(\sigma)
\end{align*}
Differentiating the initial parameters $\phi$ and $\sigma$ with respect to the hyperparameter yields
\begin{equation*}
\begin{split}
  \frac{d \phi(\psi)}{d \psi} &= 1 - \text{tanh}(\psi)^2 \\
  \frac{d \sigma(\varsigma)}{d \varsigma} &= \text{exp}(\varsigma)
\end{split}
\end{equation*}
Hence, the change of variable technique yields the following prior specifications for the hyperparameters
\begin{equation}
\begin{split}
  p_{\psi}(\psi) &= p_{\phi}(\text{tanh}(\psi))(1 - \text{tanh}(\psi)^2) \\
  p_{\varsigma}(\varsigma) &= p_{\sigma}(\text{exp}(\varsigma))\text{exp}(\varsigma)
(\#eq:hyperparamter-prior)
\end{split}
\end{equation}
For **setup 1**, the initially presented parameters will have the following prior specification
\begin{align*}
  p_{x_0} &= 1 &
  p_\phi(\phi) &= 
  \begin{cases} 
    1, & \text{if}\;\; -1 < \phi < 1  \\ 
    0, & \text{otherwise}
  \end{cases} \\
 p_{\sigma_u}(\sigma_u) &= 
 \begin{cases} 
    1, & \text{if}\;\; \sigma_u > 0  \\ 
    0, & \text{otherwise}
  \end{cases}
 & p_{\sigma_\epsilon}(\sigma_\epsilon) &= 
 \begin{cases} 
    1, & \text{if}\;\; \sigma_{\epsilon} > 0  \\ 
    0, & \text{otherwise}
  \end{cases} \\
 p_{\beta_0}(\beta_0) &= 1 & p_{\beta_1}(\beta_1) &= 1 & p_{\beta_2}(\beta_2) &= 1
\end{align*}
Note that within this uninformative flat prior setup all possibly valid parameter values will have a prior density of one. Using result \@ref(eq:hyperparamter-prior) the belonging prior specification for the hyperparameters simplifies to
\begin{equation*}
\begin{split}
p_{\psi}(\psi) &= p_{\phi}(\text{tanh}(\psi))(1 - \text{tanh}(\psi)^2) = (1 - \text{tanh}(\psi)^2) \\
  p_{\varsigma_u}(\varsigma_u) &= p_{\sigma_u}(\text{exp}(\varsigma_u))\text{exp}(\varsigma_u) = \text{exp}(\varsigma_u) \\
  p_{\varsigma_\epsilon}(\varsigma_\epsilon) &= p_{\varsigma_\epsilon}(\text{exp}(\varsigma_\epsilon))\text{exp}(\varsigma_\epsilon) = \text{exp}(\varsigma_\epsilon)
\end{split}
\end{equation*}
**Setup 2** represents the informative prior setup. The truncated normal distribution will be used for parameter $\phi$ and the gamma distribution for the parameters $\sigma_u$ and $\sigma_\epsilon$. Hence, this specification is applied
\begin{align*}
  p_{x_0} &= f^{\mathcal{N}}(x_0 | 3.5, 0.1^2) \\
  p_\phi(\phi) &= f^{\mathcal{T}\mathcal{N}}(\phi | 0.8, 0.1^2, -1, 1) &  p_{\beta_0}(\beta_0) &= f^{\mathcal{N}}(\beta_0 | 0.4, 0.1^2) \\
  p_{\sigma_u}(\sigma_u) &= f^{\mathcal{G}}(\sigma_u | 12, 0.005) &
  p_{\beta_1}(\beta_1) &= f^{\mathcal{N}}(\beta_1 | 0.5, 0.2^2) \\
  p_{\sigma_\epsilon}(\sigma_\epsilon)  &= f^{\mathcal{G}}(\sigma_{\epsilon} | 6, 0.005) & p_{\beta_2}(\beta_2) &= f^{\mathcal{N}}(\beta_2 | -0.3, 0.2^2)
\end{align*}
where $f^{\mathcal{N}}(x | \mu, \sigma^2)$ is the density function of the normal distribution with mean $\mu$ and variance $\sigma^2$ and $f^{\mathcal{G}}(x | \alpha, \beta)$ is the density function of the gamma distribution with shape parameter $\alpha$ and scale parameter $\beta$. Additionally, the truncated normal density function is denoted by $f^{\mathcal{T}\mathcal{N}}(x|\mu, \sigma^2, a, b)$ and has the following definition
$$
f^{\mathcal{T}\mathcal{N}}(x|\mu, \sigma^2, a, b) = 
\begin{cases} 
  \dfrac{1}{\sigma}\dfrac{\phi\Big(\dfrac{x-\mu}{\sigma}\Big)}{\Phi\Big(\dfrac{b-\mu}{\sigma}\Big) - \Phi\Big(\dfrac{a-\mu}{\sigma}\Big)}, & \text{if}\;\; a < x < b  \\ 
  0, & \text{otherwise}
\end{cases}
$$
with $\phi(\cdot)$ being the density function of the standard normal distribution and $\Phi(\cdot)$ its cumulative distribution function. Again, the accompanying prior specification for the hyperparameters can be obtained using \@ref(eq:hyperparamter-prior).

**Setup 3** represents the misspecified prior setup. The same prior distributions as in the previous setup will be used, however with different parameters
\begin{align*}
  p_{x_0} &= f^{\mathcal{N}}(x_0 | 3.0, 0.1^2) \\
  p_\phi(\phi) &= f^{\mathcal{T}\mathcal{N}}(\phi | 0.3, 0.1^2, -1, 1) & p_{\beta_0}(\beta_0) &= f^{\mathcal{N}}(\beta_0 | 0.8, 0.1^2) \\
  p_{\sigma_u}(\sigma_u) &= f^{\mathcal{G}}(\sigma_u | 10, 0.01) & p_{\beta_1}(\beta_1) &= f^{\mathcal{N}}(\beta_1 | 0.0, 0.1^2)  \\
  p_{\sigma_\epsilon}(\sigma_\epsilon)  &= f^{\mathcal{G}}(\sigma_{\epsilon} | 10, 0.005) & p_{\beta_2}(\beta_2) &= f^{\mathcal{N}}(\beta_2 | 0.0, 0.1^2)
\end{align*}

The MCMC approaches will use $50000$ iterations and each parameter proposal $\theta^*_i$ will be generated based on 
$$\theta_i^{*}\sim \mathcal{N}(\theta_i^{(m-1)}, 0.025^2)$$
Furthermore, the starting parameter $\theta^{(0)} = \begin{pmatrix} x_0^{(0)} & \psi^{(0)} & \beta_0^{(0)} & \beta_{1}^{(0)} & \beta_{2}^{(0)} & \varsigma_{\epsilon}^{(0)} & \varsigma_{u}^{(0)}\end{pmatrix} ^T$, for the inference methods, will be set randomly at each iteration of the simulation study with
\begin{align*}
x_0^{(0)} &\sim \mathcal{U}(3.4, 3.6) & \phi^{(0)} &\sim \mathcal{U}(0.8, 0.9) & \beta_0^{(0)} &\sim \mathcal{U}(0.35, 0.65) \\ 
\beta_{1}^{(0)} &\sim \mathcal{U}(0.3, 0.5) & \beta_{2}^{(0)} &\sim \mathcal{U}(-0.3, -0.1) & \sigma_{\epsilon}^{(0)} &\sim \mathcal{U}(0.03, 0.04) \\ 
\sigma_{u}^{(0)} &\sim \mathcal{U}(0.04, 0.06)
\end{align*}
The starting hyperparameters $\psi^{(0)}, \varsigma_{\epsilon}^{(0)}, \varsigma_{u}^{(0)}$ are obtained using the corresponding presented transformations in \@ref(eq:parameter-transformation). As visible, the starting parameters will be close to the true values in order to increase the likelihood of convergence for the Markov chains. 

One out of the $1000$ simulations will be inspected in detail and then the aggregate result of the simulation study will be presented. The detailed analysis will be conducted using simulation four. Looking at the simulated data, Figure \@ref(fig:sim-4-plot-1),  the relationship between the state and observation process can be inspected. It is visible that the observation process is varying around the state process and that the latter seems to lag behind the former.

```{r, nonlinear-simulation, eval=TRUE, echo=FALSE}
load(here::here("data", "results", "simulation_results.RData"))
```

```{r, sim-4-plot-1, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Exemplary simulation: Data", fig.width=5, fig.height=2, warning=FALSE}
library(ggplot2)

df <- data.frame(time = 0:(N - 1), x = sim_4$data_latent, 
                 y = sim_4$data_observed[, 1])
df <- reshape2::melt(df, id.vars = "time")
ggplot(data = df, aes(x = time, y = value, group = variable)) +
  geom_line(aes(color = variable)) + 
  scale_color_manual(
    values = c("red", "black"), 
    labels = c(expression(x[n]), expression(y[n]))
  ) +
  theme_light() +
  theme(
    legend.justification = c(0, 1), legend.position = c(0, 1), 
    legend.title = element_blank(), 
    legend.background = element_rect(fill = "transparent"),
    plot.margin = unit(c(0.1,0.1,0.1,0.1), "cm"),
    axis.title = element_blank(),
    axis.text.x = element_text(size = 7),
    axis.text.y = element_text(size = 7)
  ) + 
  labs(x = "n", y = "")
```

The trace plots from the Bayesian approaches can be seen in Figure \@ref(fig:sim-4-appendix-trace-plot-setup-1), Figure \@ref(fig:sim-4-appendix-trace-plot-setup-2) and Figure \@ref(fig:sim-4-appendix-trace-plot-setup-3). Moreover, as visible in the trace plots for the standard deviation hyperparameters in Figure \@ref(fig:sim-4-appendix-trace-plot-setup-1), convergence cannot be guaranteed. A burn in phase of $25000$ iterations will be set. Hence, for the next visualizations and figures the first $25000$ iterations of the Bayesian approaches have been discarded. The Gaussian kernel density estimates of the marginal posterior distributions can be seen in Figure \@ref(fig:sim-4-appendix-marginal-posterior-setup-1), Figure \@ref(fig:sim-4-appendix-marginal-posterior-setup-2) and Figure \@ref(fig:sim-4-appendix-marginal-posterior-setup-3).

Furthermore, the impact of the prior specification is visible. The misspecified, **setup 3**, and the uninformative case, **setup 1**, show the largest deviations from the true parameters.

Figure \@ref(fig:sim-4-plot-2) displays the state estimates $\hat{x}_n$ from all approaches. For the likelihood based approach this corresponds to the particle filter state estimate using the parameter maximum likelihood estimates. For the Bayesian approaches the marginal posterior mean for the latent state will be used.  One can see, that all setups are relatively good at estimating the state process. Moreover, for the Bayesian approaches $95\%$ pointwise credible intervals have been added with **setup 2** having the narrowest estimates.

Additionally, the results from the $10$ runs of the likelihood based approach using the Nelder-Mead algorithm can be inspected by looking at Table \@ref(tab:nm-result-table). The resulting parameter estimates within the $10$ repetitions vary, which can be seen by the minimum and maximum estimates displayed. However, all in all the parameter estimates are close to the true values.

```{r, nm-result-table, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
result_table <- data.frame(
  true = c(true_params_simulation, NA),
  mle = sim_4$smc_result$optim_result[
    , which.max(sim_4$smc_result$optim_result[8, ])
  ],
  min = apply(sim_4$smc_result$optim_result, MARGIN = 1, min),
  max = apply(sim_4$smc_result$optim_result, MARGIN = 1, max)
)
rownames(result_table) <- c(
  "$\\beta_0$", "$\\beta_1$", "$\\beta_2$", "$\\sigma_u$", 
  "$\\sigma_\\epsilon$", "$\\phi$", "$x_0$","$\\ell(\\theta)$"
)
result_table[4:5, ] <- exp(result_table[4:5, ])
result_table[6, ] <- tanh(result_table[6, ])
result_table <- round(result_table, 4)
opts <- options(knitr.kable.NA = "-")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("True Values", "Estimates", "Min", "Max"),
  caption = "Exemplary simulation: Results of the Nelder-Mead approach", booktabs = TRUE, escape = FALSE,
  #linesep = "\\addlinespace", 
  align = rep("c", 4)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```

Table \@ref(tab:bayes-result-table) depicts the marginal posterior means for the parameters from the three PMMH approaches. As expected, the first indication from inspecting the exemplary simulation results is that the informative **setup 2** results in the most precise parameter estimates. The aggregate analysis will clarify this observation.

```{r, bayes-result-table, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
result_table <- data.frame(
  true = c(
    true_params_simulation[1:3], 
    exp(true_params_simulation[4:5]),
    tanh(true_params_simulation[6]),
    true_params_simulation[7]
  ),
  mean_1 = c(
    colMeans(
      sim_4$pmmh_noninformative$traces[25000:50000, 3:5]
    ),
    colMeans(
      exp(sim_4$pmmh_noninformative$traces[25000:50000, 6:7])
    ),
    mean(
      tanh(sim_4$pmmh_noninformative$traces[25000:50000, 8])
    ),
    mean(sim_4$pmmh_noninformative$traces[25000:50000, 9])
  ),
  mean_2 = c(
    colMeans(
      sim_4$pmmh_true_informative$traces[25000:50000, 3:5]
    ),
    colMeans(
      exp(sim_4$pmmh_true_informative$traces[25000:50000, 6:7])
    ),
    mean(
      tanh(sim_4$pmmh_true_informative$traces[25000:50000, 8])
    ),
    mean(sim_4$pmmh_true_informative$traces[25000:50000, 9])
  ),
  mean_3 = c(
    colMeans(
      sim_4$pmmh_false_informative$traces[25000:50000, 3:5]
    ),
    colMeans(
      exp(sim_4$pmmh_false_informative$traces[25000:50000, 6:7])
    ),
    mean(
      tanh(sim_4$pmmh_false_informative$traces[25000:50000, 8])
    ),
    mean(sim_4$pmmh_false_informative$traces[25000:50000, 9])
  )
)
rownames(result_table) <- c(
  "$\\beta_0$", "$\\beta_1$", "$\\beta_2$", "$\\sigma_u$", 
  "$\\sigma_\\epsilon$", "$\\phi$", "$x_0$"
)
result_table <- round(result_table, 4)
opts <- options(knitr.kable.NA = "")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("True Values", "Setup 1", "Setup 2", "Setup 3"),
  caption = "Exemplary simulation: Results of the PMMH approaches", booktabs = TRUE, escape = FALSE,
  #linesep = "\\addlinespace", 
  align = rep("c", 4)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position")  %>%
  kableExtra::add_header_above(c("", "", "Marginal posterior means" = 3))
```

Figure \@ref(fig:aggregate-simulation-result-plot) shows boxplots of the parameter estimates from the $1000$ simulation iterations. As visible, the variations of the parameter estimates from the Nelder-Mead approach and the marginal posterior means of the other three Bayesian approaches are depicted. Furthermore, the last visualization, in the bottom right corner, shows the root mean squared error (RMSE) for the latent state estimation for all approaches
$$\text{RMSE} = \sqrt{\frac{1}{N}\sum_{n = 1}^{N}(\hat{x}_n - x_n)^2}$$
The aggregate results show that the uninformative Bayesian approach, **setup 1**, has the highest variation for the parameter and latent state estimates. Additionally, the misspecified prior setup, **setup 3**, has the highest average deviation from the true parameters. As expected, the informative prior setup, **setup 2**, has the smallest average deviation and variance. The likelihood based approach is more precise for the parameter estimation than the Bayesian approaches in **setup 2** and **setup 3**. Furthermore, the RMSE is on average smaller for the Bayesian approaches due to the estimation including the whole data set contrary to the particle filter including only the information up until the time point of interest. All in all, the effect of the prior specification on the estimation results is visible. A general outperformance of the likelihood based or Bayesian approach cannot be concluded.

```{r, sim-4-plot-2, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Exemplary simulation: Latent state estimates", fig.width=6, fig.height=8.5, warning=FALSE, message=FALSE}
library(ggplot2)
library(gridExtra)

# Plot theme:
p_theme <- theme(
  legend.position = "bottom", 
  legend.title = element_blank(), 
  legend.background = element_rect(fill = "transparent"),
  plot.margin = unit(c(0.1,0.1,0.1,0.1), "cm"),
  plot.title = element_text(hjust = 0.5, size = 8),
  axis.title = element_blank(),
  axis.text.x = element_text(size = 7),
  axis.text.y = element_text(size = 7)
)

# Plot nelder mead result
df <- data.frame(
  time = 0:(N - 1), x = sim_4$data_latent,  
  x1 = sim_4$smc_result$pf
)
df <- reshape2::melt(df, id.vars = "time")
nm <- ggplot(data = df, aes(x = time, y = value, group = variable)) +
  geom_line(aes(color = variable), size = c(rep(0.5, N), rep(0.3, N))) + 
  scale_color_manual(
    values = c("black", "red"), 
    labels = c(expression(x[n]), expression(hat(x)[n]))
  ) +
  theme_light() +
  labs(title = latex2exp::TeX("State Estimate: \\textbf{Nelder-Mead Approach}"), 
       x = "", y = "") +
  p_theme

# Plot Noninformative Prior result
df <- data.frame(
  time = 0:(N - 1), x = sim_4$data_latent, 
  x1 = sim_4$pmmh_noninformative$state_mean
)
df <- reshape2::melt(df, id.vars = "time")
q <- data.frame(
  time = 0:(N - 1),
  variable = "x1",
  lower = sim_4$pmmh_noninformative$state_lower,
  upper = sim_4$pmmh_noninformative$state_upper
)
pm_1 <- ggplot(data = df, aes(x = time, y = value, group = variable)) +
  geom_line(aes(color = variable), size = c(rep(0.5, N), rep(0.3, N))) + 
  geom_ribbon(
    data = q, aes(x = time, ymin = lower, ymax = upper, fill = "red"),  
    inherit.aes = FALSE, alpha = 0.25, show.legend = FALSE, linetype = 2,
    colour = "red", size = 0.25
  ) +
  scale_color_manual(
    values = c("black", "red"), 
    labels = c(expression(x[n]), expression(hat(x)[n]))
  ) +
  theme_light() +
  labs(title = latex2exp::TeX("State Estimate: \\textbf{Setup 1}"), 
       x = "", y = "") +
  p_theme

# Plot true informative Prior result
df <- data.frame(
  time = 0:(N - 1), x = sim_4$data_latent,  
  x1 = sim_4$pmmh_true_informative$state_mean
)
df <- reshape2::melt(df, id.vars = "time")
q <- data.frame(
  time = 0:(N - 1),
  variale = "x1",
  lower = sim_4$pmmh_true_informative$state_lower,
  upper = sim_4$pmmh_true_informative$state_upper
)
pm_2 <- ggplot(data = df, aes(x = time, y = value, group = variable)) +
  geom_line(aes(color = variable), size = c(rep(0.5, N), rep(0.3, N))) + 
  geom_ribbon(
    data = q, aes(x = time, ymin = lower, ymax = upper, fill = "red"),  
    inherit.aes = FALSE, alpha = 0.25, show.legend = FALSE, linetype = 2,
    colour = "red", size = 0.25
  ) +
  scale_color_manual(
    values = c("black", "red"), 
    labels = c(expression(x[n]), expression(hat(x)[n]))
  ) +
  theme_light() +
  labs(title = latex2exp::TeX("State Estimate: \\textbf{Setup 2}"), 
       x = "", y = "") +
  p_theme

# Plot false informative prior result
df <- data.frame(
  time = 0:(N - 1), x = sim_4$data_latent,  
  x1 = sim_4$pmmh_false_informative$state_mean
)
df <- reshape2::melt(df, id.vars = "time")
q <- data.frame(
  time = 0:(N - 1),
  variable = "x1",
  lower = sim_4$pmmh_false_informative$state_lower,
  upper = sim_4$pmmh_false_informative$state_upper
)
pm_3 <- ggplot(data = df, aes(x = time, y = value, group = variable)) +
  geom_line(aes(color = variable), size = c(rep(0.5, N), rep(0.3, N))) + 
  geom_ribbon(
    data = q, aes(x = time, ymin = lower, ymax = upper, fill = "red"),  
    inherit.aes = FALSE, alpha = 0.25, show.legend = FALSE, linetype = 2,
    colour = "red", size = 0.25
  ) +
  scale_color_manual(
    values = c("black", "red"), 
    labels = c(expression(x[n]), expression(hat(x)[n]))
  ) +
  theme_light() +
  labs(title = latex2exp::TeX("State Estimate: \\textbf{Setup 3}"), 
       x = "", y = "") +
  p_theme

get_legend <- function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}
legend <- gtable::gtable_filter(ggplot_gtable(ggplot_build(pm_2)), "guide-box")

grid.arrange(
  nm + theme(legend.position = "none"), 
  pm_1 + theme(legend.position = "none"), 
  pm_2 + theme(legend.position = "none"), 
  pm_3 + theme(legend.position = "none"), 
  ncol = 1, legend = legend, nrow = 5, 
  heights = c(1.1, 1.1, 1.1, 1.1, 0.2)
)
```

```{r, aggregate-simulation-result-plot, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Aggregate simulation: Result", fig.width=7, fig.height=10, warning=FALSE, message=FALSE}
# Plot theme
p_theme <- theme(
  legend.position = "bottom",
  legend.title = element_blank(), 
  legend.background = element_rect(fill = "transparent"),
  plot.margin = unit(c(0.2, 0.2, 0.2, 0.2), "cm"),
  plot.title = element_text(hjust = 0.5, size = 8),
  #axis.title = element_blank(),
  axis.title.x = element_text(size = 8),
  axis.title.y = element_text(size = 8, vjust = 0.5, angle = 0),
  legend.key = element_rect(fill = NA),
  axis.text.x = element_text(size = 7),
  axis.text.y = element_text(size = 7)
)

# Beta 0
df <- data.frame(
  lik = res_smc[, 1], setup1 = res_pmmh_noninformative[, 1],
  setup2 = res_pmmh_true_informative[, 1], 
  setup3 = res_pmmh_false_informative[, 1] 
)
df <- reshape2::melt(df)
df$type <- sapply(df$variable, function(x){ifelse(
  x == "lik", "Likelihood approach", "Bayesian approach")
})
p1 <- ggplot(df, aes(x = variable, y = value, fill = type), colour = "black") + 
  stat_boxplot(geom = 'errorbar') + geom_boxplot() + 
  geom_hline(
    aes(yintercept = true_params_simulation["beta_0"], colour = "True"), 
    linetype = 2, show.legend = TRUE, size = 1
  ) + 
  scale_colour_manual(values = c("grey")) + 
  theme_light() + p_theme +
  scale_x_discrete(
    labels = c(
      "lik" = "Nelder-Mead", "setup1" = "Setup 1", "setup2" = "Setup 2",
      "setup3" = "Setup 3"
    )
  ) +
  labs(y = latex2exp::TeX("$\\hat{\\beta}_0$"), x = "")

# Beta 1
df <- data.frame(
  lik = res_smc[, 2], setup1 = res_pmmh_noninformative[, 2],
  setup2 = res_pmmh_true_informative[, 2], 
  setup3 = res_pmmh_false_informative[, 2] 
)
df <- reshape2::melt(df)
df$type <- sapply(df$variable, function(x){ifelse(
  x == "lik", "Likelihood approach", "Bayesian approach")
})
p2 <- ggplot(df, aes(x = variable, y = value, fill = type), colour = "black") + 
  stat_boxplot(geom = 'errorbar') + geom_boxplot() + 
  geom_hline(
    aes(yintercept = true_params_simulation["beta_1"], colour = "True"), 
    linetype = 2, show.legend = TRUE, size = 1
  ) + 
  scale_colour_manual(values = c("grey")) + 
  theme_light() + p_theme +
  scale_x_discrete(
    labels = c(
      "lik" = "Nelder-Mead", "setup1" = "Setup 1", "setup2" = "Setup 2",
      "setup3" = "Setup 3"
    )
  ) +
  labs(y = latex2exp::TeX("$\\hat{\\beta}_1$"), x = "")

# Beta 2
df <- data.frame(
  lik = res_smc[, 3], setup1 = res_pmmh_noninformative[, 3],
  setup2 = res_pmmh_true_informative[, 3], 
  setup3 = res_pmmh_false_informative[, 3] 
)
df <- reshape2::melt(df)
df$type <- sapply(df$variable, function(x){ifelse(
  x == "lik", "Likelihood approach", "Bayesian approach")
})
p3 <- ggplot(df, aes(x = variable, y = value, fill = type), colour = "black") + 
  stat_boxplot(geom = 'errorbar') + geom_boxplot() + 
  geom_hline(
    aes(yintercept = true_params_simulation["beta_2"], colour = "True"), 
    linetype = 2, show.legend = TRUE, size = 1
  ) + 
  scale_colour_manual(values = c("grey")) + 
  theme_light() + p_theme +
  scale_x_discrete(
    labels = c(
      "lik" = "Nelder-Mead", "setup1" = "Setup 1", "setup2" = "Setup 2",
      "setup3" = "Setup 3"
    )
  ) +
  labs(y = latex2exp::TeX("$\\hat{\\beta}_2$"), x = "")

# sigma_x
df <- data.frame(
  lik = res_smc[, 4], setup1 = res_pmmh_noninformative[, 4],
  setup2 = res_pmmh_true_informative[, 4], 
  setup3 = res_pmmh_false_informative[, 4] 
)
df <- reshape2::melt(exp(df))
df$type <- sapply(df$variable, function(x){ifelse(
  x == "lik", "Likelihood approach", "Bayesian approach")
})
p4 <- ggplot(df, aes(x = variable, y = value, fill = type), colour = "black") + 
  stat_boxplot(geom = 'errorbar') + geom_boxplot() + 
  geom_hline(
    aes(yintercept = exp(true_params_simulation["sigma_x"]), colour = "True"), 
    linetype = 2, show.legend = TRUE, size = 1
  ) + 
  scale_colour_manual(values = c("grey")) + 
  theme_light() + p_theme +
  scale_x_discrete(
    labels = c(
      "lik" = "Nelder-Mead", "setup1" = "Setup 1", "setup2" = "Setup 2",
      "setup3" = "Setup 3"
    )
  ) +
  labs(y = latex2exp::TeX("$\\hat{\\sigma}_u$"), x = "")

# sigma_y
df <- data.frame(
  lik = res_smc[, 5], setup1 = res_pmmh_noninformative[, 5],
  setup2 = res_pmmh_true_informative[, 5], 
  setup3 = res_pmmh_false_informative[, 5] 
)
df <- reshape2::melt(exp(df))
df$type <- sapply(df$variable, function(x){ifelse(
  x == "lik", "Likelihood approach", "Bayesian approach")
})
p5 <- ggplot(df, aes(x = variable, y = value, fill = type), colour = "black") + 
  stat_boxplot(geom = 'errorbar') + geom_boxplot() + 
  geom_hline(
    aes(yintercept = exp(true_params_simulation["sigma_y"]), colour = "True"), 
    linetype = 2, show.legend = TRUE, size = 1
  ) + 
  scale_colour_manual(values = c("grey")) + 
  theme_light() + p_theme +
  scale_x_discrete(
    labels = c(
      "lik" = "Nelder-Mead", "setup1" = "Setup 1", "setup2" = "Setup 2",
      "setup3" = "Setup 3"
    )
  ) +
  labs(y = latex2exp::TeX("$\\hat{\\sigma}_\\epsilon$"), x = "")

# phi
df <- data.frame(
  lik = res_smc[, 6], setup1 = res_pmmh_noninformative[, 6],
  setup2 = res_pmmh_true_informative[, 6], 
  setup3 = res_pmmh_false_informative[, 6] 
)
df <- reshape2::melt(tanh(df))
df$type <- sapply(df$variable, function(x){ifelse(
  x == "lik", "Likelihood approach", "Bayesian approach")
})
p6 <- ggplot(df, aes(x = variable, y = value, fill = type), colour = "black") + 
  stat_boxplot(geom = 'errorbar') + geom_boxplot() + 
  geom_hline(
    aes(yintercept = tanh(true_params_simulation["phi"]), colour = "True"), 
    linetype = 2, show.legend = TRUE, size = 1
  ) + 
  scale_colour_manual(values = c("grey")) + 
  theme_light() + p_theme +
  scale_x_discrete(
    labels = c(
      "lik" = "Nelder-Mead", "setup1" = "Setup 1", "setup2" = "Setup 2",
      "setup3" = "Setup 3"
    )
  ) +
  labs(y = latex2exp::TeX("$\\hat{\\phi}$"), x = "")

# x_0
df <- data.frame(
  lik = res_smc[, 7], setup1 = res_pmmh_noninformative[, 7],
  setup2 = res_pmmh_true_informative[, 7], 
  setup3 = res_pmmh_false_informative[, 7] 
)
df <- reshape2::melt(df)
df$type <- sapply(df$variable, function(x){ifelse(
  x == "lik", "Likelihood approach", "Bayesian approach")
})
p7 <- ggplot(df, aes(x = variable, y = value, fill = type), colour = "black") + 
  stat_boxplot(geom = 'errorbar') + geom_boxplot() + 
  geom_hline(
    aes(yintercept = true_params_simulation["x_0"], colour = "True"), 
    linetype = 2, show.legend = TRUE, size = 1
  ) + 
  scale_colour_manual(values = c("grey")) + 
  theme_light() + p_theme +
  scale_x_discrete(
    labels = c(
      "lik" = "Nelder-Mead", "setup1" = "Setup 1", "setup2" = "Setup 2",
      "setup3" = "Setup 3"
    )
  ) +
  labs(y = latex2exp::TeX("$\\hat{\\x}_0$"), x = "")

# mse
df <- data.frame(
  lik = mse_smc, setup1 = mse_pmmh_noninformative,
  setup2 = mse_pmmh_true_informative, 
  setup3 = mse_pmmh_false_informative
)
df <- reshape2::melt(df)
df$type <- sapply(df$variable, function(x){ifelse(
  x == "lik", "Likelihood approach", "Bayesian approach")
})
p8 <- ggplot(df, aes(x = variable, y = value, fill = type), colour = "black") + 
  stat_boxplot(geom = 'errorbar') + geom_boxplot() + 
  theme_light() + p_theme +
  scale_x_discrete(
    labels = c(
      "lik" = "Nelder-Mead", "setup1" = "Setup 1", "setup2" = "Setup 2",
      "setup3" = "Setup 3"
    )
  ) +
  labs(y = latex2exp::TeX("RMSE"), x = "")

legend <- gtable::gtable_filter(ggplot_gtable(ggplot_build(p2)), "guide-box")

grid.arrange(
  arrangeGrob(
    p1 + theme(legend.position = "none"), 
    p2 + theme(legend.position = "none"), 
    p3 + theme(legend.position = "none"), 
    p4 + theme(legend.position = "none"), 
    p5 + theme(legend.position = "none"), 
    p6 + theme(legend.position = "none"), 
    p7 + theme(legend.position = "none"), 
    p8 + theme(legend.position = "none", 
               axis.title.y = element_text(angle = 90)),
    ncol = 2
  ),
  arrangeGrob(grid::nullGrob(), legend, grid::nullGrob(), nrow = 1),
  heights = c(4.5, 0.1), ncol = 1
)
rm(list = ls())
```

<!--chapter:end:Rmd//04-Simulation-Study.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Empirical application {#application}

For the empirical application the presented PMMH procedure will be applied for modeling the time varying mean of the PD. This application will be an extension of the presented results and procedures in [@h2_paper]. Besides applying the Bayesian approach as an alternative, a possible state process extension will be tested. First, the necessary steps for deriving the state-space model will be explained.

## Model implementation

Starting from the basic definition of the total log return at time $t+1$, $r_{t+1} = \ln(P_{t+1} + D_{t+1}) - \ln(P_{t})$ with price $P_{t}$ and dividend $D_t$, the log return can be reformulated as a nonlinear function of the PD, $\eta_t$. First subtracting $r_{t+1}$ from itself yields
\begin{equation*}
\begin{split}
0 &= \ln\Bigg(\frac{P_t}{P_{t+1} + D_{t+1}}\frac{P_{t+1} + D_{t+1}}{P_{t}}\Bigg)
\end{split}
\end{equation*}
then adding $\eta_t$ to both sides of the equation
\begin{equation*}
\begin{split}
\eta_{t} &= \ln\Bigg(\frac{P_t}{D_t}\frac{P_{t+1} + D_{t+1}}{P_{t}}\Bigg) - r_{t+1} = \ln\Bigg(\Bigg(1 + \frac{P_{t+1}}{D_{t+1}}\Bigg)\frac{D_{t+1}}{D_{t}}\Bigg) - r_{t+1}
\end{split}
\end{equation*}
and simplifying the above yields
\begin{equation*}
\begin{split}
\eta_{t} &= \ln(1 + \text{exp}(\eta_{t+1})) - r_{t+1} + \Delta d_{t+1} \\
r_{t+1} &= -\eta_{t} + \ln(1 + \text{exp}(\eta_{t+1})) + \Delta d_{t+1}
\end{split}
\end{equation*}
Here, $\Delta d_{t+1} = \ln(D_{t+1}) - \ln(D_t)$ represents the log dividend growth. Applying a first order Taylor expansion around the fixed steady state $\bar{\eta}$ for $\tilde{\eta}_{t+1}$ yields a linear approximation
\begin{equation}
\begin{split}
r_{t+1} &\approx -\eta_{t} + \ln(1 + \text{exp}(\bar{\eta})) + \Delta d_{t+1} + \frac{1}{1 + \text{exp}(-\bar{\eta})}(\eta_{t+1}-\bar{\eta}) \\
&\approx k -\eta_{t} + \rho\eta_{t+1} + \Delta d_{t+1}
\end{split}
\end{equation}
with $\rho$ being specified as
\begin{align}
\rho &= \frac{1}{1+\text{exp}(-\bar{\eta})} & \Longleftrightarrow && \bar{\eta} &= -\ln\Bigg(\frac{1}{\rho} - 1\Bigg)
(\#eq:rho-spec)
\end{align}
and $k$ directly following from the above
\begin{equation}
\begin{split}
k &= \ln(1 + \text{exp}(\bar{\eta})) - \bar{\eta}\frac{1}{1 + \text{exp}(-\bar{\eta})} \\
&= \ln\Bigg(1 + \text{exp}\Bigg(-\ln\Bigg(\frac{1}{\rho} - 1\Bigg)\Bigg)\Bigg) + \ln\Bigg(\frac{1}{\rho} - 1\Bigg)\frac{1}{1 + \text{exp}\Big(\ln\Big(\frac{1}{\rho} - 1\Big)\Big)} \\
&= -\ln(1 - \rho) + \rho\ln\Bigg(\frac{1}{\rho} - 1\Bigg) \\
&= -\ln(\rho) - (1 - \rho)\ln\Bigg(\frac{1}{\rho} - 1\Bigg) 
(\#eq:k-spec)
\end{split}
\end{equation}
Previous empirical applications typically assumed a constant parameter $\bar{\eta}$, as in [@campbell_paper]. However, the results presented in [@h2_paper] deliver evidence for a gradually time varying mean of the PD. By allowing this time variation, $k$ and $\rho$ become also time varying. Hence, the corresponding parameters $k_t$ and $\rho_t$ are obtained from \@ref(eq:rho-spec) and \@ref(eq:k-spec)
\begin{equation}
\begin{split}
\rho_t &= \frac{1}{1+\text{exp}(-\tilde{\eta}_t)} \\
k_t &= -\ln(\rho_t) - (1 - \rho_t)\ln\Bigg(\frac{1}{\rho_t} - 1\Bigg)
\end{split}
\end{equation}
with $\tilde{\eta}_t$ denoting the time varying local mean for the Taylor approximation. Therefore, the PD has the following specification
\begin{equation}
\begin{split}
\eta_{t} &\approx k_t - r_{t+1} + \rho_t\eta_{t+1} + \Delta d_{t+1}
(\#eq:lpd-spec)
\end{split}
\end{equation}
Similar approximations as in [@van_nieuwerburgh_paper] are adopted: $\mathbb{E}_t[\rho_{t+i}]\approx\rho_t$, $\mathbb{E}_t[k_{t+i}]\approx\;k_t$ and $\mathbb{E}_t[\rho_{t+i}\eta_{t+1+i}]\approx\mathbb{E}_t[\rho_{t+i}]\mathbb{E}_t[\eta_{t+1+i}]$. The present value formulation of the PD can then be concluded by taking the conditional expectation and iterating equation \@ref(eq:lpd-spec) forward
\begin{equation}
\begin{split}
\eta_{t} &\approx k_t - \mathbb{E}_t[r_{t+1}] + \rho_t\mathbb{E}_t[\eta_{t+1}] + \mathbb{E}_t[\Delta d_{t+1}] \\
 &\approx k_t - \mathbb{E}_t[r_{t+1}] + \rho_t\mathbb{E}_t[k_{t+1} - r_{t+2} + \rho_{t+1}\eta_{t+2} + \Delta d_{t+2}] + \mathbb{E}_t[\Delta d_{t+1}] \\
 &\approx \cdots \\
 &\approx \frac{k_t}{1-\rho_t} + \sum_{i=1}^{\infty}\rho_t^{i-1}\mathbb{E}_t[\Delta d_{t+i}^e - r_{t+i}^e] + \lim\limits_{i \rightarrow \infty}\rho_t^i\mathbb{E}_t[\eta_{t+i}]
(\#eq:present-value)
\end{split}
\end{equation}
Here, the excess dividend growth $\Delta d_{t}^e = \Delta d_t - r^f_t$ and return $r_t^e = r_t - r_t^f$ are used with $r^f_t$ being the risk-free interest rate.

A nonlinear state-space model will be used to filter the latent mean process. As in [@h2_paper], the present value formulation \@ref(eq:present-value) will be used as the observation process by adding an error term $\epsilon_t \sim\mathcal{N}(0, \sigma_{\epsilon}^2)$ that captures rational bubbles, approximation errors and other influences in $\lim\limits_{i \rightarrow \infty}\rho_t^i\mathbb{E}_t[\eta_{t+i}]$
\begin{equation}
\eta_{t} = \frac{k_t}{1-\rho_t} + \sum_{i=1}^{\infty}\rho_t^{i-1}\mathbb{\tilde{E}}_t[\Delta d_{t+i}^e - r_{t+i}^e] + \epsilon_t
 (\#eq:observation-process)
\end{equation}
A low dimensional vector autoregressive (VAR) model of order $1$ will be used for the objective expectations $\mathbb{\tilde{E}}_t$ conditional on information available at $t$. This approach was first proposed in [@campbell_shiller_paper]. The VAR model will be comprised of the PD $\eta_t$, excess dividend growth $\Delta d_t^e$, excess return $r_t^e$ and inflation $\pi_t$.

Assuming the following VAR model
\begin{equation}
\begin{split}
y_t &= \begin{pmatrix} \eta_t \\ \Delta d_t^e \\ r_t^e \\ \pi_t \end{pmatrix} = \alpha + Ay_{t-1} + v_t \\
v_t &\sim \mathcal{N}_4(0, \Sigma)
 (\#eq:ex-ante-expectations-var-model)
\end{split}
\end{equation}
and the following reparametrization by stacking the vector of constants $\alpha$ on to the parameter matrix $A$
\begin{equation}
y_t = \begin{pmatrix} \eta_t \\ \Delta d_t^e \\ r_t^e \\ \pi_t \\ 1 \end{pmatrix} = \left(
\begin{array}{c|c}
        A & \alpha \\
        \hline
        0 & 1\\
\end{array}
\right)y_{t-1} + \begin{pmatrix}v_t \\ 0\end{pmatrix} = B y_{t-1} + \begin{pmatrix}v_t \\ 0\end{pmatrix}
\end{equation}
the discounted objective expectations can be evaluated using the vector $h = \begin{pmatrix} 0 & 1 & -1 & 0 & 0\end{pmatrix}^T$
\begin{equation}
\begin{split}
\sum_{i=1}^{\infty}\rho_t^{i-1}\mathbb{\tilde{E}}_t[\Delta d_{t+i}^e - r_{t+i}^e] &= \sum_{i=1}^{\infty}\rho_t^{i-1}h^TB^iy_{t} \\
& = h^TB\sum_{i=0}^{\infty}\rho_t^{i}B^iy_{t} = h^TB(I_5 -\rho_t B)^{-1}y_t
 (\#eq:ex-ante-expectations)
\end{split}
\end{equation}
Similar to [@h2_paper], a adaptive approach will be used for the choice of VAR model sample size. Starting at the forecasting origin, observation $t = 30$, VAR models with samples $\Omega_{t,\omega} = \{\eta_\tau, \Delta d^e_\tau, r_\tau^e, \pi_\tau\;|\;\tau = t-\omega+1, ..., t\}$ and varying lengths $\omega = (10, ..., 30)$ will be fitted. The selection criterion will be the root mean squared error of the last five in sample observations of the difference of the excess dividend rate and return, $\{\Delta d_m^e - r_m^e\}^{m=t}_{m=t-4}$. Additionally, the stability of each VAR model will be checked, at each length $\omega$, by inspecting that all eigenvalues of matrix $A$, from \@ref(eq:ex-ante-expectations-var-model), have modulus less than 1. If this requirement is violated, the specific VAR model will be discarded. If at a specific time point $t$ all VAR models violate the stability requirement, the VAR model with $\omega = 10$ will be used.

The empirical application will be conducted on quarterly data for the S&P 500 starting at the 30th of September 1963 up until the 30th of September 2020, with the forecast origin being the 31st of December 1970. The 10-year treasury constant maturity rate has been used for the risk free rate $r^f_t$ and the inflation rate $\pi_t$ has been derived from the consumer price index for all urban consumers. These time series, and the price and dividend data for the S&P 500, have been provided by Robert J. Shiller [@shiller_website]

## Random walk state process

Firstly, the state-space model with a random walk state process (**RW Model**) will be analyzed. The latent process follows
\begin{equation}
\begin{split}
\tilde{\eta}_t &= \tilde{\eta}_{t-1} + u_t \\
u_t &\sim \mathcal{N}(0, \sigma_u^2)
(\#eq:basic-model-state-process)
\end{split}
\end{equation}
The parameters of interest are $\theta = \begin{pmatrix} \tilde{\eta}_0 & \sigma_{\epsilon} & \sigma_{u}\end{pmatrix}^T$ with $\tilde{\eta}_0$ being the initial value. Similar to the simulation study uninformative setup, flat priors will be used for the parameters and initial value and equal parameter transformations will be applied
\begin{align*}
\sigma_\epsilon(\varsigma_\epsilon) &= \text{exp}(\varsigma_\epsilon) & \Longleftrightarrow && \varsigma_\epsilon(\sigma_\epsilon) &= \ln(\sigma_\epsilon) \\
\sigma_u(\varsigma_u) &= \text{exp}(\varsigma_u) & \Longleftrightarrow && \varsigma_\epsilon(\sigma_u) &= \ln(\sigma_u)
\end{align*}
Hence, after having applied the change of variable technique the prior specification for the hyperparameters $\{\varsigma_\epsilon, \varsigma_u\}$ becomes
\begin{equation}
\begin{split}
  p_{\varsigma_\epsilon}(\varsigma_\epsilon) &= \text{exp}(\varsigma_\epsilon) \\
  p_{\varsigma_u}(\varsigma_u) &= \text{exp}(\varsigma_u)
\end{split}
\end{equation}
For the PMMH procedure three independent Markov chains with $M = 50000$ iterations and $J = 1000$ particles will be run and the starting parameter $\theta^{(0)}$ will be set to 
$$
\theta^{(0)} = \begin{pmatrix}\tilde{\eta}_0^{(0)} \\ \varsigma_\epsilon^{(0)} \\ \varsigma_u^{(0)} \end{pmatrix} = \begin{pmatrix}3.5 \\ \ln(0.05) \\ \ln(0.05) \end{pmatrix}
$$
Furthermore, the parameter proposals will be generated using
$$
\theta_i^{*}\sim \mathcal{N}(\theta_i^{(m-1)}, 0.02^2)
$$
Figure \@ref(fig:basic-model-trace-plot) shows the trace plots for each parameter and each Markov chain. The first 10000 iterations have been dropped. Additionally, Figure \@ref(fig:basic-model-posterior-marginal-density) displays the Gaussian kernel density estimates of the marginal posterior distributions of the random walk latent state model parameters. The corresponding Table \@ref(tab:basic-model-setup) summarizes the inference results.

```{r, basic-model-setup, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
source(here::here("R", "prepare_raw_data.R"))
source(here::here("R", "model_comparison", "model_comparison_basic_model.R"))

library(tidyverse)
library(gridExtra)

traces <- rbind(
        result[[1]]@traces[5000:20000, 3:5], 
        result[[2]]@traces[5000:20000, 3:5], 
        result[[3]]@traces[5000:20000, 3:5]
)
traces[, 2:3] <- exp(traces[, 2:3])

result_table <- data.frame(
  mean = colMeans(traces),
  median = apply(traces, 2, median),
  lower = apply(traces, 2, quantile, probs = 0.025),
  upper = apply(traces, 2, quantile, probs = 0.975)
)
rownames(result_table) <- c(
  "$\\tilde{\\eta}_0$", "$\\sigma_u$", "$\\sigma_\\epsilon$"
)
result_table <- round(result_table, 4)
opts <- options(knitr.kable.NA = "")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("Mean", "Median", "0.025\\%", "0.975\\%"),
  caption = "Random walk model: PMMH parameter inference results", 
  booktabs = TRUE, escape = FALSE,
  linesep = "\\addlinespace", 
  align = rep("c", 4)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```

Moreover, Figure \@ref(fig:basic-model-state-estimates) visualizes the marginal posterior mean for the latent state and corresponding $95\%$ pointwise credible intervals. One can directly see that the estimated latent mean captures structural changes in the PD. 

```{r, basic-model-state-estimates, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Random walk model: Filtered latent state", fig.width=6, fig.height=4, warning=FALSE}
traj <- rbind(
  result[[1]]@filter.traj[1, 10000:50000, ], 
  result[[2]]@filter.traj[1, 10000:50000, ],
  result[[3]]@filter.traj[1, 10000:50000, ]
)

df <- data.frame(time = dates, y = y$lpd, x = colMeans(traj)[-1])
df <- reshape2::melt(df, id.vars = "time")
q <- data.frame(
  time = dates,
  variable = "x",
  lower = apply(traj, 2, quantile, probs = 0.025)[-1],
  upper = apply(traj, 2, quantile, probs = 0.975)[-1]
)
ggplot(data = df, aes(x = time, y = value, group = variable)) +
  geom_line(aes(color = variable)) + 
  geom_ribbon(
    data = q, aes(x = time, ymin = lower, ymax = upper, fill = "red",
                  colour = variable),
    inherit.aes = FALSE, alpha = 0.25, show.legend = FALSE, linetype = 2,
    colour = "red", size = 0.25
  ) +
  theme_light() +
  scale_color_manual(
    values = c("black", "red"), 
    labels = unname(c(
      latex2exp::TeX("PD: $\\eta_t$"),
      latex2exp::TeX("Time-varying mean: $\\tilde{\\eta}_t$")
   ))
  ) +
  theme(
    legend.position = "bottom", 
    legend.title = element_blank(), 
    legend.background = element_rect(fill = "transparent"),
    plot.margin = unit(c(0.1,0.1,0.1,0.1), "cm"),
    plot.title = element_text(hjust = 0.5, size = 8),
    axis.title = element_blank(),
    axis.text.x = element_text(size = 7),
    axis.text.y = element_text(size = 7)
  )
```

## Cointegration analysis

The connection between economic fundamentals and financial markets will be studied by inspecting possible economic factors that drive the latent mean of the PD. In [@h2_paper] three long-term determinants of the latent mean of the PD have been found. These three factors form a cointegration relationship with the time varying mean $\tilde{\eta}_t$: consumption risk, the demographic structure of the population and the dividend payout policy of firms.

A similar cointegration analysis will be conducted by analyzing the effect of additional covariates. The previously established connection with consumption risk and the demographic structure of the population will be added. Overall, the following economic indicators will be analyzed, see Table \@ref(tab:cointegration-analysis-name-variables).

```{r, cointegration-analysis-name-variables, eval=TRUE, echo=FALSE}
result_table <- data.frame(
  description = c(
    "Consumption Risk",
    "Middle-Aged to Young Ratio",
    "Effective Federal Funds Rate",
    "Log Real M1 Money Supply (Trillions)",
    "Log Real GDP (10 Trillions)"
  )
)
rownames(result_table) <- c("$cr_t$", "$my_t$", "$fr_t$", "$ms_t$", "$gdp_t$")
opts <- options(knitr.kable.NA = "")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("Description"),
  caption = "Cointegration analysis: Exogenous covariates", 
  booktabs = TRUE, escape = FALSE,
  align = rep("l", 1)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```

Annual population data has been collected from Datastream ("US-
POP24Y" for the 20–24-year old agents, "USPOP29Y" for the 25–29, "USPOP44Y"
for the 40–44, and "USPOP49Y" for the 45–49, "USPOPTO" for the total US population). The middle-aged to young ratio has been computed as the ratio of 40-49 to the 20-29 year old agents. Quarterly data between the end of consecutive years has been obtained by applying linear interpolation. 

As has been proposed in [@consumption_risk], consumption risk has been computed as $cr_t = \ln({\sum_{i = 0}^{12}\left | co_{t-i} \right |})$ where $co_{t}$ is the residual from an autoregressive (AR) order one regression for real per capita consumption growth. The real per capita consumption has been computed by taking real personal consumption data from [@real_consumption_data] and the retrieved total US population data from Datastream.

The impact of monetary policy has been of large interest in the asset pricing literature, see [@monetary_policy_asset_prices]. Here, the effective federal funds rate, obtained from [@federal_funds_rate_data], and the log real M1 money supply, retrieved from [@m1_money_supply_data], have been added for establishing a possible link between monetary policy and the latent mean of the PD. Furthermore, the log real GDP [@gdp_data] as an indicator of economic growth has been added. 

Figure \@ref(fig:cointegration-analysis-plot-covariates) depicts the time varying mean $\tilde{\eta}_t$ and all accompanying exogenous covariates. The economic effects of the COVID-19 pandemic are directly visible by inspecting the consumption risk $cr_t$, real effective funds rate $fr_t$, log M1 money supply $ms_t$ and log real GDP $gdp_t$ time series. As a reaction to the shock that resulted in an sharp increase in consumption risk and large drop in GDP, the Federal Reserve intervened by lowering the federal funds rate, see [@fed_lower_funds_rate], and increasing the money supply by extending quantitative easing, see [@fed_qe].

```{r, cointegration-analysis-plot-covariates, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Cointegration analysis: Visualization of exogenous covariates", fig.width=6, fig.height=6.5, warning=FALSE}
traj <- rbind(
  result[[1]]@filter.traj[1, 10000:50000, ], 
  result[[2]]@filter.traj[1, 10000:50000, ],
  result[[3]]@filter.traj[1, 10000:50000, ]
)

# Plot theme:
p_theme <- theme(
  legend.justification = c(0, 1), legend.position = c(0, 1), 
  legend.title = element_blank(), 
  legend.background = element_rect(fill = "transparent"),
  plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"),
  plot.title = element_text(hjust = 0.5, size = 8),
  axis.title = element_blank(),
  legend.key = element_rect(fill = NA)
)

# e_lpd
df <- data.frame(time = dates, y = colMeans(traj)[-1])
p1 <- ggplot(data = df, aes(x = time, y = y)) +
  geom_line() + theme_light() + p_theme +
  labs(
    title = latex2exp::TeX(
      "Time-varying mean of the PD: \\textbf{$\\tilde{\\eta}_t$}"
    )
  )

# cr
df <- data.frame(time = dates, cr = y$cr)
p2 <- ggplot(data = df, aes(x = time, y = cr)) +
  geom_line() + theme_light() + p_theme +
  labs(title = latex2exp::TeX("Consumption Risk: \\textbf{$cr_t$}"))

# mys
df <- data.frame(time = dates, mys = y$mys)
p3 <- ggplot(data = df, aes(x = time, y = mys)) +
  geom_line() + theme_light() + p_theme +
  labs(title = latex2exp::TeX("Middle-Aged to Young Ratio: \\textbf{$my_t$}"))

# fr
df <- data.frame(time = dates, fr = y$fr)
p4 <- ggplot(data = df, aes(x = time, y = fr)) +
  geom_line() + theme_light() + p_theme +
  labs(title = latex2exp::TeX("Effective Federal Funds Rate: \\textbf{$fr_t$}"))

# ms
df <- data.frame(time = dates, ms = y$ms)
p5 <- ggplot(data = df, aes(x = time, y = ms)) +
  geom_line() + theme_light() + p_theme +
  labs(title = latex2exp::TeX("Log Real M1 Money Supply: \\textbf{$ms_t$}"))

# Phi
# ms
df <- data.frame(time = dates, gdp = y$gdp)
p6 <- ggplot(data = df, aes(x = time, y = gdp)) +
  geom_line() + theme_light() + p_theme +
  labs(title = latex2exp::TeX("Log Real GDP: \\textbf{$gdp_t$}"))

grid.arrange(
  p1 + theme(legend.position = "none"), 
  p2 + theme(legend.position = "none"), 
  p3 + theme(legend.position = "none"), 
  p4 + theme(legend.position = "none"), 
  p5 + theme(legend.position = "none"), 
  p6 + theme(legend.position = "none"), 
  ncol = 2
)
```

Before investigating the cointegration relationship, the individual variables will be characterized by means of the augmented Dickey-Fuller (ADF) test. Table \@ref(tab:cointegration-analysis-unit-root-tests) shows the test statistics for all variables. For variables in level the ADF test has been applied with a constant and drift and for variables in difference only a constant term has been added. All tests have been applied using only one lag.

```{r, cointegration-analysis-unit-root-tests, eval=TRUE, echo=FALSE}
traj <- rbind(
  result[[1]]@filter.traj[1, 10000:50000, ], 
  result[[2]]@filter.traj[1, 10000:50000, ],
  result[[3]]@filter.traj[1, 10000:50000, ]
)
e_lpd <- colMeans(traj)[-1]

# with corona
e_lpd1 <- urca::summary(urca::ur.df(e_lpd, type = "trend", lags = 1))
cr1 <- urca::summary(urca::ur.df(y$cr, type = "trend", lags = 1))
my1 <- urca::summary(urca::ur.df(y$mys, type = "trend", lags = 1))
fr1 <- urca::summary(urca::ur.df(y$fr, type = "trend", lags = 1))
ms1 <- urca::summary(urca::ur.df(y$ms, type = "trend", lags = 1))
gdp1 <- urca::summary(urca::ur.df(y$gdp, type = "trend", lags = 1))

d_e_lpd1 <- urca::summary(urca::ur.df(diff(e_lpd), type = "drift", lags = 1))
d_cr1 <- urca::summary(urca::ur.df(diff(y$cr), type = "drift", lags = 1))
d_my1 <- urca::summary(urca::ur.df(diff(y$mys), type = "drift", lags = 1))
d_d_my1 <- urca::summary(urca::ur.df(diff(diff(y$mys)), type = "drift", lags = 1))
d_fr1 <- urca::summary(urca::ur.df(diff(y$fr), type = "drift", lags = 1))
d_ms1 <- urca::summary(urca::ur.df(diff(y$ms), type = "drift", lags = 1))
d_gdp1 <- urca::summary(urca::ur.df(diff(y$gdp), type = "drift", lags = 1))
with_corona <- list(
  e_lpd1, cr1, my1, fr1, ms1, gdp1, d_e_lpd1, d_cr1, d_my1, d_d_my1, d_fr1, d_ms1, d_gdp1
)

result_table <- data.frame(
  with_corona = sapply(with_corona, function(x){x@teststat[1]}),
  c_1 = sapply(with_corona, function(x){x@cval[1, 1]}),
  c_2 = sapply(with_corona, function(x){x@cval[1, 2]}),
  c_3 = sapply(with_corona, function(x){x@cval[1, 3]})
)
stars <- rev(c("*", "**", "***"))
result_table <- round(result_table, 4)
result_table[, 1] <- sapply(1:13, function(i){paste(
    result_table[i, 1], 
    ifelse(
        is.na(stars[result_table[i, 1] < with_corona[[i]]@cval[1, ]][1]),
        "", stars[result_table[i, 1] < with_corona[[i]]@cval[1, ]][1]
    )
)})

rownames(result_table) <- c(
  "$\\tilde{\\eta}_t$", "$cr_t$", "$my_t$", "$fr_t$", "$ms_t$", "$gdp_t$",
  "$\\Delta\\tilde{\\eta}_t$", "$\\Delta cr_t$", "$\\Delta my_t$", "$\\Delta^2 my_t$",
  "$\\Delta fr_t$", "$\\Delta ms_t$",  "$\\Delta gdp_t$"
)
opts <- options(knitr.kable.NA = "")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("t-statistic", "$\\alpha = 1\\%$", "$\\alpha = 5\\%$",
                "$\\alpha = 10\\%$"),
  caption = "Cointegration analysis: Results ADF unit root test", 
  booktabs = TRUE, escape = FALSE,
  linesep = "", 
  align = rep("l", 4)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position") %>%
  kableExtra::add_header_above(c("", "", "Critical Values" = 3)) %>%
  kableExtra::footnote(
    symbol = c("Significance at 1% level", 
               "Significance at 5% level",
               "Significance at 10% level"),
    symbol_manual = rev(c("*", "**", "***")),
    footnote_as_chunk = F
  )
```

The test results of most factors indicate first order integration at conventional significance levels. Only the effective federal funds rate has a significance level of $10\%$. For this master's thesis the federal funds rate will be handled as a first order integrated time series. Additionally, the test results for the middle-aged to young ratio indicate second order integration. However, for longer time series there exist significant evidence for first order integration, [@long_term_mys]. Hence, the middle-aged to young ratio will be treated as a first order integrated process as well.

Moreover, the cointegration analysis will be applied using the following single equation error correction model (SECM)
\begin{equation*}
\begin{split}
\Delta \tilde{\eta}_t = &\alpha(\tilde{\eta}_{t-1} - \beta_0 - \beta_1 cr_{t-1} - \beta_2 my_{t-1} - \beta_3 fr_{t-1} - \beta_4 ms_{t-1} - \beta_5 gdp_{t-1}) + \\
&\delta_1 \Delta cr_{t} + \delta_2 \Delta my_{t} + \delta_3 \Delta fr_{t} + \beta_4 \Delta ms_{t} + \beta_5 \Delta gdp_{t} + \phi_1\Delta \tilde{\eta}_{t-1} + \phi_2\Delta \tilde{\eta}_{t-2} + e_t
\end{split}
\end{equation*}
```{r, cointegration-analysis-study-1, eval=TRUE, echo=FALSE}
traj <- rbind(
  result[[1]]@filter.traj[1, 10000:50000, ], 
  result[[2]]@filter.traj[1, 10000:50000, ],
  result[[3]]@filter.traj[1, 10000:50000, ]
)

data <- zoo::as.zoo(data.frame(
  e_lpd = colMeans(traj)[-1], cr = y$cr, mys = y$mys, fr = y$fr, ms = y$ms, 
  gdp = y$gdp
), order.by = zoo::index(1:length(y$cr)))

# complete analysis
ardl_model_1 <- ARDL::ardl(
  e_lpd ~ cr + mys + fr + ms + gdp, order = c(3, 1, 1, 1, 1, 1), data = data
)
cointegration_model_1 <- ARDL::uecm(ardl_model_1)
m_1 <- ARDL::multipliers(cointegration_model_1)
cointegration_model_1 <- summary(cointegration_model_1)

# drop cr, fr, gdp
ardl_model_2 <- ARDL::ardl(
  e_lpd ~ mys + ms, order = c(3, 1, 1), data = data
)
cointegration_model_2 <- ARDL::uecm(ardl_model_2)
m_2 <- ARDL::multipliers(cointegration_model_2)
cointegration_model_2 <- summary(cointegration_model_2)

result_table <- data.frame(
  complete = c(
    cointegration_model_1$coefficients[2, 1], m_1[1:6, 2],
    cointegration_model_1$r.squared, cointegration_model_1$adj.r.squared
  ),
  droped = c(
    cointegration_model_2$coefficients[2, 1], m_2[1, 2], NA, m_2[2, 2], 
    NA, m_2[3, 2], NA, 
    cointegration_model_2$r.squared, cointegration_model_2$adj.r.squared
  )
)
result_table <- round(result_table, 4)
stars <- rev(c("*", "**", "***"))

# Cointegration test
result_table[1, 1] <- paste(
  result_table[1, 1], 
  ifelse(
    is.na(
      stars[cointegration_model_1$coefficients[2, 3] < c(-4.7970, -4.1922, -3.8670)][1]
    ),
    "", 
    stars[cointegration_model_1$coefficients[2, 3] < c(-4.7970, -4.1922, -3.8670)][1]
  )
)
result_table[1, 2] <- paste(
  result_table[1, 2], 
  ifelse(
    is.na(
      stars[cointegration_model_2$coefficients[2, 3] < c(-4.0947, -3.5057, -3.1924)][1]
    ),
    "", 
    stars[cointegration_model_2$coefficients[2, 3] < c(-4.0947, -3.5057, -3.1924)][1]
  )
)

# Long run relationship test
result_table[2:7, 1] <- sapply(2:7, function(i){paste(
    result_table[i, 1],
    ifelse(
        is.na(stars[m_1[i - 1, 5] < c(0.01, 0.05, 0.1)][1]),
        "", stars[m_1[i - 1, 5] < c(0.01, 0.05, 0.1)][1]
    )
)})
result_table[c(2, 4, 6), 2] <- sapply(1:3, function(i){paste(
    
    result_table[c(2, 4, 6)[i], 2],
    ifelse(
        is.na(stars[m_2[i, 5] < c(0.01, 0.05, 0.1)][1]),
        "", stars[m_2[i, 5] < c(0.01, 0.05, 0.1)][1]
    )
)})
rownames(result_table) <- c(
  "$\\alpha$", "$\\beta_0$", "$\\beta_1$", "$\\beta_2$", "$\\beta_3$",
  "$\\beta_4$", "$\\beta_5$", "$R^2$", "adj $R^2$"
)
```
Table \@ref(tab:cointegration-analysis-study-2) displays the results of the inspected cointegration relationship based on a nonlinear regression approach. Test statistics for the adjustment coefficient $\alpha$ are used for testing a possible cointegration relationship. The model including all covariates has no significant cointegration relationship based on a $10\%$ significance level. The statistic of `r round(cointegration_model_1$coefficients[2, 3], 3)` is larger than the critical value $-3.8670$ at the $10\%$ level, obtained from the surface regression approach by [@cointegration_critical_values]. 
```{r, cointegration-analysis-study-2, eval=TRUE, echo=FALSE}
opts <- options(knitr.kable.NA = "-")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("All covariates", "Excluding $cr_t$, $fr_t$, $gdp_t$"),
  caption = "Cointegration analysis: Results", 
  booktabs = TRUE, escape = FALSE,
  linesep = "", 
  align = rep("l", 2)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position") %>%
  kableExtra::footnote(
    symbol = c("Significance at 1% level", 
               "Significance at 5% level",
               "Significance at 10% level"),
    symbol_manual = rev(c("*", "**", "***")),
    footnote_as_chunk = F
  )
```

Furthermore, the test has been repeated by removing the insignificant covariates $cr_t$, $fr_t$ and $gdp_t$. Again the cointegration relationship is insignificant at a $10\%$ level. However, the test statistic `r round(cointegration_model_2$coefficients[2, 3], 3)` is larger but relatively close to the critical value $-3.1924$. 

Overall, the cointegration analysis has been applied under the assumption that the latent mean of the PD follows a random walk state process. Contrary to the results presented in [@h2_paper] a cointegration relationship could no be established. Besides assuming that this connection is not present for the collected data, multiple reasons are available for the mixing results. 

Firstly, it is possible that the state process is misspecified and another specification, possibly already including covariates, is more suitable. Additionally, the impact of the missing dividend payout policy factor used in [@h2_paper] is not accounted here. Moreover, it is possible that the cointegration relationship is only present for data with lower frequency than quarterly. Furthermore, different data sources and a shorter time frame have been used. 

## State process extension with covariates

As mentioned, it is possible that the latent mean of the PD could be misspecified by using a random walk state process. Hence, this chapter will continue the analysis of exogenous covariates effecting the latent process. An additional model containing covariates will be tested and the results will be compared with the random walk model. Firstly, the model extension and the estimation results will be presented. Then the latent state estimates will be compared and model comparison performed.

Common similarities between the approaches will be that the same parameter transformations as in the [simulation study][Simulation study] chapter will be applied. Hence, the hyperparameters $\psi$, $\varsigma_\epsilon$ and $\varsigma_u$ will be used. Moreover, the PMMH approach will be run in parallel using 3 independent Markov chains, $J = 1000$ particles and $M = 50000$ iterations. Similar to the random walk state process a flat prior specification will be used and the first $10000$ iterations of the PMMH approach will be dropped.

The model extension (**Covariate Model**) describes a possible long run equilibrium relationship that could not be detected within the [cointegration analysis][Cointegration analysis]
\begin{equation}
\begin{split}
\tilde{\eta}_t &= \beta_0 + \beta_1 my_{t} + \beta_2 ms_{t} + u_t \\
u_t &\sim \mathcal{N}(0, \sigma_u^2)
(\#eq:static-model-state-process)
\end{split}
\end{equation}
Here, only the middle-aged to young ratio and the log real M1 money suppy have been added. Combined with the observation process \@ref(eq:observation-process) the necessary parameters are $\theta = \begin{pmatrix} \beta_0 & \beta_1 & \beta_2 & \sigma_{\epsilon} & \sigma_{u}\end{pmatrix}^T$. Note that due to the missing autoregressive part of the model the initial value $\tilde{\eta}_0$ does not need to be included.

Furthermore, as initial parameters $\theta^{(0)}$ the long-run equilibrium parameter estimates from the single equation error correction model from Table \@ref(tab:cointegration-analysis-study-2) will be used
$$
\theta^{(0)} = \begin{pmatrix}\beta_0^{(0)} \\ \beta_1^{(0)} \\ \beta_2^{(0)} \\ \varsigma_\epsilon^{(0)} \\ \varsigma_u^{(0)} \end{pmatrix} = \begin{pmatrix} 2.4 \\ 1.6 \\ 0.4 \\ \ln(0.05) \\ \ln(0.05) \end{pmatrix}
$$
Moreover, for this model the following proposal distribution is used
$$\theta_i^{*}\sim \mathcal{N}(\theta_i^{(m-1)}, 0.015^2)$$
Figure \@ref(fig:extension-static-model-trace) shows the trace plot for each parameter and each Markov chain. Furthermore, Figure \@ref(fig:extension-static-model-marginal-density) displays the Gaussian kernel density estimates of the marginal posterior distributions. Table \@ref(tab:pmmh-estimation-result) summarizes the estimation results
```{r, pmmh-estimation-result, eval=TRUE, echo=FALSE}
library(tidyverse)

model_1 <- lapply(
  readRDS(
    here::here("data", "results", "model_study_result_static_model.rds")
  ), function(x){x@traces[10000:50000, ]}
)
model_1 <- do.call(rbind, model_1)

result_table <- data.frame(
  model_1_mean = c(
    colMeans(model_1[, 3:5]), colMeans(exp(model_1[, 6:7]))
  ),
  model_1_median = c(
    apply(model_1[, 3:5], 2, median),
    apply(exp(model_1[, 6:7]), 2, median)
  ),
  model_1_lower = c(
    quantile(model_1[, 3], probs = 0.025), 
    quantile(model_1[, 4], probs = 0.025), 
    quantile(model_1[, 5], probs = 0.025), 
    quantile(exp(model_1[, 6]), probs = 0.025), 
    quantile(exp(model_1[, 7]), probs = 0.025)
  ),
  model_1_upper = c(
    quantile(model_1[, 3], probs = 0.975), 
    quantile(model_1[, 4], probs = 0.975), 
    quantile(model_1[, 5], probs = 0.975), 
    quantile(exp(model_1[, 6]), probs = 0.975), 
    quantile(exp(model_1[, 7]), probs = 0.975)
  )
)
result_table <- data.frame(sapply(result_table, as.numeric))

result_table <- round(result_table, 4)
rownames(result_table) <- c(
  "$\\beta_0$", "$\\beta_1$", "$\\beta_2$", "$\\sigma_u$", 
  "$\\sigma_\\epsilon$"
)
opts <- options(knitr.kable.NA = "-")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("Mean", "Median", "$0.025\\%$", "$0.975\\%$"),
  caption = "Covariate extension: PMMH results", 
  booktabs = TRUE, escape = FALSE,
  linesep = "", 
  align = rep("c", 3)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```
As can be seen, the estimated parameters $\beta_0$, $\beta_1$ and $\beta_2$ diverge from the estimated long-run relationship in the cointegration analysis. However, the estimates using the state process containing covariates are close to the results already presented in Table \@ref(tab:cointegration-analysis-study-2). Moreover, the standard deviation $\sigma_u$ for the error term in the latent state process is significantly larger for the covariate extension compared to the random walk model.

Figure \@ref(fig:pmmh-estimation-figure) displays the filtered latent state for the model extension and the model with random walk state process. Both filtered latent state estimates are moving closely together with the random walk model being smoother and the covariate extension model having more spikes. 

```{r, pmmh-estimation-figure, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Covariate extension: Filtered latent state", fig.width=6, fig.height=4, warning=FALSE}
library(ggplot2)

rw_model <- lapply(
  readRDS(
    here::here("data", "results", "model_study_result_basic_model.rds")
  ), function(x){x@filter.traj[1, 10000:50000, ]}
)
rw_model <- do.call(rbind, rw_model)[, -1]
static_model <- lapply(
  readRDS(
    here::here("data", "results", "model_study_result_static_model.rds")
  ), function(x){x@filter.traj[1, 10000:50000, ]}
)
static_model <- do.call(rbind, static_model)[, -1]

df <- data.frame(time = dates, lpd = y$lpd, rw_model = colMeans(rw_model), 
                 static_model = colMeans(static_model))
df <- reshape2::melt(df, id.vars = "time")
ggplot(data = df, aes(x = time, y = value, group = variable)) +
  geom_line(aes(color = variable)) + 
  scale_color_manual(
    values = c("black", "red", "blue"), 
    labels = unname(c(
      latex2exp::TeX("PD: $\\eta_t$"),
      latex2exp::TeX("RW model: $\\tilde{\\eta}_t$"),
      latex2exp::TeX("Covariate model: $\\tilde{\\eta}_t$")
   ))
  ) +
  theme_light() +
  theme(
    legend.position = "bottom", 
    legend.title = element_blank(), 
    legend.background = element_rect(fill = "transparent"),
    plot.margin = unit(c(0.1,0.1,0.1,0.1), "cm"),
    plot.title = element_text(hjust = 0.5, size = 8),
    axis.title = element_blank(),
    axis.text.x = element_text(size = 7),
    axis.text.y = element_text(size = 7)
  )
```
For the model comparison the Deviance information criterion (DIC) will be used. Similar to the Akaike information criterion (AIC) and the Bayesian information criterion (BIC), the model with the lower DIC should be preferred. The deviance $D(\theta)$ is defined as
$$D(\theta) = -2\ell(\theta) + C$$
with $\ell(\theta$ being the log likelihood for the likelihood function \@ref(eq:pomp-likelihood). $C$ is a constant that cancels out in the model comparison and therefore it can be discarded. 

The DIC follows as
\begin{equation}
DIC = D(\hat\theta) + 2p_d \\
\end{equation}
where $\hat\theta$ is the mean of the posterior distribution and $p_d$ is the effective number of parameters that can be approximated by
$$p_d = \frac{1}{2}\hat{\text{Var}}(D(\theta))$$
Hence, $p_d$ is half the estimated variance of the deviance from the posterior samples $\theta$. Table \@ref(tab:pmmh-model-comparison) depicts the model comparison results. The log likelihood of $\hat\theta$ has been computed using $10$ runs of the particle filter with $1000$ particles and averaging the resulting log likelihoods. As visible, measured by the DIC the random walk model is superior.

```{r, pmmh-model-loglik, eval=TRUE, echo=FALSE}
l_rw_model <- readRDS(here::here("data", "results", "rw_lik.rds"))
l_cov_model <- readRDS(here::here("data", "results", "cov_lik.rds"))
```

```{r, pmmh-model-comparison, eval=TRUE, echo=FALSE}
r <- readRDS(
    here::here("data", "results", "model_study_result_basic_model.rds")
)
N <- ncol(r[[1]]@data)
rw_model <- lapply(r, function(x){x@traces[10000:50000, ]})
rw_model <- do.call(rbind, rw_model)

model_1 <- lapply(
    readRDS(
        here::here("data", "results", "model_study_result_static_model.rds")
    ), function(x){x@traces[10000:50000, ]}
)
model_1 <- do.call(rbind, model_1)

result_table <- data.frame(
  num_params = c(3, 5),
  lik = c(l_rw_model, l_cov_model),
  dic = c(
    -2*l_rw_model + var(-2*rw_model[, 1]), 
    -2*l_cov_model + var(-2*model_1[, 1])
  )
)

rownames(result_table) <- c("RW Model", "Covariate Model")
result_table <- round(result_table, 4)
opts <- options(knitr.kable.NA = "")
knitr::kable(
  result_table, 
  row.names = TRUE,
  col.names = c("Number of parameters", "$\\ell(\\hat{\\theta})$", "DIC"),
  caption = "Covariate extension: Model comparison", 
  booktabs = TRUE, escape = FALSE,
  linesep = "", 
  align = rep("c", 4)
) %>% 
  kableExtra::kable_paper() %>%
  kableExtra::kable_styling(latex_options = "hold_position")
rm(list = ls())
```


<!--chapter:end:Rmd//05-Empirical-Application.Rmd-->

# Conclusion {#conclusion}

The basic structure of partially observed Markov processes and corresponding numerical procedures for inference have been described briefly. Furthermore, a simulation study has been conducted and the results of likelihood based and Bayesian approaches for parameter and latent state estimation have been presented. There, different prior setups have been tested and a general superiority of either the likelihood or the Bayesian approach could not be identified. However, the advantage of the Bayesian approach for providing uncertainty quantification for parameter and latent state estimates has been used for the empirical application. 

Based on the log linear approximation of the present-value formulation presented in [@campbell_shiller_paper] a nonlinear state-space model has been derived as in [@h2_paper]. Firstly, a random walk state process has been used for the latent mean of the PD. The PMMH approach has been successfully applied for estimating the parameter and latent state and for providing the corresponding uncertainty quantification.

Similar to the steps applied in [@h2_paper], a cointegration analysis has been conducted by including macroeconomic variables characterizing monetary policy effects and economic growth. However, a cointegration relationship could not be detected. An additional model containing some of the covariates has been tested. The random walk state setup and the state model containing covariates follow similar paths. However, the model comparison results showed that the model with a random walk state process is superior.

All in all, the particle marginal Metropolis Hastings approach has been successfully applied to the context of a nonlinear state-space model for the log linear approximation of the present-value formulation.

As a possible outlook, further advances in MCMC approaches for nonlinear partially observed Markov processes are available. Especially, being able to generate proposals that are more likely to be accepted, by adding gradient estimates to the MCMC approaches, seems to be promising. Additionally, another interesting application would be the likelihood based iterated filtering procedure and the corresponding confidence interval estimation using the Monte Carlo adjusted
profile algorithm.

<!--chapter:end:Rmd//06-Conclusion.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# (APPENDIX) Appendix {-}

# Appendix: Figures {#figures}

```{r, sim-4-appendix, eval=TRUE, echo=FALSE}
# get simulation result
load(here::here("data", "results", "simulation_results.RData"))

library(ggplot2)
library(gridExtra)
```

```{r, sim-4-appendix-trace-plot-setup-1, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Exemplary simulation: Trace plot for setup 1", fig.width=6, fig.height=6.75, warning=FALSE}
traces <- sim_4$pmmh_noninformative$traces

# Plot theme:
p_theme <- theme(
  legend.justification = c(0, 1), legend.position = c(0, 1), 
  legend.title = element_blank(), 
  legend.background = element_rect(fill = "transparent"),
  plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"),
  plot.title = element_text(hjust = 0.5, size = 8),
  axis.title.x = element_text(size = 8),
  axis.title.y = element_text(size = 8, angle = 0, vjust = 0.5),
  axis.text.x = element_text(size = 7),
  axis.text.y = element_text(size = 7)
)

# Beta 0
df <- data.frame(
  time = 1:nrow(traces), variable = "beta_0", beta_0 = traces[, 3]
)
p1 <- ggplot(data = df, aes(x = time, y = beta_0)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = true_params_simulation["beta_0"], color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\beta_0$}"),
       x = "m", y = latex2exp::TeX("$\\beta_0$")) +
  p_theme

# Beta 1
df <- data.frame(
  time = 1:nrow(traces), beta_1 = traces[, 4], variable = "Trace"
)
p2 <- ggplot(data = df, aes(x = time, y = beta_1)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = true_params_simulation["beta_1"], color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\beta_1$}"),
       x = "m", y = latex2exp::TeX("$\\beta_1$")) +
  p_theme

# Beta 2
df <- data.frame(
  time = 1:nrow(traces), beta_2 = traces[, 5], variable = "Trace"
)
p3 <- ggplot(data = df, aes(x = time, y = beta_2)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = true_params_simulation["beta_2"], color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\beta_2$}"),
       x = "m", y = latex2exp::TeX("$\\beta_2$")) +
  p_theme

# Sigma x
df <- data.frame(
  time = 1:nrow(traces), sigma_x = exp(traces[, 6]), variable = "Trace"
)
p4 <- ggplot(data = df, aes(x = time, y = sigma_x)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = exp(true_params_simulation["sigma_x"]), color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\sigma_u$}"),
       x = "m", y = latex2exp::TeX("$\\sigma_u$")) +
  p_theme

# Sigma y
df <- data.frame(
  time = 1:nrow(traces), sigma_y = exp(traces[, 7]), variable = "Trace"
)
p5 <- ggplot(data = df, aes(x = time, y = sigma_y)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = exp(true_params_simulation["sigma_y"]), color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\sigma_\\epsilon$}"),
       x = "m", y = latex2exp::TeX("$\\sigma_\\epsilon$")) +
  p_theme

# Phi
df <- data.frame(
  time = 1:nrow(traces), phi = tanh(traces[, 8]), variable = "Trace"
)
p6 <- ggplot(data = df, aes(x = time, y = phi)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = tanh(true_params_simulation["phi"]), color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\phi$}"),
       x = "m", y = latex2exp::TeX("$\\phi$")) +
  p_theme

# X 0
df <- data.frame(
  time = 1:nrow(traces), x_0 = traces[, 9], variable = "Trace"
)
p7 <- ggplot(data = df, aes(x = time, y = x_0)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = true_params_simulation["x_0"], color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\x_0$}"),
       x = "m", y = latex2exp::TeX("$\\x_0$")) +
  p_theme

get_legend <- function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}
legend <- get_legend(p2)

grid.arrange(
  p1 + theme(legend.position = "none"), 
  p2 + theme(legend.position = "none"), 
  p3 + theme(legend.position = "none"), 
  p4 + theme(legend.position = "none"), 
  p5 + theme(legend.position = "none"), 
  p6 + theme(legend.position = "none"), 
  p7 + theme(legend.position = "none"), 
  ncol = 2, legend = legend
)
```

```{r, sim-4-appendix-marginal-posterior-setup-1, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Exemplary simulation: Gaussian kernel density estimates of the marginal posterior distributions for setup 1", fig.width=6, fig.height=6.75, warning=FALSE}
traces <- sim_4$pmmh_noninformative$traces[25000:50000, ]

# Plot theme:
p_theme <- theme(
  legend.justification = c(0, 1), legend.position = c(0, 1), 
  legend.title = element_blank(), 
  legend.background = element_rect(fill = "transparent"),
  plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"),
  plot.title = element_text(hjust = 0.5, size = 8),
  axis.title.x = element_text(size = 8),
  axis.title.y = element_text(size = 8, vjust = 0.5),
  legend.key = element_rect(fill = NA),
  axis.text.x = element_text(size = 7),
  axis.text.y = element_text(size = 7)
)

# Beta 0
df <- data.frame(time = 1:nrow(traces), beta_0 = traces[, 3])
p1 <- ggplot(data = df, aes(x = beta_0)) +
  geom_density(aes(x = beta_0, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = true_params_simulation["beta_0"], colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(traces[, 3]), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = function(...){1}, aes(colour = "Prior", linetype = "Prior")
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\beta_0$}"),
       x = latex2exp::TeX("$\\beta_0$"))

# Beta 1
df <- data.frame(time = 1:nrow(traces), beta_1 = traces[, 4])
p2 <- ggplot(data = df, aes(x = beta_1)) +
  geom_density(aes(x = beta_1, colour = "Marginal posterior", linetype = "Marginal posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = true_params_simulation["beta_1"], colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(traces[, 4]), 
        colour = "Marginal posterior mean", linetype = "Marginal posterior mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = function(...){1}, aes(colour = "Prior", linetype = "Prior")
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Marginal posterior" = "black", "Prior" = "#30914a", 
               "True" = "red",
               "Marginal posterior mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Marginal posterior" = 1, "Marginal posterior mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\beta_1$}"),
       x = latex2exp::TeX("$\\beta_1$"))

# Beta 2
df <- data.frame(time = 1:nrow(traces), beta_2 = traces[, 5])
p3 <- ggplot(data = df, aes(x = beta_2)) +
  geom_density(aes(x = beta_2, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = true_params_simulation["beta_2"], colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(traces[, 5]), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = function(...){1}, aes(colour = "Prior", linetype = "Prior")
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\beta_2$}"),
       x = latex2exp::TeX("$\\beta_2$"))

# Sigma x
df <- data.frame(time = 1:nrow(traces), sigma_x = exp(traces[, 6]))
p4 <- ggplot(data = df, aes(x = sigma_x)) +
  geom_density(aes(x = sigma_x, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = exp(true_params_simulation["sigma_x"]), colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(exp(traces[, 6])), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = function(...){1}, aes(colour = "Prior", linetype = "Prior")
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\sigma_u$}"),
       x = latex2exp::TeX("$\\sigma_u$"))

# Sigma y
df <- data.frame(time = 1:nrow(traces), sigma_y = exp(traces[, 7]))
p5 <- ggplot(data = df, aes(x = sigma_y)) +
  geom_density(aes(x = sigma_y, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = exp(true_params_simulation["sigma_y"]), colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(exp(traces[, 7])), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = function(...){1}, aes(colour = "Prior", linetype = "Prior")
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\sigma_\\epsilon$}"),
       x = latex2exp::TeX("$\\sigma_\\epsilon$"))

# Phi
df <- data.frame(time = 1:nrow(traces), phi = tanh(traces[, 8]))
p6 <- ggplot(data = df, aes(x = phi)) +
  geom_density(aes(x = phi, colour = "Posterior", linetype = "Posterior"),
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = tanh(true_params_simulation["phi"]), colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(tanh(traces[, 8])), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = function(...){1}, aes(colour = "Prior", linetype = "Prior")
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\phi$}"),
       x = latex2exp::TeX("$\\phi$"))

# X 0
df <- data.frame(time = 1:nrow(traces), x_0 = traces[, 9])
p7 <- ggplot(data = df, aes(x = x_0)) +
  geom_density(aes(x = x_0, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = true_params_simulation["x_0"], colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(traces[, 9]), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = function(...){1}, aes(colour = "Prior", linetype = "Prior")
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\x_0$}"),
       x = latex2exp::TeX("$\\x_0$"))

get_legend <- function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}
legend <- get_legend(p2)

grid.arrange(
  p1 + theme(legend.position = "none"), 
  p2 + theme(legend.position = "none"), 
  p3 + theme(legend.position = "none"), 
  p4 + theme(legend.position = "none"), 
  p5 + theme(legend.position = "none"), 
  p6 + theme(legend.position = "none"), 
  p7 + theme(legend.position = "none"), 
  ncol = 2, legend = legend
)
```

```{r, sim-4-appendix-trace-plot-setup-2, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Exemplary simulation: Trace plot for setup 2", fig.width=6, fig.height=6.75, warning=FALSE}
traces <- sim_4$pmmh_true_informative$traces

# Plot theme:
p_theme <- theme(
  legend.justification = c(0, 1), legend.position = c(0, 1), 
  legend.title = element_blank(), 
  legend.background = element_rect(fill = "transparent"),
  plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"),
  plot.title = element_text(hjust = 0.5, size = 8),
  axis.title.x = element_text(size = 8),
  axis.title.y = element_text(size = 8, angle = 0, vjust = 0.5),
  axis.text.x = element_text(size = 7),
  axis.text.y = element_text(size = 7)
)

# Beta 0
df <- data.frame(
  time = 1:nrow(traces), variable = "beta_0", beta_0 = traces[, 3]
)
p1 <- ggplot(data = df, aes(x = time, y = beta_0)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = true_params_simulation["beta_0"], color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\beta_0$}"),
       x = "m", y = latex2exp::TeX("$\\beta_0$")) +
  p_theme

# Beta 1
df <- data.frame(
  time = 1:nrow(traces), beta_1 = traces[, 4], variable = "Trace"
)
p2 <- ggplot(data = df, aes(x = time, y = beta_1)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = true_params_simulation["beta_1"], color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\beta_1$}"),
       x = "m", y = latex2exp::TeX("$\\beta_1$")) +
  p_theme

# Beta 2
df <- data.frame(
  time = 1:nrow(traces), beta_2 = traces[, 5], variable = "Trace"
)
p3 <- ggplot(data = df, aes(x = time, y = beta_2)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = true_params_simulation["beta_2"], color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\beta_2$}"),
       x = "m", y = latex2exp::TeX("$\\beta_2$")) + 
  p_theme

# Sigma x
df <- data.frame(
  time = 1:nrow(traces), sigma_x = exp(traces[, 6]), variable = "Trace"
)
p4 <- ggplot(data = df, aes(x = time, y = sigma_x)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = exp(true_params_simulation["sigma_x"]), color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\sigma_u$}"),
       x = "m", y = latex2exp::TeX("$\\sigma_u$")) +
  p_theme

# Sigma y
df <- data.frame(
  time = 1:nrow(traces), sigma_y = exp(traces[, 7]), variable = "Trace"
)
p5 <- ggplot(data = df, aes(x = time, y = sigma_y)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = exp(true_params_simulation["sigma_y"]), color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\sigma_\\epsilon$}"),
       x = "m", y = latex2exp::TeX("$\\sigma_\\epsilon$")) +
  p_theme

# Phi
df <- data.frame(
  time = 1:nrow(traces), phi = tanh(traces[, 8]), variable = "Trace"
)
p6 <- ggplot(data = df, aes(x = time, y = phi)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = tanh(true_params_simulation["phi"]), color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\phi$}"),
       x = "m", y = latex2exp::TeX("$\\phi$")) +
  p_theme

# X 0
df <- data.frame(
  time = 1:nrow(traces), x_0 = traces[, 9], variable = "Trace"
)
p7 <- ggplot(data = df, aes(x = time, y = x_0)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = true_params_simulation["x_0"], color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\x_0$}"),
       x = "m", y = latex2exp::TeX("$\\x_0$")) +
  p_theme

get_legend <- function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}
legend <- get_legend(p2)

grid.arrange(
  p1 + theme(legend.position = "none"), 
  p2 + theme(legend.position = "none"), 
  p3 + theme(legend.position = "none"), 
  p4 + theme(legend.position = "none"), 
  p5 + theme(legend.position = "none"), 
  p6 + theme(legend.position = "none"), 
  p7 + theme(legend.position = "none"), 
  ncol = 2, legend = legend
)
```

```{r, sim-4-appendix-marginal-posterior-setup-2, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Exemplary simulation: Gaussian kernel density estimates of the marginal posterior distributions for setup 2", fig.width=6, fig.height=6.75, warning=FALSE}
traces <- sim_4$pmmh_true_informative$traces[25000:50000, ]

# Plot theme:
p_theme <- theme(
  legend.justification = c(0, 1), legend.position = c(0, 1), 
  legend.title = element_blank(), 
  legend.background = element_rect(fill = "transparent"),
  plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"),
  plot.title = element_text(hjust = 0.5, size = 8),
  axis.title.x = element_text(size = 8),
  axis.title.y = element_text(size = 8, vjust = 0.5),
  legend.key = element_rect(fill = NA),
  axis.text.x = element_text(size = 7),
  axis.text.y = element_text(size = 7)
)

# Beta 0
df <- data.frame(time = 1:nrow(traces), beta_0 = traces[, 3])
p1 <- ggplot(data = df, aes(x = beta_0)) +
  geom_density(aes(x = beta_0, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = true_params_simulation["beta_0"], colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(traces[, 3]), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = dnorm, aes(colour = "Prior", linetype = "Prior"),
    args = list(mean = 0.4, sd = 0.1)
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\beta_0$}"),
       x = latex2exp::TeX("$\\beta_0$"))

# Beta 1
df <- data.frame(time = 1:nrow(traces), beta_1 = traces[, 4])
p2 <- ggplot(data = df, aes(x = beta_1)) +
  geom_density(aes(x = beta_1, colour = "Marginal posterior", linetype = "Marginal posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = true_params_simulation["beta_1"], colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(traces[, 4]), 
        colour = "Marginal posterior mean", linetype = "Marginal posterior mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = dnorm, aes(colour = "Prior", linetype = "Prior"),
    args = list(mean = 0.5, sd = 0.2)
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Marginal posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Marginal posterior mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Marginal posterior" = 1, "Marginal posterior mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\beta_1$}"),
       x = latex2exp::TeX("$\\beta_1$"))

# Beta 2
df <- data.frame(time = 1:nrow(traces), beta_2 = traces[, 5])
p3 <- ggplot(data = df, aes(x = beta_2)) +
  geom_density(aes(x = beta_2, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = true_params_simulation["beta_2"], colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(traces[, 5]), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = dnorm, aes(colour = "Prior", linetype = "Prior"),
    args = list(mean = -0.3, sd = 0.2)
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\beta_2$}"),
       x = latex2exp::TeX("$\\beta_2$"))

# Sigma x
df <- data.frame(time = 1:nrow(traces), sigma_x = exp(traces[, 6]))
p4 <- ggplot(data = df, aes(x = sigma_x)) +
  geom_density(aes(x = sigma_x, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = exp(true_params_simulation["sigma_x"]), colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(exp(traces[, 6])), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = dgamma, aes(colour = "Prior", linetype = "Prior"),
    args = list(shape = 12, scale = 0.005)
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\sigma_u$}"),
       x = latex2exp::TeX("$\\sigma_u$"))

# Sigma y
df <- data.frame(time = 1:nrow(traces), sigma_y = exp(traces[, 7]))
p5 <- ggplot(data = df, aes(x = sigma_y)) +
  geom_density(aes(x = sigma_y, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = exp(true_params_simulation["sigma_y"]), colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(exp(traces[, 7])), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = dgamma, aes(colour = "Prior", linetype = "Prior"),
    args = list(shape = 6, scale = 0.005)
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\sigma_\\epsilon$}"),
       x = latex2exp::TeX("$\\sigma_\\epsilon$"))

# Phi
df <- data.frame(time = 1:nrow(traces), phi = tanh(traces[, 8]))
p6 <- ggplot(data = df, aes(x = phi)) +
  geom_density(aes(x = phi, colour = "Posterior", linetype = "Posterior"),
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = tanh(true_params_simulation["phi"]), colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(tanh(traces[, 8])), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = truncnorm::dtruncnorm, aes(colour = "Prior", linetype = "Prior"),
    args = list(a = -1, b = 1, mean = 0.8, sd = 0.1)
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\phi$}"),
       x = latex2exp::TeX("$\\phi$"))

# X 0
df <- data.frame(time = 1:nrow(traces), x_0 = traces[, 9])
p7 <- ggplot(data = df, aes(x = x_0)) +
  geom_density(aes(x = x_0, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = true_params_simulation["x_0"], colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(traces[, 9]), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = dnorm, aes(colour = "Prior", linetype = "Prior"),
    args = list(mean = 3.5, sd = 0.1)
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\x_0$}"),
       x = latex2exp::TeX("$\\x_0$"))

get_legend <- function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}
legend <- get_legend(p2)

grid.arrange(
  p1 + theme(legend.position = "none"), 
  p2 + theme(legend.position = "none"), 
  p3 + theme(legend.position = "none"), 
  p4 + theme(legend.position = "none"), 
  p5 + theme(legend.position = "none"), 
  p6 + theme(legend.position = "none"), 
  p7 + theme(legend.position = "none"), 
  ncol = 2, legend = legend
)
```

```{r, sim-4-appendix-trace-plot-setup-3, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Exemplary simulation: Trace plot for setup 3", fig.width=6, fig.height=6.75, warning=FALSE}
traces <- sim_4$pmmh_false_informative$traces

# Plot theme:
p_theme <- theme(
  legend.justification = c(0, 1), legend.position = c(0, 1), 
  legend.title = element_blank(), 
  legend.background = element_rect(fill = "transparent"),
  plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"),
  plot.title = element_text(hjust = 0.5, size = 8),
  axis.title.x = element_text(size = 8),
  axis.title.y = element_text(size = 8, angle = 0, vjust = 0.5),
  axis.text.x = element_text(size = 7),
  axis.text.y = element_text(size = 7)
)

# Beta 0
df <- data.frame(
  time = 1:nrow(traces), variable = "beta_0", beta_0 = traces[, 3]
)
p1 <- ggplot(data = df, aes(x = time, y = beta_0)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = true_params_simulation["beta_0"], color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\beta_0$}"),
       x = "m", y = latex2exp::TeX("$\\beta_0$")) +
  p_theme

# Beta 1
df <- data.frame(
  time = 1:nrow(traces), beta_1 = traces[, 4], variable = "Trace"
)
p2 <- ggplot(data = df, aes(x = time, y = beta_1)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = true_params_simulation["beta_1"], color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\beta_1$}"),
       x = "m", y = latex2exp::TeX("$\\beta_1$")) +
  p_theme

# Beta 2
df <- data.frame(
  time = 1:nrow(traces), beta_2 = traces[, 5], variable = "Trace"
)
p3 <- ggplot(data = df, aes(x = time, y = beta_2)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = true_params_simulation["beta_2"], color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\beta_2$}"),
       x = "m", y = latex2exp::TeX("$\\beta_2$")) +
  p_theme

# Sigma x
df <- data.frame(
  time = 1:nrow(traces), sigma_x = exp(traces[, 6]), variable = "Trace"
)
p4 <- ggplot(data = df, aes(x = time, y = sigma_x)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = exp(true_params_simulation["sigma_x"]), color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\sigma_u$}"),
       x = "m", y = latex2exp::TeX("$\\sigma_u$")) +
  p_theme

# Sigma y
df <- data.frame(
  time = 1:nrow(traces), sigma_y = exp(traces[, 7]), variable = "Trace"
)
p5 <- ggplot(data = df, aes(x = time, y = sigma_y)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = exp(true_params_simulation["sigma_y"]), color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\sigma_\\epsilon$}"),
       x = "m", y = latex2exp::TeX("$\\sigma_\\epsilon$")) +
  p_theme

# Phi
df <- data.frame(
  time = 1:nrow(traces), phi = tanh(traces[, 8]), variable = "Trace"
)
p6 <- ggplot(data = df, aes(x = time, y = phi)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = tanh(true_params_simulation["phi"]), color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\phi$}"),
       x = "m", y = latex2exp::TeX("$\\phi$")) +
  p_theme

# X 0
df <- data.frame(
  time = 1:nrow(traces), x_0 = traces[, 9], variable = "Trace"
)
p7 <- ggplot(data = df, aes(x = time, y = x_0)) +
  geom_line(aes(colour = variable)) + theme_light() +
  geom_hline(
    aes(yintercept = true_params_simulation["x_0"], color = "True"),
    show.legend = TRUE, linetype = "dashed"
  ) +
  scale_color_manual(values = c("black", "red"), labels = c("Trace", "True")) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\x_0$}"),
       x = "m", y = latex2exp::TeX("$\\x_0$")) +
  p_theme

get_legend <- function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}
legend <- get_legend(p2)

grid.arrange(
  p1 + theme(legend.position = "none"), 
  p2 + theme(legend.position = "none"), 
  p3 + theme(legend.position = "none"), 
  p4 + theme(legend.position = "none"), 
  p5 + theme(legend.position = "none"), 
  p6 + theme(legend.position = "none"), 
  p7 + theme(legend.position = "none"), 
  ncol = 2, legend = legend
)
```

```{r, sim-4-appendix-marginal-posterior-setup-3, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Exemplary simulation: Gaussian kernel density estimates of the marginal posterior distributions for setup 3", fig.width=6, fig.height=6.75, warning=FALSE}
traces <- sim_4$pmmh_false_informative$traces[25000:50000, ]

# Plot theme:
p_theme <- theme(
  legend.justification = c(0, 1), legend.position = c(0, 1), 
  legend.title = element_blank(), 
  legend.background = element_rect(fill = "transparent"),
  plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"),
  plot.title = element_text(hjust = 0.5, size = 8),
  axis.title.x = element_text(size = 8),
  axis.title.y = element_text(size = 8, vjust = 0.5),
  legend.key = element_rect(fill = NA),
  axis.text.x = element_text(size = 7),
  axis.text.y = element_text(size = 7)
)

# Beta 0
df <- data.frame(time = 1:nrow(traces), beta_0 = traces[, 3])
p1 <- ggplot(data = df, aes(x = beta_0)) +
  geom_density(aes(x = beta_0, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = true_params_simulation["beta_0"], colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(traces[, 3]), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = dnorm, aes(colour = "Prior", linetype = "Prior"),
    args = list(mean = 0.8, sd = 0.1)
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\beta_0$}"),
       x = latex2exp::TeX("$\\beta_0$"))

# Beta 1
df <- data.frame(time = 1:nrow(traces), beta_1 = traces[, 4])
p2 <- ggplot(data = df, aes(x = beta_1)) +
  geom_density(aes(x = beta_1, colour = "Marginal posterior", linetype = "Marginal posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = true_params_simulation["beta_1"], colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(traces[, 4]), 
        colour = "Marginal posterior mean", linetype = "Marginal posterior mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = dnorm, aes(colour = "Prior", linetype = "Prior"),
    args = list(mean = 0.0, sd = 0.1)
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Marginal posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Marginal posterior mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Marginal posterior" = 1, "Marginal posterior mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\beta_1$}"),
       x = latex2exp::TeX("$\\beta_1$"))

# Beta 2
df <- data.frame(time = 1:nrow(traces), beta_2 = traces[, 5])
p3 <- ggplot(data = df, aes(x = beta_2)) +
  geom_density(aes(x = beta_2, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = true_params_simulation["beta_2"], colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(traces[, 5]), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = dnorm, aes(colour = "Prior", linetype = "Prior"),
    args = list(mean = 0.0, sd = 0.1)
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\beta_2$}"),
       x = latex2exp::TeX("$\\beta_2$"))

# Sigma x
df <- data.frame(time = 1:nrow(traces), sigma_x = exp(traces[, 6]))
p4 <- ggplot(data = df, aes(x = sigma_x)) +
  geom_density(aes(x = sigma_x, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = exp(true_params_simulation["sigma_x"]), colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(exp(traces[, 6])), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = dgamma, aes(colour = "Prior", linetype = "Prior"),
    args = list(shape = 10, scale = 0.01)
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\sigma_u$}"),
       x = latex2exp::TeX("$\\sigma_u$"))

# Sigma y
df <- data.frame(time = 1:nrow(traces), sigma_y = exp(traces[, 7]))
p5 <- ggplot(data = df, aes(x = sigma_y)) +
  geom_density(aes(x = sigma_y, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = exp(true_params_simulation["sigma_y"]), colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(exp(traces[, 7])), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = dgamma, aes(colour = "Prior", linetype = "Prior"),
    args = list(shape = 10, scale = 0.005)
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\sigma_\\epsilon$}"),
       x = latex2exp::TeX("$\\sigma_\\epsilon$"))

# Phi
df <- data.frame(time = 1:nrow(traces), phi = tanh(traces[, 8]))
p6 <- ggplot(data = df, aes(x = phi)) +
  geom_density(aes(x = phi, colour = "Posterior", linetype = "Posterior"),
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = tanh(true_params_simulation["phi"]), colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(tanh(traces[, 8])), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = truncnorm::dtruncnorm, aes(colour = "Prior", linetype = "Prior"),
    args = list(a = -1, b = 1, mean = 0.3, sd = 0.1)
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\phi$}"),
       x = latex2exp::TeX("$\\phi$"))

# X 0
df <- data.frame(time = 1:nrow(traces), x_0 = traces[, 9])
p7 <- ggplot(data = df, aes(x = x_0)) +
  geom_density(aes(x = x_0, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = true_params_simulation["x_0"], colour = "True",
        linetype = "True"), show.legend = FALSE
  ) +
  geom_vline(
    aes(xintercept = mean(traces[, 9]), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = dnorm, aes(colour = "Prior", linetype = "Prior"),
    args = list(mean = 3.0, sd = 0.1)
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", "True" = "red",
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, 
               "Prior" = 1, "True" = 3), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\x_0$}"),
       x = latex2exp::TeX("$\\x_0$"))

get_legend <- function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}
legend <- get_legend(p2)

grid.arrange(
  p1 + theme(legend.position = "none"), 
  p2 + theme(legend.position = "none"), 
  p3 + theme(legend.position = "none"), 
  p4 + theme(legend.position = "none"), 
  p5 + theme(legend.position = "none"), 
  p6 + theme(legend.position = "none"), 
  p7 + theme(legend.position = "none"), 
  ncol = 2, legend = legend
)
rm(list = ls())
```

```{r, extension-basic-model-setup, eval=TRUE, echo=FALSE}
# get simulation result
source(here::here("R", "model_comparison", "model_comparison_basic_model.R"))
library(ggplot2)
library(gridExtra)
```

```{r, basic-model-trace-plot, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Random walk model: PMMH trace plots", fig.width=6, fig.height=6.75, warning=FALSE}
library(ggplot2)
library(gridExtra)

traces <- list(
  result[[1]]@traces[10000:50000, ], 
  result[[2]]@traces[10000:50000, ], 
  result[[3]]@traces[10000:50000, ]
)

# Plot theme:
p_theme <- theme(
  legend.position = "bottom", 
  legend.title = element_blank(), 
  legend.background = element_rect(fill = "transparent"),
  plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"),
  plot.title = element_text(hjust = 0.5, size = 8),
  axis.title.x = element_text(size = 8),
  axis.title.y = element_text(size = 8, angle = 0, vjust = 0.5),
  axis.text.x = element_text(size = 7),
  axis.text.y = element_text(size = 7)
)

# e_lpd_0
df <- data.frame(
  time = 1:nrow(traces[[1]]),
  value1 = traces[[1]][, 3], 
  value2 = traces[[2]][, 3], 
  value3 = traces[[3]][, 3]
)
df <- reshape2::melt(df, id.vars = "time")
p1 <- ggplot(data = df, aes(x = time, y = value)) +
  geom_line(aes(colour = variable)) + theme_light() +
  scale_colour_manual(
    values = c("red", "green", "blue"),
    labels = c("Chain 1", "Chain 2", "Chain 3")
  ) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\tilde{\\eta}_0$}"),
       x = "m", y = latex2exp::TeX("$\\tilde{\\eta}_0$")) +
  p_theme

# sigma_u
df <- data.frame(
  time = 1:nrow(traces[[1]]),
  value1 = traces[[1]][, 4], 
  value2 = traces[[2]][, 4], 
  value3 = traces[[3]][, 4]
)
df <- reshape2::melt(df, id.vars = "time")
df$value <- exp(df$value)
p2 <- ggplot(data = df, aes(x = time, y = value)) +
  geom_line(aes(colour = variable)) + theme_light() +
  scale_colour_manual(
    values = c("red", "green", "blue"),
    labels = c("Chain 1", "Chain 2", "Chain 3")
  ) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\sigma_u$}"),
       x = "m", y = latex2exp::TeX("$\\sigma_u$")) +
  p_theme

# sigma_epsilon
df <- data.frame(
  time = 1:nrow(traces[[1]]),
  value1 = traces[[1]][, 5], 
  value2 = traces[[2]][, 5], 
  value3 = traces[[3]][, 5]
)
df <- reshape2::melt(df, id.vars = "time")
df$value <- exp(df$value)
p3 <- ggplot(data = df, aes(x = time, y = value)) +
  geom_line(aes(colour = variable)) + theme_light() +
  scale_colour_manual(
    values = c("red", "green", "blue"),
    labels = c("Chain 1", "Chain 2", "Chain 3")
  ) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\sigma_\\epsilon$}"),
       x = "m", y = latex2exp::TeX("$\\sigma_\\epsilon$")) +
  p_theme

legend <- gtable::gtable_filter(ggplot_gtable(ggplot_build(p2)), "guide-box")

grid.arrange(
  p1 + theme(legend.position = "none"), 
  p2 + theme(legend.position = "none"), 
  p3 + theme(legend.position = "none"),
  ncol = 1, legend = legend, nrow = 4, 
  heights = c(1.1, 1.1, 1.1, 0.2)
)
```

```{r, basic-model-posterior-marginal-density, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Random walk model: Gaussian kernel density estimates of the marginal posterior distributions", fig.width=6, fig.height=5, warning=FALSE}
traces <- rbind(
        result[[1]]@traces[10000:50000, 3:5], 
        result[[2]]@traces[10000:50000, 3:5], 
        result[[3]]@traces[10000:50000, 3:5]
)

# Plot theme:
p_theme <- theme(
  legend.justification = c(0, 1), legend.position = c(0, 1), 
  legend.title = element_blank(), 
  legend.background = element_rect(fill = "transparent"),
  plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"),
  plot.title = element_text(hjust = 0.5, size = 8),
  axis.title.x = element_text(size = 8),
  axis.title.y = element_text(size = 8, vjust = 0.5),
  axis.text.x = element_text(size = 7),
  axis.text.y = element_text(size = 7)
)

# e_lpd_0
df <- data.frame(time = 1:nrow(traces), e_lpd_0 = traces[, 1])
p1 <- ggplot(data = df, aes(x = e_lpd_0)) +
  geom_density(aes(x = e_lpd_0, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = mean(traces[, 1]), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = function(...){1}, aes(colour = "Prior", linetype = "Prior")
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", 
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, "Prior" = 1), name = ""
  ) + 
  labs(title = latex2exp::TeX(
    "Marginal posterior: \\textbf{$\\tilde{\\eta}_0$}"
  ), x = latex2exp::TeX("$\\tilde{\\eta}_0$"))

# sigma_u
df <- data.frame(time = 1:nrow(traces), sigma_u = exp(traces[, 2]))
p2 <- ggplot(data = df, aes(x = sigma_u)) +
  geom_density(aes(x = sigma_u, colour = "Marginal posterior", linetype = "Marginal posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = mean(exp(traces[, 2])), 
        colour = "Marginal posterior mean", linetype = "Marginal posterior mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = function(...){1}, aes(colour = "Prior", linetype = "Prior")
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Marginal posterior" = "black", "Prior" = "#30914a", 
               "Marginal posterior mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Marginal posterior" = 1, "Marginal posterior mean" = 3, "Prior" = 1), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\sigma_u$}"),
       x = latex2exp::TeX("$\\sigma_u$"))

# sigma_epsilon
df <- data.frame(time = 1:nrow(traces), sigma_e = exp(traces[, 3]))
p3 <- ggplot(data = df, aes(x = sigma_e)) +
  geom_density(aes(x = sigma_e, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = mean(exp(traces[, 3])), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = function(...){1}, aes(colour = "Prior", linetype = "Prior")
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", 
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, "Prior" = 1), name = ""
  ) + 
  labs(title = latex2exp::TeX(
    "Marginal posterior: \\textbf{$\\sigma_\\epsilon$}"
  ), x = latex2exp::TeX("$\\sigma_\\epsilon$"))

get_legend <- function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}
legend <- get_legend(p2)

grid.arrange(
  p1 + theme(legend.position = "none"), 
  p2 + theme(legend.position = "none"), 
  p3 + theme(legend.position = "none"), 
  ncol = 2, legend = legend
)
rm(list = ls())
```

```{r, extension-static-model-setup, eval=TRUE, echo=FALSE}
# get simulation result
source(here::here("R", "model_comparison", "model_comparison_static_model.R"))
library(ggplot2)
library(gridExtra)
```

```{r, extension-static-model-trace, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Covariate extension: PMMH trace plots", fig.width=6, fig.height=6.75, warning=FALSE}
traces <- list(
  result[[1]]@traces[10000:50000, ], 
  result[[2]]@traces[10000:50000, ],
  result[[3]]@traces[10000:50000, ]
)

# Plot theme:
p_theme <- theme(
  legend.justification = c(0, 1), legend.position = c(0, 1), 
  legend.title = element_blank(), 
  legend.background = element_rect(fill = "transparent"),
  plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"),
  plot.title = element_text(hjust = 0.5, size = 8),
  axis.title.x = element_text(size = 8),
  axis.title.y = element_text(size = 8, angle = 0, vjust = 0.5),
  axis.text.x = element_text(size = 7),
  axis.text.y = element_text(size = 7)
)

# beta_0
df <- data.frame(
  time = 1:nrow(traces[[1]]),
  value1 = traces[[1]][, 3], 
  value2 = traces[[2]][, 3], 
  value3 = traces[[3]][, 3]
)
df <- reshape2::melt(df, id.vars = "time")
p1 <- ggplot(data = df, aes(x = time, y = value)) +
  geom_line(aes(colour = variable)) + theme_light() +
  scale_colour_manual(
    values = c("red", "green", "blue"),
    labels = c("Chain 1", "Chain 2", "Chain 3")
  ) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\beta_0$}"),
       x = "m", y = latex2exp::TeX("$\\beta_0$")) +
  p_theme

# beta_1
df <- data.frame(
  time = 1:nrow(traces[[1]]),
  value1 = traces[[1]][, 4], 
  value2 = traces[[2]][, 4], 
  value3 = traces[[3]][, 4]
)
df <- reshape2::melt(df, id.vars = "time")
p2 <- ggplot(data = df, aes(x = time, y = value)) +
  geom_line(aes(colour = variable)) + theme_light() +
  scale_colour_manual(
    values = c("red", "green", "blue"),
    labels = c("Chain 1", "Chain 2", "Chain 3")
  ) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\beta_1$}"),
       x = "m", y = latex2exp::TeX("$\\beta_1$")) +
  p_theme

# beta_2
df <- data.frame(
  time = 1:nrow(traces[[1]]),
  value1 = traces[[1]][, 5], 
  value2 = traces[[2]][, 5], 
  value3 = traces[[3]][, 5]
)
df <- reshape2::melt(df, id.vars = "time")
p3 <- ggplot(data = df, aes(x = time, y = value)) +
  geom_line(aes(colour = variable)) + theme_light() +
  scale_colour_manual(
    values = c("red", "green", "blue"),
    labels = c("Chain 1", "Chain 2", "Chain 3")
  ) +
  labs(title = latex2exp::TeX("Trace Plot: \\textbf{$\\beta_2$}"),
       x = "m", y = latex2exp::TeX("$\\beta_2$")) +
  p_theme

# sigma_u
df <- data.frame(
  time = 1:nrow(traces[[1]]),
  value1 = traces[[1]][, 6], 
  value2 = traces[[2]][, 6], 
  value3 = traces[[3]][, 6]
)
df <- reshape2::melt(df, id.vars = "time")
df[, "value"] <- exp(df[, "value"])
p4 <- ggplot(data = df, aes(x = time, y = value)) +
  geom_line(aes(colour = variable)) + theme_light() +
  scale_colour_manual(
    values = c("red", "green", "blue"),
    labels = c("Chain 1", "Chain 2", "Chain 3")
  ) +
  labs(title = latex2exp::TeX("Trace Plot: $\\sigma_u$"),
       x = "m", y = latex2exp::TeX("$\\sigma_u$")) +
  p_theme

# sigma_e
df <- data.frame(
  time = 1:nrow(traces[[1]]),
  value1 = traces[[1]][, 7], 
  value2 = traces[[2]][, 7], 
  value3 = traces[[3]][, 7]
)
df <- reshape2::melt(df, id.vars = "time")
df[, "value"] <- exp(df[, "value"])
p5 <- ggplot(data = df, aes(x = time, y = value)) +
  geom_line(aes(colour = variable)) + theme_light() +
  scale_colour_manual(
    values = c("red", "green", "blue"),
    labels = c("Chain 1", "Chain 2", "Chain 3")
  ) +
  labs(title = latex2exp::TeX("Trace Plot: $\\sigma_\\epsilon$"),
       x = "m", y = latex2exp::TeX("$\\sigma_\\epsilon$")) +
  p_theme

legend <- gtable::gtable_filter(ggplot_gtable(ggplot_build(p2)), "guide-box")

grid.arrange(
  p1 + theme(legend.position = "none"), 
  p2 + theme(legend.position = "none"), 
  p3 + theme(legend.position = "none"), 
  p4 + theme(legend.position = "none"), 
  p5 + theme(legend.position = "none"),
  ncol = 2, legend = legend, nrow = 3
)
```

```{r, extension-static-model-marginal-density, eval=TRUE, echo=FALSE, fig.align='center', fig.cap="Covariate extension: Gaussian kernel density estimates of the marginal posterior distributions", fig.width=6, fig.height=6.75, warning=FALSE}
traces <- rbind(
  result[[1]]@traces[10000:50000, ], 
  result[[2]]@traces[10000:50000, ], 
  result[[3]]@traces[10000:50000, ]
)

# Plot theme:
p_theme <- theme(
  legend.justification = c(0, 1), legend.position = c(0, 1), 
  legend.title = element_blank(), 
  legend.background = element_rect(fill = "transparent"),
  plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"),
  plot.title = element_text(hjust = 0.5, size = 8),
  axis.title.x = element_text(size = 8),
  axis.title.y = element_text(size = 8, vjust = 0.5),
  axis.text.x = element_text(size = 7),
  axis.text.y = element_text(size = 7)
)

# beta_0
df <- data.frame(time = 1:nrow(traces), beta_0 = traces[, 3])
p1 <- ggplot(data = df, aes(x = beta_0)) +
  geom_density(aes(x = beta_0, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = mean(traces[, 3]), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = function(...){1}, aes(colour = "Prior", linetype = "Prior")
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", 
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, "Prior" = 1), name = ""
  ) + 
  labs(title = latex2exp::TeX(
    "Marginal posterior: \\textbf{$\\beta_0$}"
  ), x = latex2exp::TeX("$\\beta_0$"))

# beta_1
df <- data.frame(time = 1:nrow(traces), beta_1 = traces[, 4])
p2 <- ggplot(data = df, aes(x = beta_1)) +
  geom_density(aes(x = beta_1, colour = "Marginal posterior", linetype = "Marginal posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = mean(traces[, 4]), 
        colour = "Marginal posterior mean", linetype = "Marginal posterior mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = function(...){1}, aes(colour = "Prior", linetype = "Prior")
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Marginal posterior" = "black", "Prior" = "#30914a", 
               "Marginal posterior mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Marginal posterior" = 1, "Marginal posterior mean" = 3, "Prior" = 1), name = ""
  ) + 
  labs(title = latex2exp::TeX(
    "Marginal posterior: \\textbf{$\\beta_1$}"
  ), x = latex2exp::TeX("$\\beta_1$"))

# beta_2
df <- data.frame(time = 1:nrow(traces), beta_2 = traces[, 5])
p3 <- ggplot(data = df, aes(x = beta_2)) +
  geom_density(aes(x = beta_2, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = mean(traces[, 5]), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = function(...){1}, aes(colour = "Prior", linetype = "Prior")
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", 
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, "Prior" = 1), name = ""
  ) + 
  labs(title = latex2exp::TeX(
    "Marginal posterior: \\textbf{$\\beta_2$}"
  ), x = latex2exp::TeX("$\\beta_2$"))

# sigma_u
df <- data.frame(time = 1:nrow(traces), sigma_u = exp(traces[, 6]))
p4 <- ggplot(data = df, aes(x = sigma_u)) +
  geom_density(aes(x = sigma_u, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = mean(exp(traces[, 6])), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = function(...){1}, aes(colour = "Prior", linetype = "Prior")
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", 
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, "Prior" = 1), name = ""
  ) + 
  labs(title = latex2exp::TeX("Marginal posterior: \\textbf{$\\sigma_u$}"),
       x = latex2exp::TeX("$\\sigma_u$"))

# sigma_epsilon
df <- data.frame(time = 1:nrow(traces), sigma_e = exp(traces[, 7]))
p5 <- ggplot(data = df, aes(x = sigma_e)) +
  geom_density(aes(x = sigma_e, colour = "Posterior", linetype = "Posterior"), 
               show.legend = FALSE) + 
  geom_vline(
    aes(xintercept = mean(exp(traces[, 7])), 
        colour = "Posterior Mean", linetype = "Posterior Mean"), 
    show.legend = FALSE
  ) +
  stat_function(
    fun = function(...){1}, aes(colour = "Prior", linetype = "Prior")
  ) +
  theme_light() + p_theme + 
  scale_color_manual(
    values = c("Posterior" = "black", "Prior" = "#30914a", 
               "Posterior Mean" = "#2126b5"), 
    name = ""
  ) +
  scale_linetype_manual(
    values = c("Posterior" = 1, "Posterior Mean" = 3, "Prior" = 1), name = ""
  ) + 
  labs(title = latex2exp::TeX(
    "Marginal posterior: \\textbf{$\\sigma_\\epsilon$}"
  ), x = latex2exp::TeX("$\\sigma_\\epsilon$"))

get_legend <- function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}
legend <- get_legend(p2)

grid.arrange(
  p1 + theme(legend.position = "none"), 
  p2 + theme(legend.position = "none"), 
  p3 + theme(legend.position = "none"), 
  p4 + theme(legend.position = "none"), 
  p5 + theme(legend.position = "none"),
  ncol = 2, legend = legend, nrow = 3
)
```

# Appendix: Algorithms {#algorithms}

The following algorithm descriptions are taken from [@pomp_article].

## Particle filter and systematic resampling

\begin{algorithm}
  \caption{Particle filter}
  \label{alg:particle-filter}
    \KwIn {Simulators $\mu_{\theta}(x_{0})$ and $f_{\theta}(x_{n}|x_{n-1})$ \newline
    Evaluator $g_{\theta}(y_{n}|x_{n})$ \newline
    Parameter $\theta$ \newline
    Data $y^*_{1:N}$ \newline
    Number of particles $J$
    }
  \nl Initialize filter particles: simulate $x^F_{0,j}$ from $\mu_{\theta}(\cdot)$ for $j$ in $1:J$\;
  \nl \For{$n = 1$ \To{} $N$}{%
    \nl Simulate for prediction: $x^P_{n,j}$ from $f_{\theta}(\cdot|x^F_{n-1,j})$ for $j$ in $1:J$\;
    \nl Evaluate weights: $w(n, j) = g_{\theta}(y^*_{n}|x^P_{n,j})$ for $j$ in $1:J$\;
    \nl Normalize weights: $\tilde{w}(n, j) = \frac{w(n, j)}{\sum^J_{m=1}w(n, m)}$\;
    \nl Apply Algorithm \ref{alg:systematic-resampling} to select indices $k_{1:J}$ with $P(k_j = m) = \tilde{w}(n, m)$\;
    \nl Resample: set $x^F_{n,j} = x^P_{n,k_j}$ for $j$ in $1:J$\;
    \nl Compute conditional log likelihood: $\hat{\ell}_{n|n-1} = \ln\Big(\frac{1}{J}\sum^J_{m=1}w(n, m)\Big)$\;
  }
  \KwOut{Log likelihood estimate: $\hat{\ell}(\theta) = \sum_{n=1}^N\hat{\ell}_{n|n-1}$ \newline
  Filter sample: $x^F_{n,1:J}$ for $n$ in $1:N$}
\end{algorithm}

\begin{algorithm}
  \caption{Systematic resampling}
  \label{alg:systematic-resampling}
    \KwIn {Weights $\tilde{w}_{1:J}$, normalized so that $\sum^J_{j=1}\tilde{w}_{j} = 1$
    }
  \nl Construct cumulative sum: $c_j = \sum_{m=1}^j \tilde{w}_m$ for $j$ in $1:J$\;
  \nl Draw a uniform initial sampling point: $U_1 \sim \mathcal{U}(0, \frac{1}{J})$\;
  \nl Construct evenly spaced sampling points: $U_j = U_1 + (j - 1)\frac{1}{J}$ for $j$ in $2:J$\;
  \nl Initialize: set $p=1$\;
  \nl \For{$j = 1$ \To{} $J$}{%
    \nl \While{$U_j > c_p$}{%
      \nl Step to the next resampling index: set $p = p + 1$\;
    }
    \nl Assign resampling index: set $k_j = p$\;
  }
  \KwOut{Resampling indices $k_{1:J}$}
\end{algorithm}

## PMMH

\begin{algorithm}
  \caption{PMMH}
  \label{alg:pmmh}
    \KwIn {Simulators $\mu_{\theta}(x_{0})$ and $f_{\theta}(x_{n}|x_{n-1})$ \newline
    Evaluator $g_{\theta}(y_{n}|x_{n})$ \newline
    Proposal distribution $q(\theta^*|\theta^{(m-1)})$ \newline
    Starting parameter $\theta_0$ \newline
    Prior $\pi(\theta)$ \newline
    Data $y^*_{1:N}$ \newline
    Number of particles $J$ \newline
    Number of iterations $M$
    }
  \nl Initialization: Compute $\hat{\ell}_0 = \hat{\ell}(\theta_0)$ using Algorithm \ref{alg:particle-filter} with $J$ particles\;
  \nl Draw random trajectory $x^{(0)}_{0:N}$from $x^F_{0:N,1:J}$ obtained from particle filter \;
  \nl \For{$m = 1$ \To{} $M$}{%
    \nl Draw a parameter proposal $\theta^*\sim q(\theta^*|\theta^{(m-1)})$\;
    \nl Compute $\hat{\ell}^* = \hat{\ell}(\theta^*)$ using algorithm \ref{alg:particle-filter} with $J$ particles\;
    \nl Draw random trajectory $x^{*}_{0:N}$ from $x^F_{0:N,1:J}$ obtained from particle filter \;
    \nl Compute acceptance probability $$\alpha(\theta^{*}|\theta^{(m-1)}) = \min\Bigg\{1, \frac{\text{exp}(\hat{\ell}^*)\pi(\theta^*)q(\theta^{(m-1)}|\theta^*)}{\text{exp}(\hat{\ell}^{(m-1)})\pi(\theta^{(m-1)})q(\theta^*|\theta^{(m-1)})}\Bigg\}$$\;
    \nl Generate $U \sim \mathcal{U}(0, 1)$\;
    \nl Set $$
    (\theta^{(m)}, \hat{\ell}^{(m)}, x^{(m)}_{0:N}) =
    \begin{cases}
      (\theta^{*}, \hat{\ell}^{*}, x^{*}_{0:N}), & \text{if}\;\; U < \alpha(\theta^{*}|\theta^{(m-1)}) \\
      (\theta^{(m-1)}, \hat{\ell}^{(m-1)}, x^{(m-1)}_{0:N}), & \text{otherwise}
    \end{cases}$$\;
  }
  \KwOut{Samples $x^{(1:M)}_{0:N}$, $\theta^{(1:M)}$ from posterior distribution $p(x_{0:N}, \theta|y^*_{1:N})$}
\end{algorithm}

# Appendix: Derivations {#derivations}

## Prediction & Filtering distribution

\begin{equation*}
\begin{split}
  p_{\theta}(x_n|y^*_{1:n-1}) & = \int p_{\theta}(x_n, x_{n-1}|y^*_{1:n-1})dx_{n-1} \\
  & = \int \frac{p_{\theta}(x_n, x_{n-1}, y^*_{1:n-1})}{p_{\theta}(y^*_{1:n-1})}dx_{n-1} \\
  & = \int \frac{p_{\theta}(x_n, y^*_{1:n-1} | x_{n-1})\;p_{\theta}(x_{n-1})}{p_{\theta}(y^*_{1:n-1})}dx_{n-1} \\
  & = \int \frac{p_{\theta}(x_n| y^*_{1:n-1}, x_{n-1})\;p_{\theta}(y^*_{1:n-1}, x_{n-1})\;p_{\theta}(x_{n-1})}{p_{\theta}(y^*_{1:n-1})\;p_{\theta}(x_{n-1})}dx_{n-1} \\
  & = \int f_{\theta}(x_n|x_{n-1})p_{\theta}(x_{n-1}|y^*_{1:n-1})dx_{n-1}
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
  p_{\theta}(x_n|y^*_{1:n}) & = p_{\theta}(x_n|y^*_n, y^*_{1:n-1}) = \frac{
    p_{\theta}(y^*_n, y^*_{1:n-1} | x_{n})\;p_{\theta}(x_n)
  }{
    p_{\theta}(y^*_n, y^*_{1:n-1})
  } \\
  & = \frac{
    p_{\theta}(y^*_n | y^*_{1:n-1}, x_{n})\;p_{\theta}(y^*_{1:n-1} | x_n)\;p_{\theta}(x_n)
  }{
    p_{\theta}(y^*_n | y^*_{1:n-1})\;p_{\theta}(y^*_{1:n-1})
  } \\
  & = \frac{
    p_{\theta}(y^*_n | y^*_{1:n-1}, x_{n})\;p_{\theta}(x_n | y^*_{1:n-1})p_{\theta}(y^*_{1:n-1})\;p_{\theta}(x_n)
  }{
    p_{\theta}(y^*_n|y^*_{1:n-1})\;p_{\theta}(y^*_{1:n-1})\;p_{\theta}(x_n)
  } \\
  & = \frac{g_{\theta}(y^*_n|x_{n})p_{\theta}(x_n|y^*_{1:n-1})}{p_{\theta}(y^*_n|y^*_{1:n-1})}
\end{split}
\end{equation*}

## PMMH - acceptance probability

Using that:
\begin{equation*}
\begin{split}
p(\theta, x_{0:N}|y^*_{1:N}) &\propto \mathcal{L}(\theta, x_{0:N})\pi(\theta) \\
&\propto p_{\theta}(y^*_{1:N}|x_{0:N})p_{\theta}(x_{0:N})\pi(\theta)
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
\frac{p_{\theta}(y^*_{1:N}|x_{0:N})p_{\theta}(x_{0:N})}{p_{\theta}(x_{0:N}|y^*_{1:N})} & = \frac{p(y^*_{1:N}, x_{0:N}, \theta)\;p(x_{0:N}, \theta)\;p(y^*_{1:N}, \theta)}{p(y^*_{1:N}, x_{0:N}, \theta)\;p(x_{0:N}, \theta)\;p(\theta)} \\
& = \frac{p(y^*_{1:N}, \theta)}{p(\theta)} = p_{\theta}(y^*_{1:N}) \\
& = \mathcal{L}(\theta)
\end{split}
\end{equation*}
\newpage
The acceptance probability can therefore be simplified:
\begin{equation*}
\begin{split}
\alpha(\theta^{*}|\theta^{(m-1)}) & = \min\Bigg\{1, \frac{p(\theta^*, x^*_{0:N}|y^*_{1:N})\;q(\theta^{(m-1)}, x^{(m-1)}_{0:N}|\theta^*, x^*_{0:N})}{p(\theta^{(m-1)}, x^{(m-1)}_{0:N}|y^*_{1:N})\;q(\theta^*, x^*_{0:N}|\theta^{(m-1)}, x^{(m-1)}_{0:N})}\Bigg\} \\
& = \min\Bigg\{1, \frac{p(\theta^*, x^*_{0:N}|y^*_{1:N})\;q(\theta^{(m-1)}|\theta^*)\;p_{\theta^{(m-1)}}(x^{(m-1)}_{0:N}|y^*_{1:N})}{p(\theta^{(m-1)}, x^{(m-1)}_{0:N}|y^*_{1:N})\;q(\theta^*|\theta^{(m-1)})\;p_{\theta^*}(x^*_{0:N}|y^*_{1:N})}\Bigg\} \\
& = \min\Bigg\{1, \frac{\mathcal{L}(\theta^*)\pi(\theta^*)q(\theta^{(m-1)}|\theta^*)}{\mathcal{L}(\theta^{(m-1)})\pi(\theta^{(m-1)})q(\theta^*|\theta^{(m-1)})}\Bigg\}
\end{split}
\end{equation*}


<!--chapter:end:Rmd//Appendix.Rmd-->

